{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529ded27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import math\n",
    "import pandas as pd\n",
    "import random\n",
    "import gzip\n",
    "import torch\n",
    "from sklearn import metrics\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "import gensim\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import argparse\n",
    "import warnings\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict\n",
    "import csv\n",
    "from statsmodels.tsa.stattools import acf\n",
    "from numpy import array\n",
    "from numpy import hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d03534",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5055696",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75914a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split a multivariate sequence into samples\n",
    "def Convert_format1(data, n_steps_in, n_steps_out):\n",
    "    n0=np.where(data['PeriodsSepLastTwoNnZeroDemands']>0)[0][1]\n",
    "    n1=n0+int((data.shape[0]-n0)*90/100)\n",
    "    in_seq1=data['ZNZDemand'].values[n0:]\n",
    "    in_seq2=data['LastQtte'].values[n0:]\n",
    "    in_seq3=data['WeekDay'].values[n0:]\n",
    "    in_seq4=data['Interval'].values[n0:]\n",
    "    in_seq5=data['PeriodsSepLastTwoNnZeroDemands'].values[n0:]\n",
    "    in_seq6=data['Month'].values[n0:]\n",
    "    in_seq1 = in_seq1.reshape((len(in_seq1), 1))\n",
    "    in_seq2 = in_seq2.reshape((len(in_seq2), 1))\n",
    "    in_seq3 = in_seq3.reshape((len(in_seq3), 1))\n",
    "    in_seq4 = in_seq4.reshape((len(in_seq4), 1))\n",
    "    in_seq5 = in_seq5.reshape((len(in_seq5), 1))\n",
    "    in_seq6 = in_seq6.reshape((len(in_seq6), 1))\n",
    "    # horizontally stack columns\n",
    "    dataset = hstack((in_seq1, in_seq2,in_seq3,in_seq4,in_seq5,in_seq6))\n",
    "    X = []\n",
    "    for i in range(len(dataset)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps_in\n",
    "        out_end_ix = end_ix + n_steps_out\n",
    "        # check if we are beyond the dataset\n",
    "        if (out_end_ix-1) > len(dataset):\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = list(dataset[i:end_ix,1:]), list(dataset[end_ix-1:out_end_ix-1, 0])\n",
    "        X.append([seq_x,seq_y])\n",
    "    size=int((n1) / 5)\n",
    "    train_data=X[:n1]\n",
    "    test_data=X[n1:]\n",
    "    firsttrain = X[:4*size]\n",
    "    firstvalid = X[4*size:n1]\n",
    "    alldata=train_data+test_data\n",
    "    return firsttrain,firstvalid,train_data,test_data,alldata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e3848b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset_load(Dataset):\n",
    "    def __init__(self,xy=None):\n",
    "        self.x_data=np.asarray([el[0] for el in xy],dtype=np.float32)\n",
    "        self.y_data =np.asarray([el[1] for el in xy ],dtype=np.float32)\n",
    "        self.x_data = torch.from_numpy(self.x_data)\n",
    "        self.y_data = torch.from_numpy(self.y_data)\n",
    "        self.len=len(self.x_data)\n",
    "      \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209fa228",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_confusion_matrix(cf,labels,pred,group_names=None,categories='auto',count=True,percent=True,cbar=True,xyticks=True,xyplotlabels=True,sum_stats=True,figsize=None,cmap='Blues',title=None):\n",
    "    blanks = ['' for i in range(cf.size)]\n",
    "\n",
    "    if group_names and len(group_names)==cf.size:\n",
    "        group_labels = [\"{}\\n\".format(value) for value in group_names]\n",
    "    else:\n",
    "        group_labels = blanks\n",
    "\n",
    "    if count:\n",
    "        group_counts = [\"{0:0.0f}\\n\".format(value) for value in cf.flatten()]\n",
    "    else:\n",
    "        group_counts = blanks\n",
    "\n",
    "    if percent:\n",
    "        group_percentages = [\"{0:.2%}\".format(value) for value in cf.flatten()/np.sum(cf)]\n",
    "    else:\n",
    "        group_percentages = blanks\n",
    "\n",
    "    box_labels = [f\"{v1}{v2}{v3}\".strip() for v1, v2, v3 in zip(group_labels,group_counts,group_percentages)]\n",
    "    box_labels = np.asarray(box_labels).reshape(cf.shape[0],cf.shape[1])\n",
    "\n",
    "\n",
    "    # CODE TO GENERATE SUMMARY STATISTICS & TEXT FOR SUMMARY STATS\n",
    "    if sum_stats:\n",
    "        #if it is a binary confusion matrix, show some more stats\n",
    "        if len(cf)==2:\n",
    "            #Metrics for Binary Confusion Matrices\n",
    "            precision, recall, thresholds = precision_recall_curve(labels, pred)\n",
    "            Auc_PRC=metrics.auc(recall, precision)\n",
    "            Auc_ROC=metrics.roc_auc_score(labels, pred)\n",
    "            stats_text = \"\\n\\nAU_PRC={:0.3f}\\nAU_ROC={:0.3f}\".format(Auc_PRC,Auc_ROC)\n",
    "        else:\n",
    "            stats_text = \"\\n\\nAccuracy={:0.3f}\".format(accuracy)\n",
    "    else:\n",
    "        stats_text = \"\"\n",
    "\n",
    "\n",
    "    # SET FIGURE PARAMETERS ACCORDING TO OTHER ARGUMENTS\n",
    "    if figsize==None:\n",
    "        #Get default figure size if not set\n",
    "        figsize = plt.rcParams.get('figure.figsize')\n",
    "\n",
    "    if xyticks==False:\n",
    "        #Do not show categories if xyticks is False\n",
    "        categories=False\n",
    "\n",
    "\n",
    "    # MAKE THE HEATMAP VISUALIZATION\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.heatmap(cf,annot=box_labels,fmt=\"\",cmap=cmap,cbar=cbar,xticklabels=categories,yticklabels=categories)\n",
    "\n",
    "    if xyplotlabels:\n",
    "        plt.ylabel('True label')\n",
    "        plt.xlabel('Predicted label' + stats_text)\n",
    "    else:\n",
    "        plt.xlabel(stats_text)\n",
    "    \n",
    "    if title:\n",
    "        plt.title(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6252a640",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b483d9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Deepnet(nn.Module):\n",
    "    def __init__ (self,RNN,RNN_hidden_size,RNN_sigma,layer_size,N_sigma,dropprob,n_features):\n",
    "        super(Deepnet,self).__init__()\n",
    "        self.RNN=RNN\n",
    "        self.RNN_hidden_size=RNN_hidden_size\n",
    "        self.RNN_sigma=RNN_sigma\n",
    "        \n",
    "        self.layer_size=layer_size\n",
    "        self.N_sigma=N_sigma\n",
    "        \n",
    "        self.input_channels=n_features\n",
    "        self.dropprob=dropprob\n",
    "        \n",
    "        if self.RNN=='LSTM':\n",
    "            self.rnn = nn.LSTM(self.input_channels, RNN_hidden_size, num_layers=1, bidirectional=False).to(device)\n",
    "            self.FC_size= RNN_hidden_size\n",
    "        elif self.RNN=='BiLSTM':\n",
    "            self.rnn = nn.LSTM(self.input_channels, RNN_hidden_size, num_layers=1, bidirectional=True).to(device)\n",
    "            self.FC_size= 2*RNN_hidden_size\n",
    "        elif self.RNN=='GRU':\n",
    "            self.rnn = nn.GRU(self.input_channels, RNN_hidden_size, num_layers=1, bidirectional=False).to(device)\n",
    "            self.FC_size= RNN_hidden_size\n",
    "        elif self.RNN=='BiGRU':\n",
    "            self.rnn = nn.GRU(self.input_channels, RNN_hidden_size, num_layers=1, bidirectional=True).to(device)\n",
    "            self.FC_size= 2*RNN_hidden_size\n",
    "        for layer_p in self.rnn._all_weights:\n",
    "            for p in layer_p:\n",
    "                if 'weight' in p:\n",
    "                    torch.nn.init.normal_(self.rnn.__getattr__(p),mean=0,std=RNN_sigma)\n",
    "        \n",
    "        #weights between LSTM or CNN or GRU layers and fully connected layer\n",
    "        self.wHidden = torch.randn(self.FC_size, self.layer_size).to(device)\n",
    "        self.wHiddenBias = torch.randn(self.layer_size).to(device)\n",
    "        self.wHidden.requires_grad = True\n",
    "        self.wHiddenBias.requires_grad = True\n",
    "        \n",
    "        #weights between the fully connected layer and the output node\n",
    "        self.wNeu=torch.randn(self.layer_size,1).to(device)\n",
    "        self.wNeuBias=torch.randn(1).to(device) \n",
    "        torch.nn.init.xavier_uniform(self.wNeu)\n",
    "        torch.nn.init.xavier_uniform(self.wHidden)\n",
    "        self.wNeu.requires_grad = True\n",
    "        self.wNeuBias.requires_grad = True\n",
    "        \n",
    "        torch.nn.init.normal_(self.wNeu,mean=0,std=self.N_sigma)\n",
    "        torch.nn.init.normal_(self.wNeuBias,mean=0,std=self.N_sigma)\n",
    "        torch.nn.init.normal_(self.wHidden,mean=0,std=self.N_sigma)\n",
    "        torch.nn.init.normal_(self.wHiddenBias,mean=0,std=self.N_sigma)\n",
    "        \n",
    "        self.dropout = torch.nn.Dropout(p=dropprob, inplace=False) #Dropout Layer (Dropout rate= p)\n",
    "        self.max = torch.nn.MaxPool1d(3, stride=1) #Pooling layer (pooling size =3)\n",
    "    def get_weights(self):\n",
    "        ll = []\n",
    "        for layer_p in self.rnn._all_weights:\n",
    "            for p in layer_p:\n",
    "                if 'weight' in p:\n",
    "                    ll.append(self.rnn.__getattr__(p))\n",
    "        return ll\n",
    "    def forward(self,x):\n",
    "        x=x.permute(1,0,2)\n",
    "        output,_=self.rnn(x)\n",
    "        if self.RNN=='BiLSTM' or self.RNN=='BiGRU':\n",
    "            Normal_RNN=output[-1, :, :self.RNN_hidden_size]\n",
    "            Rev_RNN=output[0, :, self.RNN_hidden_size:]\n",
    "            x = torch.cat((Normal_RNN, Rev_RNN), 1)\n",
    "            x=self.dropout(x)\n",
    "        else:\n",
    "            x = output[-1, :, :]\n",
    "            x=self.dropout(x)\n",
    "        x=x @ self.wHidden + self.wHiddenBias\n",
    "        x=x.clamp(min=0)\n",
    "        x=self.dropout(x)\n",
    "        x=x @ self.wNeu + self.wNeuBias\n",
    "        return (torch.sigmoid(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f72547",
   "metadata": {},
   "source": [
    "# Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb04b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logsampler(a,b):\n",
    "        x=np.random.uniform(low=0,high=1)\n",
    "        y=10**((math.log10(b)-math.log10(a))*x + math.log10(a))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521544b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Calibration(RNN,w1,w0,firsttrain_loader,firstvalid_loader,n_features,metric):\n",
    "    print('start')\n",
    "    best_AUC = 0\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    print(device)\n",
    "    learning_steps_list = [50, 100, 150, 200, 250, 300, 350, 400]\n",
    "    for number in range(40):\n",
    "        # hyper-parameters\n",
    "        RNN_hidden_size_list = [20, 50, 80, 100]\n",
    "        RNN_hidden_size = random.choice(RNN_hidden_size_list)\n",
    "        dropoutList = [0, 0.15, 0.3, 0.45]\n",
    "        dropprob = random.choice(dropoutList)\n",
    "        layer_size_list = [32, 64]\n",
    "        layer_size = random.choice(layer_size_list)\n",
    "        learning_rate_list = [10**-5,10**-4,10**-3,10**-2]\n",
    "        learning_rate=random.choice(learning_rate_list)\n",
    "        RNN_sigma = logsampler(10 ** -4, 10 ** -2)\n",
    "        N_sigma = logsampler(10 ** -4, 10 ** -2)\n",
    "        model_auc = []\n",
    "        model = Deepnet(RNN,RNN_hidden_size,RNN_sigma,layer_size,N_sigma,dropprob,n_features).to(device)\n",
    "        optimizer = torch.optim.Adam(\n",
    "                model.get_weights() + [model.wNeu, model.wNeuBias, model.wHidden, model.wHiddenBias],\n",
    "                lr=learning_rate)\n",
    "\n",
    "        learning_steps = 0\n",
    "        while learning_steps <= 400:\n",
    "            auc = []\n",
    "            model.train()\n",
    "            for i, (data, target) in enumerate(firsttrain_loader):\n",
    "\n",
    "                data = data.to(device)\n",
    "                target = target.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                output = model(data)\n",
    "                loss = F.binary_cross_entropy(output, target,weight=((torch.abs((target)) * w1) - (torch.subtract(target,1) * w0)))\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                if (learning_steps % 50 == 0):\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        model.eval()\n",
    "                        auc = []\n",
    "                        for j, (data1, target1) in enumerate(firstvalid_loader):\n",
    "                            data1 = data1.to(device)\n",
    "                            target1 = target1.to(device)\n",
    "\n",
    "                            # Forward pass\n",
    "                            output = model(data1)\n",
    "\n",
    "                            pred = output.cpu().detach().numpy().reshape(output.shape[0])\n",
    "                            labels = target1.cpu().numpy().reshape(output.shape[0])\n",
    "                            if output.shape[0] > 60:\n",
    "                                if (metric=='ROC'):\n",
    "                                    auc.append(metrics.roc_auc_score(labels, pred))\n",
    "                                elif (metric=='PRC'):\n",
    "                                    precision, recall, _ = precision_recall_curve(labels, pred)\n",
    "                                    auc.append(metrics.auc(recall, precision))\n",
    "                                else :\n",
    "                                    print('Choose proper metric')\n",
    "                                    break;\n",
    "                        model_auc.append(np.mean(auc))\n",
    "\n",
    "                        model.train()\n",
    "            learning_steps += 1\n",
    "\n",
    "        for n in range(8):\n",
    "            AUC = model_auc[n]\n",
    "            # print(AUC)\n",
    "            if AUC > best_AUC:\n",
    "                best_AUC = AUC\n",
    "                best_learning_steps = learning_steps_list[n]\n",
    "                best_LearningRate = learning_rate\n",
    "                best_RNN_hidden_size=RNN_hidden_size\n",
    "                best_dropprob = dropprob\n",
    "                best_layer_size= layer_size\n",
    "                best_RNN_sigma = RNN_sigma\n",
    "                best_N_sigma=N_sigma\n",
    "\n",
    "    print('best_AUC=', best_AUC)\n",
    "    print('best_learning_steps=', best_learning_steps)\n",
    "    print('best_LearningRate=', best_LearningRate)\n",
    "    print('best_dropprob=', best_dropprob)\n",
    "    print('best_RNN_hidden_size=', best_RNN_hidden_size)\n",
    "    print('best_layer_size=', best_layer_size)\n",
    "    print('best_RNN_sigma=', best_RNN_sigma)\n",
    "    print('best_N_sigma=', best_N_sigma)\n",
    "\n",
    "    best_hyperparameters = {'best_learning_steps': best_learning_steps, \n",
    "                            'best_LearningRate': best_LearningRate,\n",
    "                            'best_dropprob': best_dropprob, \n",
    "                            'best_RNN_hidden_size': best_RNN_hidden_size,\n",
    "                            'best_layer_size': best_layer_size, \n",
    "                            'best_RNN_sigma': best_RNN_sigma,\n",
    "                            'best_N_sigma':best_N_sigma}\n",
    "    return best_hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82e5ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_model(RNN,w1,w0,best_hyperparameters,train_loader,n_features,metric):\n",
    "    best_learning_steps=best_hyperparameters['best_learning_steps']\n",
    "    best_LearningRate=best_hyperparameters['best_LearningRate']\n",
    "    best_RNN_hidden_size=best_hyperparameters['best_RNN_hidden_size']\n",
    "    best_dropprob=best_hyperparameters['best_dropprob']\n",
    "    best_N_sigma=best_hyperparameters['best_N_sigma']\n",
    "    best_RNN_sigma=best_hyperparameters['best_RNN_sigma']\n",
    "    best_layer_size=best_hyperparameters['best_layer_size']\n",
    "    best_AUC=0\n",
    "    best_threshold=0.5\n",
    "    for number_models in range(5):\n",
    "        model = Deepnet(RNN,best_RNN_hidden_size,best_RNN_sigma,best_layer_size,best_N_sigma,best_dropprob,n_features).to(device)\n",
    "        optimizer = torch.optim.Adam(\n",
    "                model.get_weights() + [model.wNeu, model.wNeuBias, model.wHidden, model.wHiddenBias],\n",
    "                lr=best_LearningRate)\n",
    "\n",
    "        learning_steps=0\n",
    "        model.train()\n",
    "        while learning_steps<=best_learning_steps:\n",
    "            \n",
    "            for i, (data, target) in enumerate(train_loader):\n",
    "                data = data.to(device)\n",
    "                target = target.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                output = model(data)\n",
    "                loss = F.binary_cross_entropy(output, target,weight=((torch.abs((target)) * w1) - (torch.subtract(target,1) * w0)))\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            learning_steps+=1\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            auc=[]\n",
    "            threshold=[]\n",
    "            for i, (data, target) in enumerate(train_loader):\n",
    "                data = data.to(device)\n",
    "                target = target.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                output = model(data)\n",
    "\n",
    "                pred=output.cpu().detach().numpy().reshape(output.shape[0])\n",
    "                labels=target.cpu().numpy().reshape(output.shape[0])\n",
    "                if output.shape[0]>60:\n",
    "                    if (metric=='ROC'):\n",
    "                        auc.append(metrics.roc_auc_score(labels, pred))\n",
    "                        fpr, tpr, thresholds = roc_curve(labels, pred)\n",
    "                        gmeans=np.sqrt(tpr*(1-fpr))\n",
    "                    elif (metric=='PRC'):\n",
    "                        precision, recall, thresholds = precision_recall_curve(labels, pred)\n",
    "                        auc.append(metrics.auc(recall, precision))\n",
    "                        gmeans = (2*precision*recall)/(precision+recall)\n",
    "                    else :\n",
    "                        print('Choose proper metric')\n",
    "                        break;\n",
    "                    ix = np.argmax(gmeans)\n",
    "                    threshold.append(thresholds[ix])\n",
    "            #             \n",
    "            AUC_training=np.mean(auc)\n",
    "            print('AUC on training data for model ',number_models+1,' = ',AUC_training)\n",
    "            if AUC_training>best_AUC:\n",
    "                best_AUC=AUC_training\n",
    "                best_threshold=np.mean(threshold)\n",
    "                best_model=model\n",
    "    return best_model,best_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63aa0be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_predict(best_model,test_loader):\n",
    "\n",
    "    with torch.no_grad():\n",
    "        best_model.eval()\n",
    "        auc = []\n",
    "\n",
    "        for i, (data, target) in enumerate(test_loader):\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            output = best_model(data)\n",
    "            pred = output.cpu().detach().numpy().reshape(output.shape[0])\n",
    "            myprob = \"\\n\".join(map(str, pred[:]))\n",
    "            labels = target.cpu().numpy().reshape(output.shape[0])\n",
    "            if output.shape[0] > 50:\n",
    "                auc.append(metrics.roc_auc_score(labels, pred))\n",
    "\n",
    "        AUC_test = np.mean(auc)\n",
    "    return (labels,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa32a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_performance=True\n",
    "\n",
    "# Product data path\n",
    "path=\"./gen data/\"\n",
    "product=\"Prd1\"\n",
    "DATA_Micro=pd.read_csv(path+product+\".csv\", sep=';',encoding = \"ISO-8859-1\")\n",
    "\n",
    "# choose a number of time steps\n",
    "Nout=1\n",
    "Nin=1\n",
    "metric='ROC'\n",
    "# Data process\n",
    "n_steps_in, n_steps_out = Nin, Nout\n",
    "firsttrain,firstvalid,train_data,test_data,alldata=Convert_format1(DATA_Micro,n_steps_in,n_steps_out)\n",
    "n_features=firsttrain[0][0][0].shape[0]\n",
    "firsttrain_dataset=dataset_load(firsttrain)\n",
    "firstvalid_dataset=dataset_load(firstvalid)\n",
    "train_dataset=dataset_load(train_data)\n",
    "test_dataset=dataset_load(test_data)\n",
    "all_dataset=dataset_load(alldata)\n",
    "batch_size=64\n",
    "firsttrain_loader = DataLoader(dataset=firsttrain_dataset,batch_size=batch_size,shuffle=False)\n",
    "firstvalid_loader = DataLoader(dataset=firstvalid_dataset,batch_size=batch_size,shuffle=False)\n",
    "train_loader = DataLoader(dataset=train_dataset,batch_size=batch_size,shuffle=False)\n",
    "test_loader = DataLoader(dataset=test_dataset,batch_size=batch_size,shuffle=False)\n",
    "alldata_loader = DataLoader(dataset=all_dataset,batch_size=10000,shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb81855",
   "metadata": {},
   "source": [
    "# Generate Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6636a944",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hyperparameters=Calibration(RNN,w1,w0,firsttrain_loader,firstvalid_loader,n_features,metric)\n",
    "best_model,best_threshold=Train_model(RNN,w1,w0,best_hyperparameters,train_loader,n_features,metric)\n",
    "labels,pred=test_predict(best_model,test_loader)\n",
    "prediction_test=np.where(pred<best_threshold,0,pred)\n",
    "prediction_test=np.where(prediction_test>best_threshold,1,prediction_test)\n",
    "labels_train,pred_train=train_predict(best_model,alldata_loader)\n",
    "prediction_train=np.where(pred_train<best_threshold,0,pred_train)\n",
    "prediction_train=np.where(prediction_train>best_threshold,1,prediction_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b432c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.5)\n",
    "cf = confusion_matrix(labels_train,prediction_train)\n",
    "Matrix_labels = [\"TN\",\"FP\",\"FN\",\"TP\"]\n",
    "categories = [\"N\", \"P\"]\n",
    "make_confusion_matrix(cf,labels_train,pred_train, group_names=Matrix_labels,categories=categories, cmap=\"binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aeb4333",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.5)\n",
    "cf = confusion_matrix(labels,prediction_test)\n",
    "Matrix_labels = [\"TN\",\"FP\",\"FN\",\"TP\"]\n",
    "categories = [\"N\", \"P\"]\n",
    "make_confusion_matrix(cf,labels,pred, group_names=Matrix_labels,categories=categories, cmap=\"binary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f2acb9",
   "metadata": {},
   "source": [
    "# generate results for different w1 and save them in a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea70934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model process\n",
    "header = ['product', 'w1','LSTM', 'BiLSTM', 'GRU','BiGRU']\n",
    "RNN_list=['LSTM','BiLSTM','GRU','BiGRU']\n",
    "w1_list=[1,DATA_Micro['ZNZDemand'].value_counts()[0]/DATA_Micro['ZNZDemand'].value_counts()[1]/2,DATA_Micro['ZNZDemand'].value_counts()[0]/DATA_Micro['ZNZDemand'].value_counts()[1]]\n",
    "w0=1\n",
    "for w1 in w1_list:\n",
    "    AUC_list=[]\n",
    "    for RNN in RNN_list:\n",
    "        best_hyperparameters=Calibration(RNN,w1,w0,firsttrain_loader,firstvalid_loader,n_features,metric)\n",
    "        best_model,best_threshold=Train_model(RNN,w1,w0,best_hyperparameters,train_loader,n_features,metric)\n",
    "        auc=test_predict(best_model,test_loader)\n",
    "        AUC_list.append(auc)\n",
    "    dicti={\n",
    "    'product':product,\n",
    "    'w1':w1,\n",
    "    'LSTM':AUC_list[0],\n",
    "    'BiLSTM':AUC_list[1],\n",
    "    'GRU':AUC_list[2],\n",
    "    'BiGRU':AUC_list[3]\n",
    "    }\n",
    "    # saving results\n",
    "    try:\n",
    "        df=pd.read_csv('Bi_LSTM_GRU.csv')\n",
    "        new=False\n",
    "    except:\n",
    "        new=True\n",
    "    if new:\n",
    "        with open('Bi_LSTM_GRU.csv','w') as fd:\n",
    "            writer = csv.writer(fd)\n",
    "            writer.writerow(header)\n",
    "            writer = csv.DictWriter(fd, fieldnames=header)\n",
    "            writer.writerow(dicti)\n",
    "    else:\n",
    "        with open('Bi_LSTM_GRU.csv','a',newline='') as fd:\n",
    "            writer = csv.DictWriter(fd, fieldnames=header)\n",
    "            writer.writerow(dicti)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
