{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "529ded27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda\\envs\\Deeplearning\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import math\n",
    "import pandas as pd\n",
    "import random\n",
    "import gzip\n",
    "import torch\n",
    "from sklearn import metrics\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "import gensim\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import argparse\n",
    "import warnings\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict\n",
    "import csv\n",
    "from statsmodels.tsa.stattools import acf\n",
    "from numpy import array\n",
    "from numpy import hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20d03534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5055696",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75914a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split a multivariate sequence into samples\n",
    "def Convert_format(data, n_steps_in):\n",
    "    n0=np.where(data['PeriodsSepLastTwoNnZeroDemands']>0)[0][0]\n",
    "    n1=n0+int((data.shape[0]-n0)*90/100)\n",
    "    in_seq1=data['ZNZDemand'].values[n0:].reshape((-1, 1))\n",
    "    in_seq2=data['LastQty'].values[n0:].reshape((-1, 1))\n",
    "    in_seq3=data['WeekDay'].values[n0:].reshape((-1, 1))\n",
    "    in_seq4=data['Interval'].values[n0:].reshape((-1, 1))\n",
    "    in_seq5=data['PeriodsSepLastTwoNnZeroDemands'].values[n0:].reshape((-1, 1))\n",
    "    in_seq6=data['Month'].values[n0:].reshape((-1, 1))\n",
    "    # horizontally stack columns\n",
    "    dataset = hstack((in_seq1, in_seq2,in_seq3,in_seq4,in_seq5,in_seq6))\n",
    "    X = []\n",
    "    for i in range(len(dataset)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps_in\n",
    "        out_end_ix = end_ix + 1\n",
    "        # check if we are beyond the dataset\n",
    "        if (out_end_ix-1) > len(dataset):\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = list(dataset[i:end_ix,1:]), list(dataset[end_ix-1:out_end_ix-1, 0])\n",
    "        X.append([seq_x,seq_y])\n",
    "    size=int((n1) / 5)\n",
    "    train_data=X[:n1]\n",
    "    test_data=X[n1:]\n",
    "    calib_data = X[:4*size]\n",
    "    valid_data = X[4*size:n1]\n",
    "    return calib_data,valid_data,train_data,test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19e3848b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset_load(Dataset):\n",
    "    def __init__(self,xy=None):\n",
    "        self.x_data=np.asarray([el[0] for el in xy],dtype=np.float32)\n",
    "        self.y_data =np.asarray([el[1] for el in xy ],dtype=np.float32)\n",
    "        self.x_data = torch.from_numpy(self.x_data)\n",
    "        self.y_data = torch.from_numpy(self.y_data)\n",
    "        self.len=len(self.x_data)\n",
    "      \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "209fa228",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_confusion_matrix(cf,labels,pred,group_names=None,categories='auto',count=True,percent=True,cbar=True,xyticks=True,xyplotlabels=True,sum_stats=True,figsize=None,cmap='Blues',title=None):\n",
    "    blanks = ['' for i in range(cf.size)]\n",
    "\n",
    "    if group_names and len(group_names)==cf.size:\n",
    "        group_labels = [\"{}\\n\".format(value) for value in group_names]\n",
    "    else:\n",
    "        group_labels = blanks\n",
    "\n",
    "    if count:\n",
    "        group_counts = [\"{0:0.0f}\\n\".format(value) for value in cf.flatten()]\n",
    "    else:\n",
    "        group_counts = blanks\n",
    "\n",
    "    if percent:\n",
    "        group_percentages = [\"{0:.2%}\".format(value) for value in cf.flatten()/np.sum(cf)]\n",
    "    else:\n",
    "        group_percentages = blanks\n",
    "\n",
    "    box_labels = [f\"{v1}{v2}{v3}\".strip() for v1, v2, v3 in zip(group_labels,group_counts,group_percentages)]\n",
    "    box_labels = np.asarray(box_labels).reshape(cf.shape[0],cf.shape[1])\n",
    "\n",
    "\n",
    "    # CODE TO GENERATE SUMMARY STATISTICS & TEXT FOR SUMMARY STATS\n",
    "    if sum_stats:\n",
    "        #if it is a binary confusion matrix, show some more stats\n",
    "        if len(cf)==2:\n",
    "            #Metrics for Binary Confusion Matrices\n",
    "            precision, recall, thresholds = precision_recall_curve(labels, pred)\n",
    "            Auc_PRC=metrics.auc(recall, precision)\n",
    "            Auc_ROC=metrics.roc_auc_score(labels, pred)\n",
    "            stats_text = \"\\n\\nAU_PRC={:0.3f}\\nAU_ROC={:0.3f}\".format(Auc_PRC,Auc_ROC)\n",
    "        else:\n",
    "            stats_text = \"\\n\\nAccuracy={:0.3f}\".format(accuracy)\n",
    "    else:\n",
    "        stats_text = \"\"\n",
    "\n",
    "\n",
    "    # SET FIGURE PARAMETERS ACCORDING TO OTHER ARGUMENTS\n",
    "    if figsize==None:\n",
    "        #Get default figure size if not set\n",
    "        figsize = plt.rcParams.get('figure.figsize')\n",
    "\n",
    "    if xyticks==False:\n",
    "        #Do not show categories if xyticks is False\n",
    "        categories=False\n",
    "\n",
    "\n",
    "    # MAKE THE HEATMAP VISUALIZATION\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.heatmap(cf,annot=box_labels,fmt=\"\",cmap=cmap,cbar=cbar,xticklabels=categories,yticklabels=categories)\n",
    "\n",
    "    if xyplotlabels:\n",
    "        plt.ylabel('True label')\n",
    "        plt.xlabel('Predicted label' + stats_text)\n",
    "    else:\n",
    "        plt.xlabel(stats_text)\n",
    "    \n",
    "    if title:\n",
    "        plt.title(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6252a640",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b483d9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Deepnet(nn.Module):\n",
    "    def __init__ (self,RNN,RNN_hidden_size,RNN_mean,RNN_sigma,layer_size,dropprob,n_features,Nout=1):\n",
    "        super(Deepnet,self).__init__()\n",
    "        self.RNN=RNN\n",
    "        self.RNN_hidden_size=RNN_hidden_size\n",
    "        self.RNN_sigma=RNN_sigma   \n",
    "        self.layer_size=layer_size\n",
    "        self.RNN_mean=RNN_mean   \n",
    "        self.input_channels=n_features\n",
    "        self.dropprob=dropprob\n",
    "        self.out=Nout\n",
    "        if self.RNN=='LSTM':\n",
    "            self.rnn = nn.LSTM(self.input_channels, RNN_hidden_size, num_layers=1, bidirectional=False).to(device)\n",
    "            self.FC_size= RNN_hidden_size\n",
    "        elif self.RNN=='BiLSTM':\n",
    "            self.rnn = nn.LSTM(self.input_channels, RNN_hidden_size, num_layers=1, bidirectional=True).to(device)\n",
    "            self.FC_size= 2*RNN_hidden_size\n",
    "        elif self.RNN=='GRU':\n",
    "            self.rnn = nn.GRU(self.input_channels, RNN_hidden_size, num_layers=1, bidirectional=False).to(device)\n",
    "            self.FC_size= RNN_hidden_size\n",
    "        elif self.RNN=='BiGRU':\n",
    "            self.rnn = nn.GRU(self.input_channels, RNN_hidden_size, num_layers=1, bidirectional=True).to(device)\n",
    "            self.FC_size= 2*RNN_hidden_size\n",
    "        for layer_p in self.rnn._all_weights:\n",
    "            for p in layer_p:\n",
    "                if 'weight' in p:\n",
    "                    torch.nn.init.normal_(self.rnn.__getattr__(p),mean=RNN_mean,std=RNN_sigma)  \n",
    "        self.dropout = torch.nn.Dropout(p=dropprob, inplace=False) #Dropout Layer (Dropout rate= p)\n",
    "        #weights between LSTM or GRU layers and fully connected layer\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.FC_size,self.layer_size),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=dropprob, inplace=False),\n",
    "            nn.Linear(self.layer_size, self.out),\n",
    "            nn.Sigmoid())\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x=x.permute(1,0,2)\n",
    "        output,_=self.rnn(x)\n",
    "        if self.RNN=='BiLSTM' or self.RNN=='BiGRU':\n",
    "            Normal_RNN=output[-1, :, :self.RNN_hidden_size]\n",
    "            Rev_RNN=output[0, :, self.RNN_hidden_size:]\n",
    "            x = torch.cat((Normal_RNN, Rev_RNN), 1)\n",
    "            x=self.dropout(x)\n",
    "        else:\n",
    "            x = output[-1, :, :]\n",
    "            x=self.dropout(x)\n",
    "        x = self.classifier(x)\n",
    "        return (x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f72547",
   "metadata": {},
   "source": [
    "# Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1cb04b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logsampler(a,b):\n",
    "        x=np.random.uniform(low=0,high=1)\n",
    "        y=10**((math.log10(b)-math.log10(a))*x + math.log10(a))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "521544b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Calibration(RNN,w1,w0,calib_loader,valid_loader,n_features,metric):\n",
    "    best_AUC = 0\n",
    "    if verbose:\n",
    "        print('Training on ',device)\n",
    "    max_learning_steps = 500\n",
    "    RNN_hidden_size_list = [20, 50, 80, 100]\n",
    "    dropoutList = [0, 0.15, 0.3, 0.45]\n",
    "    layer_size_list = [32, 64]\n",
    "    learning_rate_list = [10**-5,10**-4,10**-3,10**-2]\n",
    "    max_num_models = 40\n",
    "    for number in range(max_num_models):\n",
    "        if verbose:\n",
    "            print('model {0} out of {1}'.format(number+1,max_num_models))\n",
    "        # hyper-parameters\n",
    "        RNN_hidden_size = random.choice(RNN_hidden_size_list)\n",
    "        dropprob = random.choice(dropoutList)\n",
    "        layer_size = random.choice(layer_size_list)\n",
    "        learning_rate=random.choice(learning_rate_list)\n",
    "        RNN_mean = random.choice([-1,1])*logsampler(10 ** -4, 1)\n",
    "        RNN_sigma = logsampler(10 ** -4, 10 ** -2)\n",
    "        model = Deepnet(RNN,RNN_hidden_size,RNN_mean,RNN_sigma,layer_size,dropprob,n_features).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
    "        learning_steps = 0\n",
    "        while learning_steps < max_learning_steps:\n",
    "            loss_per_epoch = 0\n",
    "            model.train()\n",
    "            for batch_idx, (data1, target1, _, _) in enumerate(calib_loader):\n",
    "                data = data1.to(device)\n",
    "                target = target1.to(device)\n",
    "                # Forward pass\n",
    "                outputs = model(data)\n",
    "                criterion = nn.BCELoss(reduction='mean',weight=((torch.abs((target)) * w1) - (torch.subtract(target,1) * w0)))\n",
    "                loss = criterion(outputs, target)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                loss_per_epoch+=loss.item()\n",
    "            learning_steps+=1\n",
    "            if (learning_steps % 100 == 0):\n",
    "                if verbose:\n",
    "                    print(\"model trained for {0} epochs, loss per epoch is {1}.\".format(learning_steps,loss_per_epoch/(batch_idx+1)))\n",
    "                with torch.no_grad():\n",
    "                    model.eval()\n",
    "                    auc = []\n",
    "                    for batch_idx, (data1, target1, _, _) in enumerate(valid_loader):\n",
    "                        data = data1.to(device)\n",
    "                        target = target1.to(device)\n",
    "                        # Forward pass\n",
    "                        output = model(data)\n",
    "                        pred = output.cpu().detach().numpy().reshape(output.shape[0])\n",
    "                        labels = target.cpu().numpy().reshape(output.shape[0])\n",
    "                        if output.shape[0] > 60:\n",
    "                            if (metric=='ROC'):\n",
    "                                auc.append(metrics.roc_auc_score(labels, pred))\n",
    "                            elif (metric=='PRC'):\n",
    "                                precision, recall, _ = precision_recall_curve(labels, pred)\n",
    "                                auc.append(metrics.auc(recall, precision))\n",
    "                            else :\n",
    "                                print('Choose proper metric')\n",
    "                    AUC = np.mean(auc)\n",
    "                    # print(AUC)\n",
    "                    if AUC > best_AUC:\n",
    "                        best_AUC = AUC\n",
    "                        best_learning_steps = learning_steps\n",
    "                        best_LearningRate = learning_rate\n",
    "                        best_RNN_hidden_size=RNN_hidden_size\n",
    "                        best_dropprob = dropprob\n",
    "                        best_layer_size= layer_size\n",
    "                        best_RNN_sigma = RNN_sigma\n",
    "                        best_RNN_mean = RNN_mean\n",
    "    if verbose:\n",
    "        print('best_AUC=', best_AUC)\n",
    "        print('best_learning_steps=', best_learning_steps)\n",
    "        print('best_LearningRate=', best_LearningRate)\n",
    "        print('best_dropprob=', best_dropprob)\n",
    "        print('best_RNN_hidden_size=', best_RNN_hidden_size)\n",
    "        print('best_layer_size=', best_layer_size)\n",
    "        print('best_RNN_sigma=', best_RNN_sigma)\n",
    "        print('best_RNN_mean=', best_RNN_mean)\n",
    "    best_hyperparameters = {'best_learning_steps': best_learning_steps, \n",
    "                            'best_LearningRate': best_LearningRate,\n",
    "                            'best_dropprob': best_dropprob, \n",
    "                            'best_RNN_hidden_size': best_RNN_hidden_size,\n",
    "                            'best_layer_size': best_layer_size, \n",
    "                            'best_RNN_sigma': best_RNN_sigma,\n",
    "                            'best_RNN_mean':best_RNN_mean}\n",
    "    return best_hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d82e5ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_model(RNN,w1,w0,best_hyperparameters,train_loader,n_features,metric):\n",
    "    best_learning_steps=8#best_hyperparameters['best_learning_steps']\n",
    "    best_LearningRate=best_hyperparameters['best_LearningRate']\n",
    "    best_RNN_hidden_size=best_hyperparameters['best_RNN_hidden_size']\n",
    "    best_dropprob=best_hyperparameters['best_dropprob']\n",
    "    best_RNN_mean=best_hyperparameters['best_RNN_mean']\n",
    "    best_RNN_sigma=best_hyperparameters['best_RNN_sigma']\n",
    "    best_layer_size=best_hyperparameters['best_layer_size']\n",
    "    best_AUC=0\n",
    "    best_threshold=0.5\n",
    "    for number_models in range(5):\n",
    "        model = Deepnet(RNN,best_RNN_hidden_size,best_RNN_mean,best_RNN_sigma,best_layer_size,best_dropprob,n_features).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(),lr=best_LearningRate)\n",
    "        learning_steps=0\n",
    "        model.train()\n",
    "        while learning_steps<best_learning_steps:\n",
    "            for batch_idx, (data1, target1, _, _) in enumerate(train_loader):\n",
    "                data = data1.to(device)\n",
    "                target = target1.to(device)\n",
    "                outputs = model(data)\n",
    "                criterion = nn.BCELoss(reduction='mean',weight=((torch.abs((target)) * w1) - (torch.subtract(target,1) * w0)))\n",
    "                loss = criterion(outputs, target)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            learning_steps+=1\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            auc=[]\n",
    "            threshold=[]\n",
    "            for batch_idx, (data1, target1, _, _) in enumerate(train_loader):\n",
    "                data = data1.to(device)\n",
    "                target = target1.to(device)\n",
    "                # Forward pass\n",
    "                output = model(data)\n",
    "                pred = output.cpu().detach().numpy().reshape(output.shape[0])\n",
    "                labels=target.cpu().numpy().reshape(output.shape[0])\n",
    "                if pred.shape[0]>60:\n",
    "                    if (metric=='ROC'):\n",
    "                        auc.append(metrics.roc_auc_score(labels, pred))\n",
    "                        fpr, tpr, thresholds = roc_curve(labels, pred)\n",
    "                        gmeans=np.sqrt(tpr*(1-fpr))\n",
    "                        threshold.append(thresholds[np.argmax(gmeans)])\n",
    "                    elif (metric=='PRC'):\n",
    "                        precision, recall, thresholds = precision_recall_curve(labels, pred)\n",
    "                        auc.append(metrics.auc(recall, precision))\n",
    "                        gmeans=(2*precision*recall)/(precision+recall)\n",
    "                        threshold.append(thresholds[np.argmax(gmeans)])\n",
    "                    else :\n",
    "                        print('Choose proper metric') \n",
    "            AUC_training=np.mean(auc)\n",
    "            if verbose:\n",
    "                print('AUC on training data for model ',number_models+1,' = ',AUC_training)\n",
    "            if AUC_training>best_AUC:\n",
    "                best_AUC=AUC_training\n",
    "                best_threshold=np.mean(np.array(threshold),axis=0)\n",
    "                best_model=model\n",
    "    return best_model,best_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63aa0be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_predict(best_model,test_loader):\n",
    "\n",
    "    with torch.no_grad():\n",
    "        best_model.eval()\n",
    "        auc = []\n",
    "\n",
    "        for batch_idx, (data, target) in enumerate(test_loader):\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            output = best_model(data)\n",
    "            pred = output.cpu().detach().numpy().reshape(output.shape[0])\n",
    "            labels = target.cpu().numpy().reshape(output.shape[0])\n",
    "            if output.shape[0] > 50:\n",
    "                auc.append(metrics.roc_auc_score(labels, pred))\n",
    "\n",
    "        AUC_test = np.mean(auc)\n",
    "    return (labels,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa32a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_performance=True\n",
    "\n",
    "# Product data path\n",
    "path=\"./gen data/\"\n",
    "product=\"KS807 SOLUTION NETTOYAGE ECHANTILLONS\"\n",
    "DATA=pd.read_csv(path+product+\".csv\", sep=';',encoding = \"ISO-8859-1\")\n",
    "\n",
    "# choose a number of time steps\n",
    "Nout=1\n",
    "Nin=1\n",
    "metric='ROC'\n",
    "# Data process\n",
    "n_steps_in, n_steps_out = Nin, Nout\n",
    "calib_data,valid_data,train_data,test_data=Convert_format(DATA,n_steps_in)\n",
    "n_features=calib_data[0][0][0].shape[0]\n",
    "batch_size=64\n",
    "calib_dataset=dataset_load(calib_data)\n",
    "valid_dataset=dataset_load(valid_data)\n",
    "train_dataset=dataset_load(train_data)\n",
    "test_dataset=dataset_load(test_data)\n",
    "calib_loader = DataLoader(dataset=calib_dataset,batch_size=batch_size,shuffle=False)\n",
    "valid_loader = DataLoader(dataset=valid_dataset,batch_size=batch_size,shuffle=False)\n",
    "train_loader = DataLoader(dataset=train_dataset,batch_size=batch_size,shuffle=False)\n",
    "test_loader = DataLoader(dataset=test_dataset,batch_size=batch_size,shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb81855",
   "metadata": {},
   "source": [
    "# Generate Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6636a944",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hyperparameters=Calibration(RNN,w1,w0,firsttrain_loader,firstvalid_loader,n_features,metric)\n",
    "best_model,best_threshold=Train_model(RNN,w1,w0,best_hyperparameters,train_loader,n_features,metric)\n",
    "labels,pred=test_predict(best_model,test_loader)\n",
    "prediction_test=np.where(pred<best_threshold,0,pred)\n",
    "prediction_test=np.where(prediction_test>best_threshold,1,prediction_test)\n",
    "labels_train,pred_train=train_predict(best_model,alldata_loader)\n",
    "prediction_train=np.where(pred_train<best_threshold,0,pred_train)\n",
    "prediction_train=np.where(prediction_train>best_threshold,1,prediction_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b432c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.5)\n",
    "cf = confusion_matrix(labels_train,prediction_train)\n",
    "Matrix_labels = [\"TN\",\"FP\",\"FN\",\"TP\"]\n",
    "categories = [\"N\", \"P\"]\n",
    "make_confusion_matrix(cf,labels_train,pred_train, group_names=Matrix_labels,categories=categories, cmap=\"binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aeb4333",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.5)\n",
    "cf = confusion_matrix(labels,prediction_test)\n",
    "Matrix_labels = [\"TN\",\"FP\",\"FN\",\"TP\"]\n",
    "categories = [\"N\", \"P\"]\n",
    "make_confusion_matrix(cf,labels,pred, group_names=Matrix_labels,categories=categories, cmap=\"binary\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "59f2acb9",
   "metadata": {},
   "source": [
    "# generate and save, in csv file, results for different w1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea70934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model process\n",
    "header = ['product', 'w1','LSTM', 'BiLSTM', 'GRU','BiGRU']\n",
    "RNN_list=['LSTM','BiLSTM','GRU','BiGRU']\n",
    "w1_list=[1,DATA_Micro['ZNZDemand'].value_counts()[0]/DATA_Micro['ZNZDemand'].value_counts()[1]/2,DATA_Micro['ZNZDemand'].value_counts()[0]/DATA_Micro['ZNZDemand'].value_counts()[1]]\n",
    "w0=1\n",
    "for w1 in w1_list:\n",
    "    AUC_list=[]\n",
    "    for RNN in RNN_list:\n",
    "        best_hyperparameters=Calibration(RNN,w1,w0,firsttrain_loader,firstvalid_loader,n_features,metric)\n",
    "        best_model,best_threshold=Train_model(RNN,w1,w0,best_hyperparameters,train_loader,n_features,metric)\n",
    "        auc=test_predict(best_model,test_loader)\n",
    "        AUC_list.append(auc)\n",
    "    dicti={\n",
    "    'product':product,\n",
    "    'w1':w1,\n",
    "    'LSTM':AUC_list[0],\n",
    "    'BiLSTM':AUC_list[1],\n",
    "    'GRU':AUC_list[2],\n",
    "    'BiGRU':AUC_list[3]\n",
    "    }\n",
    "    # saving results\n",
    "    try:\n",
    "        df=pd.read_csv('Bi_LSTM_GRU.csv')\n",
    "        new=False\n",
    "    except:\n",
    "        new=True\n",
    "    if new:\n",
    "        with open('Bi_LSTM_GRU.csv','w') as fd:\n",
    "            writer = csv.writer(fd)\n",
    "            writer.writerow(header)\n",
    "            writer = csv.DictWriter(fd, fieldnames=header)\n",
    "            writer.writerow(dicti)\n",
    "    else:\n",
    "        with open('Bi_LSTM_GRU.csv','a',newline='') as fd:\n",
    "            writer = csv.DictWriter(fd, fieldnames=header)\n",
    "            writer.writerow(dicti)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "78e58e174bbf6f139d99cb175ce740dfc54e8c53f1ce58a8848962c143d22910"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
