{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b09e58e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\envs\\Deeplearning\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import math\n",
    "import pandas as pd\n",
    "import random\n",
    "import gzip\n",
    "import torch\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import warnings\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import make_scorer,balanced_accuracy_score,roc_curve,precision_recall_curve,mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from statsmodels.tsa.stattools import acf\n",
    "from numpy import hstack\n",
    "import time\n",
    "from torchsampler import ImbalancedDatasetSampler\n",
    "from einops.layers.torch import Rearrange\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4d8c02",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ac88bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class datasets():\n",
    "    def __init__(self,data,n_steps_in):\n",
    "        self.data=data\n",
    "        self.n_steps_in=n_steps_in\n",
    "        self.n0 = np.where(data['PeriodsSepLastTwoNnZeroDemands']>0)[0][0]\n",
    "        self.n1 = int((data.shape[0]-self.n0)*9/10) #self.n0+\n",
    "        self.calib_size = int(self.n1*9/10)\n",
    "        in_seq1=data['Qty'].values[self.n0:].reshape((-1, 1))\n",
    "        in_seq2=data['Month'].values[self.n0:].reshape((-1, 1))\n",
    "        in_seq3=data['WeekDay'].values[self.n0:].reshape((-1, 1))\n",
    "        in_seq4=data['LastQty'].values[self.n0:].reshape((-1, 1))\n",
    "        in_seq5=data['Interval'].values[self.n0:].reshape((-1, 1))\n",
    "        in_seq6=data['PeriodsSepLastTwoNnZeroDemands'].values[self.n0:].reshape((-1, 1))\n",
    "        in_seq7=data['ZNZDemand'].values[self.n0:].reshape((-1, 1))\n",
    "        self.dataset = hstack((in_seq1, in_seq2,in_seq3,in_seq4,in_seq5,in_seq6,in_seq7))\n",
    "        if np.count_nonzero(np.isnan(self.dataset))>0:\n",
    "            print('nan found')\n",
    "        self.scaler = MinMaxScaler(feature_range = (0, 1))\n",
    "        self.scaler.fit(data[['Qty']].values[self.n0:self.n1])\n",
    "        #random.shuffle(self.dataset)\n",
    "        \n",
    "    def get_deep2net_datasets(self):\n",
    "        X = []\n",
    "        for i in range(len(self.dataset)):\n",
    "            # find the end of this pattern\n",
    "            end_ix = i + self.n_steps_in\n",
    "            # check if we are beyond the dataset\n",
    "            if (end_ix) > len(self.dataset):\n",
    "                break\n",
    "            # gather input and output parts of the pattern\n",
    "            seq_x1, seq_y1 = list(self.dataset[i:end_ix,1:-1]), list(self.dataset[end_ix-1:end_ix, -1])\n",
    "            seq_x2, seq_y2 = [self.dataset[i+self.n_steps_in-1:i+self.n_steps_in,3],self.dataset[i+self.n_steps_in-1:i+self.n_steps_in,4]], list(self.dataset[i+self.n_steps_in-1:i+self.n_steps_in,0])\n",
    "            X.append([seq_x1, seq_y1, seq_x2, seq_y2])\n",
    "        train_data=X[:self.n1]\n",
    "        test_data=X[self.n1:]\n",
    "        calib_data = X[:self.calib_size]\n",
    "        valid_data = X[self.calib_size:self.n1]\n",
    "        #random.shuffle(calib_data)\n",
    "        #random.shuffle(valid_data)\n",
    "        #random.shuffle(train_data)\n",
    "        #random.shuffle(test_data)\n",
    "        return calib_data,valid_data,train_data,test_data\n",
    "    \n",
    "    def get_reg_datasets(self):\n",
    "        X = []\n",
    "        dataset1=self.dataset.copy()\n",
    "        dataset1[:,0]=self.scaler.transform(np.array(dataset1[:,0]).reshape(len(dataset1[:,0]),1)).ravel()\n",
    "        for i in range(self.n_steps_in-1,len(dataset1)):\n",
    "            # gather input and output parts of the pattern\n",
    "            seq_x, seq_y = dataset1[i:i+1,1:-1], dataset1[i:i+1, 0]\n",
    "            X.append([seq_x,seq_y])\n",
    "        train_data=X[:self.n1]\n",
    "        test_data=X[self.n1:]\n",
    "        calib_data = X[:self.calib_size]\n",
    "        valid_data = X[self.calib_size:self.n1]\n",
    "        return calib_data,valid_data,train_data,test_data,self.scaler\n",
    "    \n",
    "    def get_ADI(self):\n",
    "        ADI = (self.data['ZNZDemand'].value_counts()[1]+self.data['ZNZDemand'].value_counts()[0])/self.data['ZNZDemand'].value_counts()[1]\n",
    "        return(ADI)\n",
    "    \n",
    "    def get_n0(self):\n",
    "        return self.n0+self.n_steps_in\n",
    "    \n",
    "    def get_n1(self):\n",
    "        return self.n0+self.n1+self.n_steps_in-1\n",
    "    \n",
    "    def get_CV2(self):\n",
    "        Test_df = (self.data[self.data['ZNZDemand']==1][['Qty','Interval']]).dropna(axis=0)\n",
    "        CV2 = (Test_df['Qty'].std()/Test_df['Qty'].mean())**2\n",
    "        return (CV2)\n",
    "    \n",
    "    def get_n_obs(self):\n",
    "        n_obs=self.data.shape[0]-self.n0\n",
    "        return (n_obs)\n",
    "    \n",
    "    def get_train_Y(self):\n",
    "        return (self.dataset[self.n_steps_in-1:self.n_steps_in-1+self.n1 ,0].tolist())\n",
    "    \n",
    "    def get_test_Y(self):\n",
    "        return (self.dataset[self.n1+self.n_steps_in-1:, 0].tolist())\n",
    "    \n",
    "    \n",
    "    def get_Nlags(self):\n",
    "        autocorr=acf(self.data['Qty'],False,100)\n",
    "        Nlags=np.where(autocorr==max(autocorr[(autocorr<1)]))[0][0]\n",
    "        return (Nlags)\n",
    "    \n",
    "    def get_w0(self):\n",
    "        w0 = 1\n",
    "        return w0\n",
    "    \n",
    "    def get_w1(self):\n",
    "        w1 = self.data['ZNZDemand'].value_counts()[0]/self.data['ZNZDemand'].value_counts()[1]/2\n",
    "        return (w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7f3f94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset_load(Dataset):\n",
    "    def __init__(self,xy=None):\n",
    "        self.x1_data=np.asarray([el[0] for el in xy],dtype=np.float32)\n",
    "        self.y1_data =np.asarray([el[1] for el in xy ],dtype=np.float32)\n",
    "        self.x2_data=np.asarray([el[2] for el in xy],dtype=np.float32).squeeze(axis=2)\n",
    "        self.y2_data =np.asarray([el[3] for el in xy ],dtype=np.float32)\n",
    "        self.x1_data = torch.from_numpy(self.x1_data)\n",
    "        self.y1_data = torch.from_numpy(self.y1_data)\n",
    "        self.x2_data = torch.from_numpy(self.x2_data)\n",
    "        self.y2_data = torch.from_numpy(self.y2_data)\n",
    "        self.len=len(self.x1_data)\n",
    "    def __getitem__(self, index):\n",
    "        return self.x1_data[index], self.y1_data[index], self.x2_data[index], self.y2_data[index]\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    def get_labels(self):\n",
    "        return self.y1_data.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bca65212",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset_load1(Dataset):\n",
    "    def __init__(self,xy=None):\n",
    "        self.x=np.asarray([el[0] for el in xy],dtype=np.float32)\n",
    "        self.y =np.asarray([el[1] for el in xy ],dtype=np.float32)\n",
    "        self.x = torch.from_numpy(self.x)\n",
    "        self.y = torch.from_numpy(self.y)\n",
    "        self.len=len(self.x)\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "063adbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logsampler(a,b):\n",
    "    x=np.random.uniform(low=0,high=1)\n",
    "    y=10**((math.log10(b)-math.log10(a))*x + math.log10(a))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edaab01",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a684ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MPIS(Y,F):\n",
    "    pis = 0\n",
    "    Stock = 0\n",
    "    Out_stock = 0\n",
    "    n_Out_Stock = 0\n",
    "    for i in range (0,len(F)):\n",
    "        Stock += F[i]-Y[i]\n",
    "        if Stock<0:\n",
    "            Out_stock -= Stock\n",
    "            n_Out_Stock += 1\n",
    "            Stock = 0\n",
    "        pis += Stock\n",
    "    return ((pis,Out_stock,n_Out_Stock))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf912587",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MASE(X,Y,F):\n",
    "    N1=len(X)\n",
    "    N2=len(Y)\n",
    "    D1=np.sum(abs(np.array(Y)-np.array(F)))/N2\n",
    "    D2=np.sum(abs(np.array(X[1:])-np.array(X[:-1])))/N1\n",
    "    return (D1/D2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed31e102",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSSE(X,Y,F):\n",
    "    h=len(Y)\n",
    "    n=len(X)\n",
    "    D1=(1/h)*np.sum((np.array(Y)-np.array(F))**2)\n",
    "    D2=(1/(n-1))*np.sum((np.array(X[1:])-np.array(X[:-1]))**2)\n",
    "    return (np.sqrt(D1/D2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b19bcf",
   "metadata": {},
   "source": [
    "# Network 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3554d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Deepnet(nn.Module):\n",
    "    def __init__ (self,RNN,RNN_hidden_size,RNN_mean,RNN_sigma,layer_size,dropprob,n_features,Nout=1):\n",
    "        super(Deepnet,self).__init__()\n",
    "        self.RNN=RNN\n",
    "        self.RNN_hidden_size=RNN_hidden_size\n",
    "        self.RNN_sigma=RNN_sigma   \n",
    "        self.layer_size=layer_size\n",
    "        self.RNN_mean=RNN_mean   \n",
    "        self.input_channels=n_features\n",
    "        self.dropprob=dropprob\n",
    "        self.out=Nout\n",
    "        if self.RNN=='LSTM':\n",
    "            self.rnn = nn.LSTM(self.input_channels, RNN_hidden_size, num_layers=1, bidirectional=False, batch_first=True)\n",
    "            self.FC_size= RNN_hidden_size\n",
    "        elif self.RNN=='BiLSTM':\n",
    "            self.rnn = nn.LSTM(self.input_channels, RNN_hidden_size, num_layers=1, bidirectional=True, batch_first=True)\n",
    "            self.FC_size= 2*RNN_hidden_size\n",
    "        elif self.RNN=='GRU':\n",
    "            self.rnn = nn.GRU(self.input_channels, RNN_hidden_size, num_layers=1, bidirectional=False, batch_first=True)\n",
    "            self.FC_size= RNN_hidden_size\n",
    "        elif self.RNN=='BiGRU':\n",
    "            self.rnn = nn.GRU(self.input_channels, RNN_hidden_size, num_layers=1, bidirectional=True, batch_first=True)\n",
    "            self.FC_size= 2*RNN_hidden_size\n",
    "        for layer_p in self.rnn._all_weights:\n",
    "            for p in layer_p:\n",
    "                if 'weight' in p:\n",
    "                    torch.nn.init.normal_(self.rnn.__getattr__(p),mean=RNN_mean,std=RNN_sigma)  \n",
    "        self.dropout = torch.nn.Dropout(p=dropprob, inplace=False) #Dropout Layer (Dropout rate= p)\n",
    "        self.pool_fn = Rearrange('b c (l n)-> b c l n', l=2)\n",
    "        # classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.FC_size,self.layer_size),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=dropprob, inplace=False),\n",
    "            nn.Linear(self.layer_size, self.out),\n",
    "            nn.Sigmoid())\n",
    "    \n",
    "    def forward(self,x):\n",
    "        output,_=self.rnn(x)\n",
    "        if self.RNN=='BiLSTM' or self.RNN=='BiGRU':\n",
    "            output = self.pool_fn(output)\n",
    "            Normal_RNN=output[:, -1, 0, :]\n",
    "            Rev_RNN=output[:, 0, 1, :]\n",
    "            x = torch.cat((Normal_RNN, Rev_RNN), 1)\n",
    "        else:\n",
    "            x = output[:, -1, :]\n",
    "        x = self.classifier(x)\n",
    "        return (x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2a47015",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_early_stopping(model,train_loader,valid_loader,optimizer,w0,w1,maxepochs=100,epochs_for_early_stop=0,save_model=False):\n",
    "    best_model = None\n",
    "    best_loss = np.inf\n",
    "    counter = 0\n",
    "    nepochs=0\n",
    "    valid_losses =[]\n",
    "    train_losses = []\n",
    "    while nepochs<maxepochs:\n",
    "        model.train()\n",
    "        train_loss=0\n",
    "        for batch_idx, (data, target, _, _) in enumerate(train_loader):\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            output = model(data)\n",
    "            criterion = nn.BCELoss(reduction='mean',weight=((torch.abs((target)) * w1) - (torch.subtract(target,1) * w0)))\n",
    "            loss = criterion(output, target)#\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss+=loss.item()\n",
    "        if verbose:\n",
    "            print('Model trained for {0} epochs out of {1}. Training loss is {2}'.format(nepochs+1,maxepochs,loss.item()))\n",
    "        train_losses.append(train_loss/(batch_idx+1))\n",
    "        nepochs +=1\n",
    "        if epochs_for_early_stop>0:\n",
    "            with torch.no_grad():\n",
    "                model.eval() \n",
    "                valid_loss=0\n",
    "                for batch_idx, (data, target, _, _) in enumerate(valid_loader):\n",
    "                    data = data.to(device)\n",
    "                    target = target.to(device)\n",
    "                    output = model(data)\n",
    "                    criterion = nn.BCELoss(reduction='mean',weight=((torch.abs((target)) * w1) - (torch.subtract(target,1) * w0)))\n",
    "                    loss = criterion(output, target )#\n",
    "                    valid_loss+=loss.item()\n",
    "                valid_losses.append(valid_loss/(batch_idx+1))\n",
    "                counter+=1\n",
    "                if valid_losses[-1]<best_loss:\n",
    "                    if verbose:\n",
    "                        print('Validation loss decreased from {0} to {1}'.format(best_loss,valid_losses[-1]))\n",
    "                    best_loss = valid_losses[-1]\n",
    "                    best_model = model\n",
    "                    counter = 0\n",
    "                else:\n",
    "                    if verbose:\n",
    "                        print('Counter for early stopping: {0} out of {1}'.format(counter,epochs_for_early_stop))\n",
    "                    if counter == epochs_for_early_stop:\n",
    "                        print('early stopping at epoch ', nepochs-counter)\n",
    "                        if save_model:\n",
    "                            torch.save(best_model,model_dir+'/best_model.pkl')\n",
    "                        return (best_model,nepochs-epochs_for_early_stop)\n",
    "    print('no early stopping')\n",
    "    if save_model:\n",
    "        torch.save(best_model,model_dir+'/best_model.pkl')\n",
    "    return (model,nepochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f12d5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_predict(best_model,test_loader,return_threshold=False,metric='ROC'):\n",
    "    with torch.no_grad():\n",
    "        best_model.eval()\n",
    "        roc = []\n",
    "        prc = []\n",
    "        threshold_list = []\n",
    "        for batch_idx, (data, target, _, _) in enumerate(test_loader):\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            # Forward pass\n",
    "            output = best_model(data)\n",
    "            pred = output.cpu().detach().numpy().reshape(output.shape[0])\n",
    "            labels = target.cpu().numpy().reshape(output.shape[0])\n",
    "            if output.shape[0] > 16:\n",
    "                try:\n",
    "                    roc.append(metrics.roc_auc_score(labels, pred))\n",
    "                    precision, recall, thresholds = precision_recall_curve(labels, pred)\n",
    "                    prc.append(metrics.auc(recall, precision))\n",
    "                    if return_threshold:\n",
    "                        if (metric=='ROC'):\n",
    "                            fpr, tpr, thresholds = roc_curve(labels, pred)\n",
    "                            gmeans=np.sqrt(tpr*(1-fpr))\n",
    "                            threshold_list.append(thresholds[np.argmax(gmeans)])\n",
    "                        elif (metric=='PRC'):\n",
    "                            gmeans=(2*precision*recall)/(precision+recall)\n",
    "                            threshold_list.append(thresholds[np.argmax(gmeans)])\n",
    "                except:\n",
    "                    continue\n",
    "        AUROC = np.mean(roc)\n",
    "        AUPRC = np.mean(prc)\n",
    "        if verbose:\n",
    "            print('AUROC is ',AUROC)\n",
    "            print('AUPRC is ',AUPRC)\n",
    "    if return_threshold:\n",
    "        threshold=np.mean(np.array(threshold_list),axis=0)\n",
    "        return (AUROC,AUPRC,threshold)\n",
    "    return (AUROC,AUPRC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f699e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Calibration(RNN,w1,w0,calib_loader,valid_loader,n_features,max_num_models=40,maxepochs=500,epochs_for_early_stop=50):\n",
    "    best_AUROC = 0\n",
    "    best_AUPRC = 0\n",
    "    if verbose:\n",
    "        print('Training on ',device)\n",
    "    RNN_hidden_size_list = [20, 40, 60]\n",
    "    dropoutList = [0, 0.15, 0.3]\n",
    "    layer_size_list = [32, 64]\n",
    "    learning_rate_list = [10**-5,10**-4,10**-3,10**-2]\n",
    "    for number in range(max_num_models):\n",
    "        if verbose:\n",
    "            print('model {0} out of {1}'.format(number+1,max_num_models))\n",
    "        # hyper-parameters\n",
    "        RNN_hidden_size = random.choice(RNN_hidden_size_list)\n",
    "        dropprob = random.choice(dropoutList)\n",
    "        layer_size = random.choice(layer_size_list)\n",
    "        learning_rate=random.choice(learning_rate_list)\n",
    "        RNN_mean = random.choice([-1,1])*logsampler(10 ** -4, 1)\n",
    "        RNN_sigma = logsampler(10 ** -4, 10 ** -2)\n",
    "        model = Deepnet(RNN,RNN_hidden_size,RNN_mean,RNN_sigma,layer_size,dropprob,n_features).to(device)\n",
    "        optimizer = optim.AdamW(model.parameters(),lr=learning_rate,weight_decay=1e-6)\n",
    "        best_model,learning_steps = Train_early_stopping(model,calib_loader,valid_loader,optimizer,w0,w1,maxepochs=maxepochs,\n",
    "                                                         epochs_for_early_stop=epochs_for_early_stop,save_model=False)\n",
    "        AUROC,AUPRC = test_predict(best_model,valid_loader)\n",
    "        if (AUROC > best_AUROC):\n",
    "            best_AUROC = AUROC\n",
    "            best_AUPRC = AUPRC\n",
    "            best_learning_steps = learning_steps\n",
    "            best_LearningRate = learning_rate\n",
    "            best_RNN_hidden_size=RNN_hidden_size\n",
    "            best_dropprob = dropprob\n",
    "            best_layer_size= layer_size\n",
    "            best_RNN_sigma = RNN_sigma\n",
    "            best_RNN_mean = RNN_mean\n",
    "    if verbose:\n",
    "        print('best_AUROC=', best_AUROC)\n",
    "        print('best_AUPRC=', best_AUPRC)\n",
    "        print('best_learning_steps=', best_learning_steps)\n",
    "        print('best_LearningRate=', best_LearningRate)\n",
    "        print('best_dropprob=', best_dropprob)\n",
    "        print('best_RNN_hidden_size=', best_RNN_hidden_size)\n",
    "        print('best_layer_size=', best_layer_size)\n",
    "        print('best_RNN_sigma=', best_RNN_sigma)\n",
    "        print('best_RNN_mean=', best_RNN_mean)\n",
    "    best_hyperparameters = {'best_learning_steps': best_learning_steps, \n",
    "                            'best_LearningRate': best_LearningRate,\n",
    "                            'best_dropprob': best_dropprob, \n",
    "                            'best_RNN_hidden_size': best_RNN_hidden_size,\n",
    "                            'best_layer_size': best_layer_size, \n",
    "                            'best_RNN_sigma': best_RNN_sigma,\n",
    "                            'best_RNN_mean':best_RNN_mean}\n",
    "    return best_hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b84c7539",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_model(RNN,w1,w0,best_hyperparameters,train_loader,n_features):\n",
    "    best_learning_steps=best_hyperparameters['best_learning_steps']\n",
    "    best_LearningRate=best_hyperparameters['best_LearningRate']\n",
    "    best_RNN_hidden_size=best_hyperparameters['best_RNN_hidden_size']\n",
    "    best_dropprob=best_hyperparameters['best_dropprob']\n",
    "    best_RNN_mean=best_hyperparameters['best_RNN_mean']\n",
    "    best_RNN_sigma=best_hyperparameters['best_RNN_sigma']\n",
    "    best_layer_size=best_hyperparameters['best_layer_size']\n",
    "    best_AUROC = 0\n",
    "    best_AUPRC = 0\n",
    "    best_threshold = 0.5\n",
    "    for number_models in range(5):\n",
    "        model = Deepnet(RNN,best_RNN_hidden_size,best_RNN_mean,best_RNN_sigma,best_layer_size,best_dropprob,n_features).to(device)\n",
    "        optimizer = optim.AdamW(model.parameters(),lr=best_LearningRate,weight_decay=1e-6)\n",
    "        trained_model,_ = Train_early_stopping(model,train_loader,train_loader,optimizer,w0,w1,\n",
    "                                               maxepochs=best_learning_steps,epochs_for_early_stop=0,save_model=False)\n",
    "        AUROC,AUPRC,threshold = test_predict(trained_model,train_loader,return_threshold=True)\n",
    "        if verbose:\n",
    "            print('AUROC on training data for model ',number_models+1,' = ',AUROC)\n",
    "            print('AUPRC on training data for model ',number_models+1,' = ',AUPRC)\n",
    "        if (AUROC > best_AUROC) :\n",
    "            best_AUROC = AUROC\n",
    "            best_AUPRC = AUPRC\n",
    "            best_threshold = threshold\n",
    "            best_model = trained_model\n",
    "    return best_model,best_threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e75857",
   "metadata": {},
   "source": [
    "# Network 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eecef793",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Deepnet2(nn.Module):\n",
    "    def __init__ (self,layer_number,layer_size,dropprob,n_features):\n",
    "        super(Deepnet2,self).__init__()\n",
    "                \n",
    "        self.layer_size=layer_size\n",
    "        self.layer_number=layer_number\n",
    "        self.input_channels=n_features\n",
    "        self.dropprob=dropprob\n",
    "\n",
    "        self.FC=self.layer_size\n",
    "        self.fc1=nn.Linear(self.input_channels,self.layer_size)\n",
    "        if (self.layer_number>1):\n",
    "            self.fc2=nn.Linear(self.layer_size,self.layer_size)\n",
    "        if (self.layer_number>2):\n",
    "            self.fc3=nn.Linear(self.layer_size,self.layer_size)\n",
    "        self.fc_f=nn.Linear(self.layer_size,1)\n",
    "        \n",
    "        self.dropout = torch.nn.Dropout(p=dropprob, inplace=False) #Dropout Layer (Dropout rate= p)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x=F.relu(self.fc1(x))\n",
    "        x=self.dropout(x)\n",
    "        if (self.layer_number>1):\n",
    "            x=F.relu(self.fc2(x))\n",
    "            x=self.dropout(x)\n",
    "        \n",
    "        if (self.layer_number>2):\n",
    "            x=F.relu(self.fc3(x))\n",
    "            x=self.dropout(x)\n",
    "        \n",
    "        x=F.relu(self.fc_f(x))\n",
    "\n",
    "        return (x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56c9447e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_early_stopping_2(best_model1,model,train_loader,valid_loader,optimizer,best_threshold,maxepochs=100,epochs_for_early_stop=0,save_model=False):\n",
    "    best_model = None\n",
    "    best_loss = np.inf\n",
    "    counter = 0\n",
    "    nepochs=0\n",
    "    valid_losses =[]\n",
    "    train_losses = []\n",
    "    criterion = nn.MSELoss()\n",
    "    while nepochs<maxepochs:\n",
    "        model.train()\n",
    "        train_loss=0\n",
    "        for batch_idx, (data1, target1, data2, target2) in enumerate(train_loader):\n",
    "            data = data1.to(device)\n",
    "            target = target1.to(device)\n",
    "            # Forward pass\n",
    "            ZNZ_output = best_model1(data).cpu().detach().numpy()\n",
    "            ZNZ_output=np.where(ZNZ_output<best_threshold,0,ZNZ_output)\n",
    "            ZNZ_output=np.where(ZNZ_output>best_threshold,1,ZNZ_output)\n",
    "            data2 = torch.hstack((data2,torch.tensor(ZNZ_output)))\n",
    "            data = data2.to(device)\n",
    "            target = target2.to(device)\n",
    "            # Forward pass\n",
    "            output = model(data)\n",
    "            loss = criterion(output,target)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss+=loss.item()\n",
    "        if verbose:\n",
    "            print('Model trained for {0} epochs out of {1}. Training loss is {2}'.format(nepochs+1,maxepochs,loss.item()))\n",
    "        train_losses.append(train_loss/(batch_idx+1))\n",
    "        nepochs +=1\n",
    "        if epochs_for_early_stop>0:\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                valid_loss=0\n",
    "                for batch_idx, (data1, target1, data2, target2) in enumerate(valid_loader):\n",
    "                    data = data1.to(device)\n",
    "                    target = target1.to(device)\n",
    "                    # Forward pass\n",
    "                    ZNZ_output = best_model1(data).cpu().detach().numpy()\n",
    "                    ZNZ_output=np.where(ZNZ_output<best_threshold,0,ZNZ_output)\n",
    "                    ZNZ_output=np.where(ZNZ_output>best_threshold,1,ZNZ_output)\n",
    "                    data2 = torch.hstack((data2,torch.tensor(ZNZ_output)))\n",
    "                    data = data2.to(device)\n",
    "                    target = target2.to(device)\n",
    "                    # Forward pass\n",
    "                    output = model(data)\n",
    "                    loss = criterion(output,target)\n",
    "                    valid_loss+=loss.item()\n",
    "                valid_losses.append(valid_loss/(batch_idx+1))\n",
    "                counter+=1\n",
    "                if valid_losses[-1]<best_loss:\n",
    "                    if verbose:\n",
    "                        print('Validation loss decreased from {0} to {1}'.format(best_loss,valid_losses[-1]))\n",
    "                    best_loss = valid_losses[-1]\n",
    "                    best_model = model\n",
    "                    counter = 0\n",
    "                else:\n",
    "                    if verbose:\n",
    "                        print('Counter for early stopping: {0} out of {1}'.format(counter,epochs_for_early_stop))\n",
    "                    if counter == epochs_for_early_stop:\n",
    "                        print('early stopping at epoch ', nepochs-counter)\n",
    "                        if save_model:\n",
    "                            torch.save(best_model,model_dir+'/best_model.pkl')\n",
    "                        return (best_model,nepochs-epochs_for_early_stop,best_loss)\n",
    "    print('no early stopping')\n",
    "    if save_model:\n",
    "        torch.save(best_model,model_dir+'/best_model.pkl')\n",
    "    return (model,nepochs,train_losses[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "848ba664",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Calibration2(calib_loader,valid_loader,n_features,best_model1,best_threshold,max_num_models=40,maxepochs=500,epochs_for_early_stop=50):\n",
    "    best_MSE = np.inf\n",
    "    if verbose:\n",
    "        print('Training on ',device)\n",
    "    dropoutList = [0, 0.15, 0.3, 0.45]\n",
    "    layer_number_list=[1,2,3]\n",
    "    layer_size_list = [16, 32]\n",
    "    learning_rate_list = [10**-5,10**-4,10**-3,10**-2]\n",
    "    for number in range(max_num_models):\n",
    "        if verbose:\n",
    "            print('model {0} out of {1}'.format(number+1,max_num_models))\n",
    "        # hyper-parameters\n",
    "        dropprob = random.choice(dropoutList)\n",
    "        layer_number=random.choice(layer_number_list)\n",
    "        layer_size = random.choice(layer_size_list)\n",
    "        learning_rate=random.choice(learning_rate_list)\n",
    "        model_MSE = []\n",
    "        model = Deepnet2(layer_number,layer_size,dropprob,n_features).to(device)\n",
    "        optimizer = optim.AdamW(model.parameters(),lr=learning_rate,weight_decay=1e-6)\n",
    "        best_model,learning_steps,MSE = Train_early_stopping_2(best_model1,model,calib_loader,valid_loader,optimizer,best_threshold,\n",
    "                                                               maxepochs=maxepochs,epochs_for_early_stop=epochs_for_early_stop)\n",
    "        if MSE < best_MSE:\n",
    "            best_MSE = MSE\n",
    "            best_learning_steps = learning_steps\n",
    "            best_LearningRate = learning_rate\n",
    "            best_dropprob = dropprob\n",
    "            best_layer_number= layer_number\n",
    "            best_layer_size= layer_size\n",
    "    \n",
    "    if verbose:\n",
    "        print('best_MSE=', best_MSE)\n",
    "        print('best_learning_steps=', best_learning_steps)\n",
    "        print('best_LearningRate=', best_LearningRate)\n",
    "        print('best_dropprob=', best_dropprob)\n",
    "        print('best_layer_number=', best_layer_number)\n",
    "        print('best_layer_size=', best_layer_size)\n",
    "\n",
    "    best_hyperparameters = {'best_learning_steps': best_learning_steps, \n",
    "                            'best_LearningRate': best_LearningRate,\n",
    "                            'best_dropprob': best_dropprob, \n",
    "                            'best_layer_number': best_layer_number,\n",
    "                            'best_layer_size': best_layer_size}\n",
    "    return best_hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a2b7ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_model2(best_hyperparameters,train_loader,n_features,best_model1,best_threshold):\n",
    "    best_learning_steps=best_hyperparameters['best_learning_steps']\n",
    "    best_LearningRate=best_hyperparameters['best_LearningRate']\n",
    "    best_dropprob=best_hyperparameters['best_dropprob']\n",
    "    best_layer_number=best_hyperparameters['best_layer_number']\n",
    "    best_layer_size=best_hyperparameters['best_layer_size']\n",
    "    best_MSE=np.inf\n",
    "\n",
    "    for number_models in range(5):\n",
    "        model = Deepnet2(best_layer_number,best_layer_size,best_dropprob,n_features).to(device)\n",
    "        optimizer = optim.AdamW(model.parameters(),lr=best_LearningRate,weight_decay=1e-6)\n",
    "        trained_model,learning_steps,MSE = Train_early_stopping_2(best_model1,model,train_loader,train_loader,optimizer,best_threshold,\n",
    "                                                                  maxepochs=best_learning_steps,epochs_for_early_stop=0)\n",
    "        if verbose:\n",
    "            print('MSE on training data for model ',number_models+1,' = ',MSE)\n",
    "        if MSE<best_MSE:\n",
    "            best_MSE=MSE\n",
    "            best_model=trained_model\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5967f274",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_generate_data(data1,pred1,data2,pred2,step_idx):\n",
    "    if pred1==0:\n",
    "        Month = data1[step_idx+1][-1][0]\n",
    "        weekday = data1[step_idx+1][-1][1]\n",
    "        lstqt = data1[step_idx][-1][2]\n",
    "        interv = 1+data1[step_idx][-1][3]\n",
    "        PSLTNZD = data1[step_idx][-1][4]\n",
    "        \n",
    "    else:\n",
    "        Month = data1[step_idx+1][-1][0]\n",
    "        weekday = data1[step_idx+1][-1][1]\n",
    "        lstqt = pred2\n",
    "        interv = 1\n",
    "        PSLTNZD = data1[step_idx][-1][3] #Interval of previous demand\n",
    "\n",
    "    new_data1 = [Month,weekday,lstqt,interv,PSLTNZD]\n",
    "    new_data2 = [lstqt,interv]\n",
    "    data1[step_idx+1] = torch.vstack((data1[step_idx][1:],torch.tensor(new_data1).unsqueeze(dim=0)))\n",
    "    data2[step_idx+1] = torch.tensor(new_data2)\n",
    "    return (data1, data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d83b5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_predict2(best_model1,best_model2,test_loader,best_threshold,n_steps_out):\n",
    "    labels = []\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        best_model1.eval()\n",
    "        best_model2.eval()\n",
    "        for batch_idx, (data1, target1, data2, target2) in enumerate(test_loader):\n",
    "            n = data1.shape[0]\n",
    "            preds = []\n",
    "            for step_idx in range (n):\n",
    "                data = data1[step_idx].unsqueeze(dim=0).to(device)\n",
    "                # Forward pass 1\n",
    "                ZNZ_output = best_model1(data).cpu().detach().numpy()\n",
    "                ZNZ_output=np.where(ZNZ_output<best_threshold,0,ZNZ_output)\n",
    "                ZNZ_output=np.where(ZNZ_output>best_threshold,1,ZNZ_output)\n",
    "                data = torch.hstack((data2[step_idx].unsqueeze(dim=0),torch.tensor(ZNZ_output)))\n",
    "                data = data.to(device)\n",
    "                # Forward pass 2\n",
    "                output = best_model2(data)\n",
    "                pred=output.cpu().detach() * ZNZ_output\n",
    "                preds.append(pred.ravel().item())\n",
    "                if step_idx<n-1:\n",
    "                    data1,data2 = test_generate_data(data1,ZNZ_output,data2,pred,step_idx)\n",
    "            labels.extend(target2.ravel())\n",
    "            predictions.extend(preds)\n",
    "    return (labels,predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4409d3",
   "metadata": {},
   "source": [
    "# Regression Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "85f16749",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegNetwork(nn.Module):\n",
    "    def __init__(self,RNN , RNN_type , RNN_size , RNN_mean , RNN_sigma , NN_number , NN_size , NN_mean , NN_sigma ,\n",
    "                 dropprob , n_features):\n",
    "        super(RegNetwork,self).__init__()\n",
    "        self.RNN=RNN\n",
    "        self.RNN_type=RNN_type\n",
    "        self.RNN_size=RNN_size\n",
    "        self.RNN_mean=RNN_mean        \n",
    "        self.RNN_sigma=RNN_sigma\n",
    "        self.NN_number=NN_number\n",
    "        self.NN_mean=NN_mean\n",
    "        self.NN_sigma=NN_sigma\n",
    "        self.dropprob=dropprob\n",
    "        self.input_channels=n_features\n",
    "        self.FC_size=NN_size\n",
    "        if self.RNN:\n",
    "            if RNN_type=='LSTM':\n",
    "                self.rnn = nn.LSTM(self.input_channels, RNN_size, num_layers=1, bidirectional=False).to(device)\n",
    "                self.input_channels= RNN_size\n",
    "            elif RNN_type=='BiLSTM':\n",
    "                self.rnn = nn.LSTM(self.input_channels, RNN_size, num_layers=1, bidirectional=True).to(device)\n",
    "                self.input_channels= 2*RNN_size\n",
    "            elif RNN_type=='GRU':\n",
    "                self.rnn = nn.GRU(self.input_channels, RNN_size, num_layers=1, bidirectional=False).to(device)\n",
    "                self.input_channels= RNN_size\n",
    "            elif RNN_type=='BiGRU':\n",
    "                self.rnn = nn.GRU(self.input_channels, RNN_size, num_layers=1, bidirectional=True).to(device)\n",
    "                self.input_channels= 2*RNN_size\n",
    "\n",
    "            for layer_p in self.rnn._all_weights:\n",
    "                for p in layer_p:\n",
    "                    if 'weight' in p:\n",
    "                        torch.nn.init.normal_(self.rnn.__getattr__(p),mean=self.RNN_mean,std=self.RNN_sigma)\n",
    "        \n",
    "        self.fc1=nn.Linear(self.input_channels,self.FC_size)\n",
    "        torch.nn.init.normal_(self.fc1.weight,mean=self.NN_mean,std=self.NN_sigma)\n",
    "        torch.nn.init.normal_(self.fc1.bias,mean=self.NN_mean,std=self.NN_sigma)\n",
    "        self.classifier1 = nn.Sequential(\n",
    "            self.fc1,\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=dropprob, inplace=False))\n",
    "        if (self.NN_number>1):\n",
    "            self.fc2=nn.Linear(self.FC_size,self.FC_size)\n",
    "            torch.nn.init.normal_(self.fc2.weight,mean=self.NN_mean,std=self.NN_sigma)\n",
    "            torch.nn.init.normal_(self.fc2.bias,mean=self.NN_mean,std=self.NN_sigma)\n",
    "            self.classifier2 = nn.Sequential(\n",
    "                self.fc2,\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(p=dropprob, inplace=False))\n",
    "        if (self.NN_number>2):\n",
    "            self.fc3=nn.Linear(self.FC_size,self.FC_size)\n",
    "            torch.nn.init.normal_(self.fc3.weight,mean=self.NN_mean,std=self.NN_sigma)\n",
    "            torch.nn.init.normal_(self.fc3.bias,mean=self.NN_mean,std=self.NN_sigma)\n",
    "            self.classifier3 = nn.Sequential(\n",
    "                self.fc3,\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(p=dropprob, inplace=False))\n",
    "        self.fc_f=nn.Linear(self.FC_size,1)\n",
    "        torch.nn.init.normal_(self.fc_f.weight,mean=self.NN_mean,std=self.NN_sigma)\n",
    "        torch.nn.init.normal_(self.fc_f.bias,mean=self.NN_mean,std=self.NN_sigma)\n",
    "        self.dropout = torch.nn.Dropout(p=dropprob, inplace=False) #Dropout Layer (Dropout rate= p)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        if self.RNN:\n",
    "            x=x.permute(1,0,2)\n",
    "            output, _ = self.rnn(x)\n",
    "            if self.RNN_type=='BiLSTM' or self.RNN_type=='BiGRU':\n",
    "                Normal_RNN=output[-1, :, :self.RNN_size]\n",
    "                Rev_RNN=output[0, :, self.RNN_size:]\n",
    "                x = torch.cat((Normal_RNN, Rev_RNN), 1)\n",
    "                x = F.relu(x)\n",
    "                x = self.dropout(x)\n",
    "            else:\n",
    "                x = output[-1, :, :]\n",
    "                x = F.relu(x)\n",
    "                x = self.dropout(x)\n",
    "                \n",
    "        x=self.classifier1(x)\n",
    "        if (self.NN_number>1):\n",
    "            x=self.classifier2(x)\n",
    "        \n",
    "        if (self.NN_number>2):\n",
    "            x=self.classifier3(x)\n",
    "        \n",
    "        predict = torch.sigmoid(self.fc_f(x))\n",
    "        return (predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cd6b098a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_early_stopping_3(model,train_loader,valid_loader,optimizer,maxepochs=100,epochs_for_early_stop=0,save_model=False):\n",
    "    best_model = None\n",
    "    best_loss = np.inf\n",
    "    counter = 0\n",
    "    nepochs=0\n",
    "    valid_losses =[]\n",
    "    train_losses = []\n",
    "    criterion = nn.MSELoss()\n",
    "    while nepochs<maxepochs:\n",
    "        model.train()\n",
    "        train_loss=0\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            # Forward pass\n",
    "            output = model(data)\n",
    "            criterion = nn.MSELoss()\n",
    "            loss = criterion(output,target)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss+=loss.item()\n",
    "        if verbose:\n",
    "            print('Model trained for {0} epochs out of {1}. Training loss is {2}'.format(nepochs+1,maxepochs,loss.item()))\n",
    "        train_losses.append(train_loss/(batch_idx+1))\n",
    "        nepochs +=1\n",
    "        if epochs_for_early_stop>0:\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                valid_loss=0\n",
    "                for batch_idx, (data, target) in enumerate(valid_loader):\n",
    "                    data = data.to(device)\n",
    "                    target = target.to(device)\n",
    "                    # Forward pass\n",
    "                    output = model(data)\n",
    "                    criterion = nn.MSELoss()\n",
    "                    loss = criterion(output,target)\n",
    "                    valid_loss+=loss.item()\n",
    "                valid_losses.append(valid_loss/(batch_idx+1))\n",
    "                counter+=1\n",
    "                if valid_losses[-1]<best_loss:\n",
    "                    if verbose:\n",
    "                        print('Validation loss decreased from {0} to {1}'.format(best_loss,valid_losses[-1]))\n",
    "                    best_loss = valid_losses[-1]\n",
    "                    best_model = model\n",
    "                    counter = 0\n",
    "                else:\n",
    "                    if verbose:\n",
    "                        print('Counter for early stopping: {0} out of {1}'.format(counter,epochs_for_early_stop))\n",
    "                    if counter == epochs_for_early_stop:\n",
    "                        print('early stopping at epoch ', nepochs-counter)\n",
    "                        if save_model:\n",
    "                            torch.save(best_model,model_dir+'/best_model.pkl')\n",
    "                        return (best_model,nepochs-epochs_for_early_stop,best_loss)\n",
    "    print('no early stopping')\n",
    "    if save_model:\n",
    "        torch.save(best_model,model_dir+'/best_model.pkl')\n",
    "    return (model,nepochs,train_losses[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5e263212",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Calibration3(RNN,RNN_type,calib_loader,valid_loader,n_features,max_num_models=40,maxepochs=500,epochs_for_early_stop=50):\n",
    "    best_MSE = 100\n",
    "    if verbose:\n",
    "        print('Training on ',device)\n",
    "    RNN_hidden_size_list = [20, 50, 80, 100]\n",
    "    dropoutList = [0, 0.15, 0.3, 0.45]\n",
    "    weight_decay_list=[10**-6,10**-5,10**-4]\n",
    "    layer_number_list=[2,3]\n",
    "    layer_size_list = [32, 64]\n",
    "    learning_rate_list = [10**-5,10**-4,10**-3,10**-3]\n",
    "    max_learning_steps = 400\n",
    "    optimizers =['adam','sgd']\n",
    "    for number in range(max_num_models):\n",
    "        if verbose:\n",
    "            print('model {0} out of {1}'.format(number+1,max_num_models))\n",
    "        # hyper-parameters\n",
    "        RNN_hidden_size = random.choice(RNN_hidden_size_list)\n",
    "        dropprob = random.choice(dropoutList)\n",
    "        RNN_mean=random.choice([-1,1])*logsampler(10 ** -4, 1)\n",
    "        RNN_sigma = logsampler(10 ** -4, 1)\n",
    "        Weight_decay=random.choice(weight_decay_list)\n",
    "        optimizer_ = random.choice(optimizers)\n",
    "        if RNN:\n",
    "            layer_number=1\n",
    "        else: \n",
    "            layer_number=random.choice(layer_number_list)\n",
    "        layer_size = random.choice(layer_size_list)\n",
    "        learning_rate=random.choice(learning_rate_list)\n",
    "        NN_mean=random.choice([-1,1])*logsampler(10 ** -4, 1)\n",
    "        N_sigma = logsampler(10 ** -4, 1)\n",
    "        model_MSE = []\n",
    "        \n",
    "        #init model\n",
    "        model = RegNetwork(RNN,RNN_type,RNN_hidden_size,RNN_mean,RNN_sigma,layer_number,layer_size,NN_mean,N_sigma,\n",
    "                           dropprob,n_features).to(device)\n",
    "        if optimizer_ == 'sgd':\n",
    "            optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate,weight_decay=Weight_decay)\n",
    "        else:\n",
    "            optimizer = torch.optim.AdamW(model.parameters(),lr=learning_rate,weight_decay=Weight_decay)\n",
    "        #training\n",
    "        best_model,learning_steps,MSE = Train_early_stopping_3(model,calib_loader,valid_loader,optimizer,\n",
    "                                                               maxepochs=maxepochs,epochs_for_early_stop=epochs_for_early_stop)\n",
    "        if MSE < best_MSE:\n",
    "            best_MSE = MSE\n",
    "            best_learning_steps = learning_steps\n",
    "            best_LearningRate = learning_rate\n",
    "            best_dropprob = dropprob\n",
    "            best_layer_number= layer_number\n",
    "            best_layer_size= layer_size\n",
    "            best_NN_mean=NN_mean\n",
    "            best_N_sigma=N_sigma\n",
    "            best_RNN_hidden_size= RNN_hidden_size\n",
    "            best_RNN_mean=RNN_mean\n",
    "            best_RNN_sigma=RNN_sigma\n",
    "            best_weight_decay=Weight_decay\n",
    "            best_optimizer=optimizer_\n",
    "    \n",
    "    if verbose:\n",
    "        print('best_MSE=', best_MSE)\n",
    "        print('best_learning_steps=', best_learning_steps)\n",
    "        print('best_LearningRate=', best_LearningRate)\n",
    "        print('best_dropprob=', best_dropprob)\n",
    "        print('best_weight_decay=',best_weight_decay)\n",
    "        print('best_layer_number=', best_layer_number)\n",
    "        print('best_layer_size=', best_layer_size)\n",
    "        print('best_NN_mean=', best_NN_mean)\n",
    "        print('best_N_sigma=', best_N_sigma)\n",
    "        print('best_RNN_hidden_size=', best_RNN_hidden_size)\n",
    "        print('best_RNN_mean=', best_RNN_mean)\n",
    "        print('best_RNN_sigma=', best_RNN_sigma)\n",
    "        print('best_optimizer=', best_optimizer)\n",
    "    best_hyperparameters = {'best_learning_steps': best_learning_steps, \n",
    "                            'best_LearningRate': best_LearningRate,\n",
    "                            'best_dropprob': best_dropprob, \n",
    "                            'best_weight_decay':best_weight_decay,\n",
    "                            'best_layer_number': best_layer_number,\n",
    "                            'best_layer_size': best_layer_size, \n",
    "                            'best_NN_mean':best_NN_mean,\n",
    "                            'best_N_sigma':best_N_sigma,\n",
    "                            'best_RNN_hidden_size' :best_RNN_hidden_size,\n",
    "                            'best_RNN_mean':best_RNN_mean,\n",
    "                            'best_RNN_sigma': best_RNN_sigma,\n",
    "                            'best_optimizer': best_optimizer}\n",
    "    return best_hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "06f41a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_model3(RNN,RNN_type,best_hyperparameters,train_loader,n_features):\n",
    "    best_learning_steps=best_hyperparameters['best_learning_steps']\n",
    "    best_LearningRate=best_hyperparameters['best_LearningRate']\n",
    "    best_dropprob=best_hyperparameters['best_dropprob']\n",
    "    best_weight_decay=best_hyperparameters['best_weight_decay']\n",
    "    best_layer_number=best_hyperparameters['best_layer_number']\n",
    "    best_N_sigma=best_hyperparameters['best_N_sigma']\n",
    "    best_layer_size=best_hyperparameters['best_layer_size']\n",
    "    best_RNN_hidden_size=best_hyperparameters['best_RNN_hidden_size']\n",
    "    best_RNN_sigma=best_hyperparameters['best_RNN_sigma']\n",
    "    best_RNN_mean=best_hyperparameters['best_RNN_mean']\n",
    "    best_NN_mean=best_hyperparameters['best_NN_mean']\n",
    "    best_optimizer=best_hyperparameters['best_optimizer']\n",
    "    best_MSE=100\n",
    "\n",
    "    for number_models in range(5):\n",
    "        model = RegNetwork(RNN,RNN_type,best_RNN_hidden_size,best_RNN_mean,best_RNN_sigma,best_layer_number,\n",
    "                           best_layer_size,best_NN_mean,best_N_sigma,best_dropprob,n_features).to(device)\n",
    "        if best_optimizer == 'sgd':\n",
    "            optimizer = torch.optim.SGD(model.parameters(),lr=best_LearningRate,weight_decay=best_weight_decay)\n",
    "        else:\n",
    "            optimizer = torch.optim.Adam(model.parameters(),lr=best_LearningRate,weight_decay=best_weight_decay)\n",
    "        trained_model,learning_steps,MSE = Train_early_stopping_3(model,train_loader,train_loader,optimizer,\n",
    "                                                                  maxepochs=best_learning_steps,epochs_for_early_stop=0)\n",
    "        if verbose:\n",
    "            print('MSE on training data for model ',number_models+1,' = ',MSE)\n",
    "        if (MSE<best_MSE):\n",
    "            best_MSE=MSE\n",
    "            best_model=trained_model\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3d997389",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_predict3(best_model,test_loader):\n",
    "    with torch.no_grad():\n",
    "        best_model.eval()\n",
    "        for batch_idx, (data, target) in enumerate(test_loader):\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            # Forward pass\n",
    "            output = best_model(data)\n",
    "            pred = output.cpu().detach().numpy().reshape(output.shape[0])\n",
    "            labels = target.cpu().numpy().reshape(output.shape[0])\n",
    "    return (pred,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cda9d3",
   "metadata": {},
   "source": [
    "# SES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "faf38a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SES(ts,alpha=0.1):\n",
    "    d = np.array(ts) # Transform the input into a numpy array\n",
    "    cols = len(d) # Historical period length\n",
    "    \n",
    "    #level (a), periodicity(p) and forecast (f)\n",
    "    f = np.full((cols),np.nan)\n",
    "    \n",
    "    # Initialization\n",
    "    f[0] = d[0]\n",
    "    # Create all the t+1 forecasts\n",
    "    for t in range(0,cols-1):        \n",
    "        f[t+1] = alpha*d[t] + (1-alpha)*f[t]   \n",
    "                      \n",
    "    df = pd.DataFrame.from_dict({\"Demand\":d,\"Forecast\":f,\"Error\":d-f})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2fe25812",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_SES(DATA_Micro,n1,n2):\n",
    "    Opt_SES=[0,10000]\n",
    "    for i in np.arange (0,1,0.001):\n",
    "        SES_train=SES(DATA_Micro.Qty[n1:n2],alpha=i)\n",
    "        if (np.sqrt(mean_squared_error(SES_train['Forecast'].values,DATA_Micro['Qty'][n1:n2].values))<Opt_SES[1]):\n",
    "            Opt_SES[1]=np.sqrt(mean_squared_error(SES_train['Forecast'].values,DATA_Micro['Qty'][n1:n2].values))\n",
    "            Opt_SES[0]=i\n",
    "    return (Opt_SES[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612d52db",
   "metadata": {},
   "source": [
    "# Croston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "686cadc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Croston(ts,extra_periods=1,alpha=0.4):\n",
    "    d = np.array(ts) # Transform the input into a numpy array\n",
    "    cols = len(d) # Historical period length\n",
    "    d = np.append(d,[np.nan]*extra_periods) # Append np.nan into the demand array to cover future periods\n",
    "    \n",
    "    #level (a), periodicity(p) and forecast (f)\n",
    "    a,p,f = np.full((3,cols+extra_periods),np.nan)\n",
    "    q = 1 #periods since last demand observation\n",
    "    \n",
    "    # Initialization\n",
    "    first_occurence = np.argmax(d[:cols]>0)\n",
    "    a[0] = d[first_occurence]\n",
    "    p[0] = 1 + first_occurence\n",
    "    f[0] = a[0]/p[0]\n",
    "    # Create all the t+1 forecasts\n",
    "    for t in range(0,cols):        \n",
    "        if d[t] > 0:\n",
    "            a[t+1] = alpha*d[t] + (1-alpha)*a[t] \n",
    "            p[t+1] = alpha*q + (1-alpha)*p[t]\n",
    "            f[t+1] = a[t+1]/p[t+1]\n",
    "            q = 1           \n",
    "        else:\n",
    "            a[t+1] = a[t]\n",
    "            p[t+1] = p[t]\n",
    "            f[t+1] = f[t]\n",
    "            q += 1\n",
    "       \n",
    "    # Future Forecast \n",
    "    a[cols+1:cols+extra_periods] = a[cols]\n",
    "    p[cols+1:cols+extra_periods] = p[cols]\n",
    "    f[cols+1:cols+extra_periods] = f[cols]\n",
    "                      \n",
    "    df = pd.DataFrame.from_dict({\"Demand\":d,\"Forecast\":f,\"Period\":p,\"Level\":a,\"Error\":d-f})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "86b4b7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_CR(DATA_Micro,n1,n2):\n",
    "    Opt_CR=[0,100]\n",
    "    for i in np.arange (0,1,0.001):\n",
    "        Cr_train=Croston(DATA_Micro.Qty[n1:n2],alpha=i)\n",
    "        if (np.sqrt(mean_squared_error(Cr_train['Forecast'].values[:-1],DATA_Micro['Qty'][n1:n2].values))<Opt_CR[1]):\n",
    "            Opt_CR[1]=np.sqrt(mean_squared_error(Cr_train['Forecast'].values[:-1],DATA_Micro['Qty'][n1:n2].values))\n",
    "            Opt_CR[0]=i\n",
    "    return(Opt_CR[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13821cb",
   "metadata": {},
   "source": [
    "# SBA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c1749ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SBA(ts,extra_periods=1,alpha=0.1,beta=0.1):\n",
    "    d = np.array(ts) # Transform the input into a numpy array\n",
    "    cols = len(d) # Historical period length\n",
    "    d = np.append(d,[np.nan]*extra_periods) # Append np.nan into the demand array to cover future periods\n",
    "    \n",
    "    #level (a), periodicity(p) and forecast (f)\n",
    "    a,p,f = np.full((3,cols+extra_periods),np.nan)\n",
    "    q = 1 #periods since last demand observation\n",
    "    \n",
    "    # Initialization\n",
    "    first_occurence = np.argmax(d[:cols]>0)\n",
    "    a[0] = d[first_occurence]\n",
    "    p[0] = 1 + first_occurence\n",
    "    f[0] = a[0]/p[0]\n",
    "    # Create all the t+1 forecasts\n",
    "    for t in range(0,cols):        \n",
    "        if d[t] > 0:\n",
    "            a[t+1] = alpha*d[t] + (1-alpha)*a[t] \n",
    "            p[t+1] = beta*q + (1-beta)*p[t]\n",
    "            f[t+1] = (1-alpha/2)*a[t+1]/p[t+1]\n",
    "            q = 1           \n",
    "        else:\n",
    "            a[t+1] = a[t]\n",
    "            p[t+1] = p[t]\n",
    "            f[t+1] = f[t]\n",
    "            q += 1\n",
    "       \n",
    "    # Future Forecast \n",
    "    a[cols+1:cols+extra_periods] = a[cols]\n",
    "    p[cols+1:cols+extra_periods] = p[cols]\n",
    "    f[cols+1:cols+extra_periods] = f[cols]\n",
    "                      \n",
    "    df = pd.DataFrame.from_dict({\"Demand\":d,\"Forecast\":f,\"Period\":p,\"Level\":a,\"Error\":d-f})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d4379bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_SBA(DATA_Micro,n1,n2):\n",
    "    Opt_SBA=[0,0,100]\n",
    "    for i in np.arange (0,1,0.01):\n",
    "        for j in np.arange (0,1,0.01):\n",
    "            SBA_train=SBA(DATA_Micro.Qty[n1:n2],alpha=i,beta=j)\n",
    "            if (np.sqrt(mean_squared_error(SBA_train['Forecast'].values[:-1],DATA_Micro['Qty'][n1:n2].values))<Opt_SBA[2]):\n",
    "                Opt_SBA[2]=np.sqrt(mean_squared_error(SBA_train['Forecast'].values[:-1],DATA_Micro['Qty'][n1:n2].values))\n",
    "                Opt_SBA[1]=j\n",
    "                Opt_SBA[0]=i\n",
    "    return(Opt_SBA[0],Opt_SBA[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "465d3ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_results(predictions,n_steps_out):\n",
    "    n = len(predictions)\n",
    "    for i in range (0,n,n_steps_out):\n",
    "        end_id = min(n,i+n_steps_out)\n",
    "        predictions[i:end_id]=[predictions[i]]*(end_id-i)\n",
    "    return (predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac5991b",
   "metadata": {},
   "source": [
    "# Generating results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dea40bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_csv(product,n_obs,ADI,CV2,n_steps_in,n_steps_out,AUROC_HM,AUPRC_HM,MASE_HM,MASE_NN,MASE_SES,MASE_Cr,MASE_SBA,MASE_RNN,\n",
    "                MASE_Naive,MASE_ZF,RMSSE_HM,RMSSE_NN,RMSSE_SES,RMSSE_Cr,RMSSE_SBA,RMSSE_RNN,RMSSE_Naive,RMSSE_ZF,PIS_HM,PIS_NN,\n",
    "                PIS_SES,PIS_Cr,PIS_SBA,PIS_RNN,PIS_Naive,PIS_ZF,file='Forecast_results.csv'):\n",
    "    header=['product','Samples','ADI','CV2','n_steps_in','n_steps_out','AUROC_HM','AUPRC_HM','MASE_HM','MASE_NN','MASE_SES','MASE_Cr','MASE_SBA','MASE_RNN','MASE_Naive','MASE_ZF',\n",
    "    'RMSSE_HM','RMSSE_NN','RMSSE_SES','RMSSE_Cr','RMSSE_SBA','RMSSE_RNN','RMSSE_Naive','RMSSE_ZF','PIS_HM','PIS_NN',\n",
    "    'PIS_SES','PIS_Cr','PIS_SBA','PIS_RNN','PIS_Naive','PIS_ZF']\n",
    "    dicti={\n",
    "        'product':product,\n",
    "        'Samples':n_obs,\n",
    "        'ADI':ADI,\n",
    "        'CV2':CV2,\n",
    "        'n_steps_in':n_steps_in,\n",
    "        'n_steps_out':n_steps_out,\n",
    "        'AUROC_HM':AUROC_HM,\n",
    "        'AUPRC_HM':AUPRC_HM,\n",
    "        'MASE_HM':MASE_HM,\n",
    "        'MASE_NN':MASE_NN,\n",
    "        'MASE_SES':MASE_SES,\n",
    "        'MASE_Cr':MASE_Cr,\n",
    "        'MASE_SBA':MASE_SBA,\n",
    "        'MASE_RNN':MASE_RNN,\n",
    "        'MASE_Naive':MASE_Naive,\n",
    "        'MASE_ZF': MASE_ZF,\n",
    "        'RMSSE_HM':RMSSE_HM,\n",
    "        'RMSSE_NN':RMSSE_NN,\n",
    "        'RMSSE_SES':RMSSE_SES,\n",
    "        'RMSSE_Cr':RMSSE_Cr,\n",
    "        'RMSSE_SBA':RMSSE_SBA,\n",
    "        'RMSSE_RNN':RMSSE_RNN,\n",
    "        'RMSSE_Naive':RMSSE_Naive,\n",
    "        'RMSSE_ZF':RMSSE_ZF,\n",
    "        'PIS_HM':PIS_HM,\n",
    "        'PIS_NN':PIS_NN,\n",
    "        'PIS_SES':PIS_SES,\n",
    "        'PIS_Cr':PIS_Cr,\n",
    "        'PIS_SBA':PIS_SBA,\n",
    "        'PIS_RNN':PIS_RNN,\n",
    "        'PIS_Naive':PIS_Naive,\n",
    "        'PIS_ZF':PIS_ZF\n",
    "        }\n",
    "    # saving results\n",
    "    try:\n",
    "        df=pd.read_csv(results_folder+'Forecast_results.csv')\n",
    "        new=False\n",
    "    except:\n",
    "        new=True\n",
    "    if new:\n",
    "        with open(results_folder+'Forecast_results.csv','w') as fd:\n",
    "            writer = csv.writer(fd)\n",
    "            writer.writerow(header)\n",
    "            writer = csv.DictWriter(fd, fieldnames=header)\n",
    "            writer.writerow(dicti)\n",
    "    else:\n",
    "        with open(results_folder+'Forecast_results.csv','a',newline='') as fd:\n",
    "            writer = csv.DictWriter(fd, fieldnames=header)\n",
    "            writer.writerow(dicti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3cf99f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_forecast(product,TP,HM,NN,RNN,SES,CR,SBA,Naive,ZF,n_steps_out_list):\n",
    "    header=['TP']\n",
    "    results=pd.DataFrame(columns=header)\n",
    "    results['TP']=TP\n",
    "    for i in range (len(n_steps_out_list)):\n",
    "        results['HM_{0}'.format(n_steps_out_list[i])] = HM[i]\n",
    "        results['NN_{0}'.format(n_steps_out_list[i])] = NN[i]\n",
    "        results['SES_{0}'.format(n_steps_out_list[i])] = SES[i]\n",
    "        results['Cr_{0}'.format(n_steps_out_list[i])] = CR[i]\n",
    "        results['SBA_{0}'.format(n_steps_out_list[i])] = SBA[i]\n",
    "        results['RNN_{0}'.format(n_steps_out_list[i])] = RNN[i]\n",
    "        results['Naive_{0}'.format(n_steps_out_list[i])] = Naive[i]\n",
    "    results['ZF'] = ZF\n",
    "    results.to_csv(results_folder+product+'.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "07efe112",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(product,n_steps_in=1,n_steps_out_list=[1],path=\"./DATA/\"):  \n",
    "    DATA=pd.read_csv(path+product+\".csv\", sep=';',encoding = \"ISO-8859-1\")\n",
    "\n",
    "    #preprocess\n",
    "    data = datasets(DATA,n_steps_in)\n",
    "    \n",
    "    #number of input lags, train and test samples\n",
    "    Nlags = data.get_Nlags()\n",
    "    n0 = data.get_n0()\n",
    "    n1 = data.get_n1()\n",
    "    \n",
    "    #loss function coef\n",
    "    w0 = data.get_w0()\n",
    "    w1 = data.get_w1()\n",
    "    \n",
    "    #metric\n",
    "    metric='ROC'\n",
    "    \n",
    "    #number of features\n",
    "    n_features=5\n",
    "    n_features2=3\n",
    "    n_features3=5\n",
    "    \n",
    "    #model type and param\n",
    "    RNN='BiGRU'\n",
    "    batch_size=64\n",
    "    evaluate_performance=True\n",
    "    max_num_models=50\n",
    "    maxepochs_Deep2net=200\n",
    "    maxepochs_NN_RNN=500\n",
    "    epochs_for_early_stop=50\n",
    "\n",
    "    #Deep2net\n",
    "    calib_data,valid_data,train_data,test_data=data.get_deep2net_datasets()\n",
    "    calib_dataset=dataset_load(calib_data)\n",
    "    valid_dataset=dataset_load(valid_data)\n",
    "    train_dataset=dataset_load(train_data)\n",
    "    test_dataset=dataset_load(test_data)\n",
    "\n",
    "    calib_loader = DataLoader(dataset=calib_dataset,sampler=ImbalancedDatasetSampler(calib_dataset),batch_size=batch_size,shuffle=False)\n",
    "    valid_loader = DataLoader(dataset=valid_dataset,batch_size=batch_size,shuffle=False)\n",
    "    train_loader = DataLoader(dataset=train_dataset,sampler=ImbalancedDatasetSampler(train_dataset),batch_size=batch_size,shuffle=False)\n",
    "\n",
    "    best_hyperparameters=Calibration(RNN,w1,w0,calib_loader,valid_loader,n_features,max_num_models,maxepochs_Deep2net,epochs_for_early_stop)\n",
    "    best_model1,best_threshold=Train_model(RNN,w1,w0,best_hyperparameters,train_loader,n_features)\n",
    "    best_hyperparameters2=Calibration2(calib_loader,valid_loader,n_features2,best_model1,best_threshold,max_num_models,maxepochs_Deep2net,epochs_for_early_stop)\n",
    "    best_model2=Train_model2(best_hyperparameters2,train_loader,n_features2,best_model1,best_threshold)\n",
    "    \n",
    "    #RegNetwork-RNN\n",
    "    calib_data1,valid_data1,train_data1,test_data1,scaler=data.get_reg_datasets()\n",
    "    calib_dataset1=dataset_load1(calib_data1)\n",
    "    valid_dataset1=dataset_load1(valid_data1)\n",
    "    train_dataset1=dataset_load1(train_data1)\n",
    "    test_dataset1=dataset_load1(test_data1)\n",
    "    calib_loader1 = DataLoader(dataset=calib_dataset1,batch_size=batch_size,shuffle=False)\n",
    "    valid_loader1 = DataLoader(dataset=valid_dataset1,batch_size=batch_size,shuffle=False)\n",
    "    train_loader1 = DataLoader(dataset=train_dataset1,batch_size=batch_size,shuffle=False)\n",
    "    test_loader1 = DataLoader(dataset=test_dataset1,batch_size=10000,shuffle=False)\n",
    "    best_hyperparameters_RNN=Calibration3(True,RNN,calib_loader1,valid_loader1,n_features3,max_num_models,maxepochs_NN_RNN,epochs_for_early_stop)\n",
    "    best_model_RNN=Train_model3(True,RNN,best_hyperparameters_RNN,train_loader1,n_features3)\n",
    "    pred_RNN, labels_RNN = test_predict3(best_model_RNN,test_loader1)\n",
    "    pred_RNN_=scaler.inverse_transform([pred_RNN]).ravel()\n",
    "\n",
    "    #RegNetwork-MLP\n",
    "    best_hyperparameters_MLP=Calibration3(False,RNN,calib_loader1,valid_loader1,n_features3,max_num_models,maxepochs_NN_RNN,epochs_for_early_stop)\n",
    "    best_model_MLP=Train_model3(False,RNN,best_hyperparameters_MLP,train_loader1,n_features3)\n",
    "    pred_MLP, labels_MLP = test_predict3(best_model_MLP,test_loader1)\n",
    "    pred_MLP_=scaler.inverse_transform([pred_MLP]).ravel()\n",
    "    \n",
    "    #SES\n",
    "    SES_test=SES(DATA.Qty[:],evaluate_SES(DATA,n0,n1))\n",
    "    SES_pred_=SES_test['Forecast'][n1:]\n",
    "    \n",
    "    #Croston\n",
    "    Cr_test=Croston(DATA.Qty[0:],alpha=evaluate_CR(DATA,n0,n1))\n",
    "    Cr_pred_=Cr_test['Forecast'][n1:-1]\n",
    "    \n",
    "    #SBA\n",
    "    Opt_SBA=evaluate_SBA(DATA,n0,n1)\n",
    "    SBA_test=SBA(DATA.Qty[0:],alpha=Opt_SBA[0],beta=Opt_SBA[1])\n",
    "    SBA_pred_=SBA_test['Forecast'][n1:-1]\n",
    "    \n",
    "    #Naive\n",
    "    naiv_pred_ = DATA['Qty'][n1-Nlags:-Nlags].values\n",
    "    \n",
    "    #Saving to CSV\n",
    "    n_obs = data.get_n_obs()\n",
    "    ADI = data.get_ADI()\n",
    "    CV2 = data.get_CV2()\n",
    "    Train_X = data.get_train_Y()\n",
    "    Test_Y = data.get_test_Y()\n",
    "    \n",
    "    test_loader = DataLoader(dataset=test_dataset,batch_size=batch_size,shuffle=False)\n",
    "    AUROC_HM,AUPRC_HM = test_predict(best_model1,test_loader)\n",
    "    \n",
    "    HM_predictions, NN_predictions, RNN_predictions, SES_predictions, Cr_predictions, SBA_predictions, Naive_predictions  = [],[],[],[],[],[],[]\n",
    "    \n",
    "    for n_steps_out in n_steps_out_list:\n",
    "        test_loader = DataLoader(dataset=test_dataset,batch_size=n_steps_out,shuffle=False)\n",
    "        labels_HM,predictions_HM = test_predict2(best_model1,best_model2,test_loader,best_threshold,n_steps_out)\n",
    "        HM_predictions.append(predictions_HM)\n",
    "        MASE_HM = MASE(Train_X,Test_Y,predictions_HM)\n",
    "        RMSSE_HM = RMSSE(Train_X,Test_Y,predictions_HM)\n",
    "        PIS_HM = MPIS(Test_Y,predictions_HM)\n",
    "\n",
    "        pred_MLP = change_results(pred_MLP_,n_steps_out)\n",
    "        pred_RNN = change_results(pred_RNN_,n_steps_out)\n",
    "        NN_predictions.append(pred_MLP)\n",
    "        RNN_predictions.append(pred_RNN)\n",
    "        MASE_NN = MASE(Train_X,Test_Y,pred_MLP)\n",
    "        RMSSE_NN = RMSSE(Train_X,Test_Y,pred_MLP)\n",
    "        PIS_NN = MPIS(Test_Y,pred_MLP)\n",
    "        MASE_RNN = MASE(Train_X,Test_Y,pred_RNN)\n",
    "        RMSSE_RNN = RMSSE(Train_X,Test_Y,pred_RNN)\n",
    "        PIS_RNN = MPIS(Test_Y,pred_RNN)\n",
    "    \n",
    "        SES_pred = change_results(list(SES_pred_),n_steps_out)\n",
    "        SES_predictions.append(SES_pred)\n",
    "        MASE_SES = MASE(Train_X,Test_Y,SES_pred)\n",
    "        RMSSE_SES = RMSSE(Train_X,Test_Y,SES_pred)\n",
    "        PIS_SES = MPIS(Test_Y,SES_pred)\n",
    "\n",
    "        Cr_pred = change_results(list(Cr_pred_),n_steps_out)\n",
    "        Cr_predictions.append(Cr_pred)\n",
    "        MASE_Cr = MASE(Train_X,Test_Y,Cr_pred)\n",
    "        RMSSE_Cr = RMSSE(Train_X,Test_Y,Cr_pred)\n",
    "        PIS_Cr = MPIS(Test_Y,Cr_pred)\n",
    "\n",
    "        SBA_pred = change_results(list(SBA_pred_),n_steps_out)\n",
    "        SBA_predictions.append(SBA_pred)\n",
    "        MASE_SBA = MASE(Train_X,Test_Y,SBA_pred)\n",
    "        RMSSE_SBA = RMSSE(Train_X,Test_Y,SBA_pred)\n",
    "        PIS_SBA = MPIS(Test_Y,SBA_pred)\n",
    "\n",
    "        if (Nlags<n_steps_out):\n",
    "            naiv_pred = change_results(naiv_pred_,n_steps_out)\n",
    "        else:\n",
    "            naiv_pred = naiv_pred_\n",
    "        Naive_predictions.append(naiv_pred)\n",
    "        MASE_Naive = MASE(Train_X,Test_Y,naiv_pred)\n",
    "        RMSSE_Naive = RMSSE(Train_X,Test_Y,naiv_pred)\n",
    "        PIS_Naive = MPIS(Test_Y,naiv_pred)\n",
    "    \n",
    "        ZF_pred = [0]*len(Test_Y)\n",
    "        MASE_ZF = MASE(Train_X,Test_Y,ZF_pred)\n",
    "        RMSSE_ZF = RMSSE(Train_X,Test_Y,ZF_pred)\n",
    "        PIS_ZF = MPIS(Test_Y,ZF_pred)\n",
    "        \n",
    "        save_to_csv(product,n_obs,ADI,CV2,n_steps_in,n_steps_out,AUROC_HM,AUPRC_HM,MASE_HM,MASE_NN,MASE_SES,MASE_Cr,MASE_SBA,MASE_RNN,\n",
    "                    MASE_Naive,MASE_ZF,RMSSE_HM,RMSSE_NN,RMSSE_SES,RMSSE_Cr,RMSSE_SBA,RMSSE_RNN,RMSSE_Naive,RMSSE_ZF,PIS_HM,PIS_NN,\n",
    "                    PIS_SES,PIS_Cr,PIS_SBA,PIS_RNN,PIS_Naive,PIS_ZF)\n",
    "    if save_predictions:\n",
    "        save_forecast(product,Test_Y,HM_predictions,NN_predictions,RNN_predictions, SES_predictions, Cr_predictions, SBA_predictions, Naive_predictions,ZF_pred,n_steps_out_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a5616715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "early stopping at epoch  66\n",
      "early stopping at epoch  22\n",
      "no early stopping\n",
      "early stopping at epoch  81\n",
      "no early stopping\n",
      "early stopping at epoch  111\n",
      "early stopping at epoch  22\n",
      "no early stopping\n",
      "early stopping at epoch  48\n",
      "early stopping at epoch  69\n",
      "no early stopping\n",
      "early stopping at epoch  107\n",
      "no early stopping\n",
      "early stopping at epoch  1\n",
      "early stopping at epoch  23\n",
      "early stopping at epoch  42\n",
      "early stopping at epoch  51\n",
      "early stopping at epoch  2\n",
      "early stopping at epoch  24\n",
      "no early stopping\n",
      "early stopping at epoch  91\n",
      "early stopping at epoch  39\n",
      "early stopping at epoch  1\n",
      "early stopping at epoch  11\n",
      "early stopping at epoch  68\n",
      "early stopping at epoch  15\n",
      "early stopping at epoch  1\n",
      "early stopping at epoch  1\n",
      "early stopping at epoch  32\n",
      "early stopping at epoch  1\n",
      "no early stopping\n",
      "early stopping at epoch  1\n",
      "no early stopping\n",
      "early stopping at epoch  123\n",
      "early stopping at epoch  1\n",
      "early stopping at epoch  76\n",
      "early stopping at epoch  15\n",
      "early stopping at epoch  21\n",
      "early stopping at epoch  150\n",
      "early stopping at epoch  33\n",
      "early stopping at epoch  1\n",
      "early stopping at epoch  18\n",
      "early stopping at epoch  148\n",
      "no early stopping\n",
      "early stopping at epoch  1\n",
      "early stopping at epoch  67\n",
      "early stopping at epoch  45\n",
      "no early stopping\n",
      "early stopping at epoch  36\n",
      "no early stopping\n",
      "no early stopping\n",
      "no early stopping\n",
      "no early stopping\n",
      "no early stopping\n",
      "no early stopping\n",
      "early stopping at epoch  25\n",
      "early stopping at epoch  1\n",
      "early stopping at epoch  16\n",
      "early stopping at epoch  13\n",
      "early stopping at epoch  1\n",
      "early stopping at epoch  1\n",
      "early stopping at epoch  48\n",
      "early stopping at epoch  1\n",
      "early stopping at epoch  1\n",
      "early stopping at epoch  6\n",
      "early stopping at epoch  36\n",
      "early stopping at epoch  62\n",
      "early stopping at epoch  1\n",
      "early stopping at epoch  1\n",
      "early stopping at epoch  27\n",
      "early stopping at epoch  66\n",
      "early stopping at epoch  26\n",
      "early stopping at epoch  52\n",
      "early stopping at epoch  1\n",
      "early stopping at epoch  1\n",
      "early stopping at epoch  1\n",
      "early stopping at epoch  81\n",
      "early stopping at epoch  3\n",
      "early stopping at epoch  73\n",
      "early stopping at epoch  1\n",
      "early stopping at epoch  5\n",
      "early stopping at epoch  28\n",
      "early stopping at epoch  16\n",
      "early stopping at epoch  5\n",
      "early stopping at epoch  56\n",
      "early stopping at epoch  1\n",
      "early stopping at epoch  109\n",
      "early stopping at epoch  9\n",
      "early stopping at epoch  1\n",
      "early stopping at epoch  1\n",
      "early stopping at epoch  1\n",
      "early stopping at epoch  9\n",
      "early stopping at epoch  5\n",
      "early stopping at epoch  32\n",
      "early stopping at epoch  1\n",
      "early stopping at epoch  31\n",
      "early stopping at epoch  1\n",
      "early stopping at epoch  1\n",
      "early stopping at epoch  2\n",
      "early stopping at epoch  1\n",
      "early stopping at epoch  1\n",
      "early stopping at epoch  103\n",
      "early stopping at epoch  12\n",
      "early stopping at epoch  1\n",
      "early stopping at epoch  27\n",
      "no early stopping\n",
      "no early stopping\n",
      "no early stopping\n",
      "no early stopping\n",
      "no early stopping\n",
      "early stopping at epoch  88\n",
      "no early stopping\n",
      "no early stopping\n",
      "no early stopping\n",
      "no early stopping\n",
      "early stopping at epoch  449\n",
      "no early stopping\n",
      "no early stopping\n",
      "early stopping at epoch  1\n",
      "no early stopping\n",
      "early stopping at epoch  1\n",
      "early stopping at epoch  151\n",
      "no early stopping\n",
      "early stopping at epoch  21\n",
      "no early stopping\n",
      "early stopping at epoch  182\n",
      "early stopping at epoch  140\n",
      "no early stopping\n",
      "early stopping at epoch  1\n",
      "early stopping at epoch  85\n",
      "early stopping at epoch  1\n",
      "no early stopping\n",
      "early stopping at epoch  22\n",
      "early stopping at epoch  30\n",
      "early stopping at epoch  24\n",
      "no early stopping\n",
      "early stopping at epoch  29\n",
      "no early stopping\n",
      "no early stopping\n",
      "early stopping at epoch  49\n",
      "no early stopping\n",
      "no early stopping\n",
      "no early stopping\n",
      "early stopping at epoch  351\n",
      "early stopping at epoch  366\n",
      "early stopping at epoch  175\n",
      "early stopping at epoch  5\n",
      "early stopping at epoch  171\n",
      "early stopping at epoch  261\n",
      "early stopping at epoch  37\n",
      "early stopping at epoch  304\n",
      "no early stopping\n",
      "no early stopping\n",
      "early stopping at epoch  54\n",
      "early stopping at epoch  1\n",
      "no early stopping\n",
      "no early stopping\n",
      "early stopping at epoch  60\n",
      "early stopping at epoch  47\n",
      "early stopping at epoch  396\n",
      "no early stopping\n",
      "no early stopping\n",
      "no early stopping\n",
      "no early stopping\n",
      "no early stopping\n",
      "early stopping at epoch  1\n",
      "early stopping at epoch  1\n",
      "early stopping at epoch  181\n",
      "early stopping at epoch  121\n",
      "early stopping at epoch  100\n",
      "early stopping at epoch  1\n",
      "no early stopping\n",
      "early stopping at epoch  356\n",
      "early stopping at epoch  1\n",
      "early stopping at epoch  1\n",
      "early stopping at epoch  3\n",
      "no early stopping\n",
      "no early stopping\n",
      "early stopping at epoch  354\n",
      "no early stopping\n",
      "early stopping at epoch  288\n",
      "no early stopping\n",
      "early stopping at epoch  29\n",
      "early stopping at epoch  161\n",
      "early stopping at epoch  421\n",
      "no early stopping\n",
      "early stopping at epoch  198\n",
      "no early stopping\n",
      "early stopping at epoch  290\n",
      "no early stopping\n",
      "no early stopping\n",
      "early stopping at epoch  29\n",
      "no early stopping\n",
      "early stopping at epoch  81\n",
      "early stopping at epoch  168\n",
      "no early stopping\n",
      "no early stopping\n",
      "early stopping at epoch  2\n",
      "no early stopping\n",
      "early stopping at epoch  1\n",
      "no early stopping\n",
      "no early stopping\n",
      "no early stopping\n",
      "early stopping at epoch  379\n",
      "early stopping at epoch  1\n",
      "no early stopping\n",
      "no early stopping\n",
      "early stopping at epoch  88\n",
      "no early stopping\n",
      "early stopping at epoch  336\n",
      "no early stopping\n",
      "early stopping at epoch  1\n",
      "early stopping at epoch  395\n",
      "early stopping at epoch  220\n",
      "early stopping at epoch  362\n",
      "no early stopping\n",
      "no early stopping\n",
      "no early stopping\n",
      "no early stopping\n",
      "no early stopping\n",
      "Model trained on product \"Prd1225\" in 54 minutes and 9 seconds\n"
     ]
    }
   ],
   "source": [
    "global device\n",
    "global verbose \n",
    "global results_folder\n",
    "global save_predictions\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "verbose = False\n",
    "save_predictions = True\n",
    "results_folder = './Results/'\n",
    "data_folder = './DATA/'\n",
    "\n",
    "\n",
    "#n_forecast_ahead\n",
    "n_steps_in = 14\n",
    "n_steps_out_list = [1,3,5,7,14]\n",
    "products_list = ['Prd1225']#open('product_List.txt','r').read().strip().split('\\n')\n",
    "for product in products_list:\n",
    "    start=time.time()\n",
    "    if verbose:\n",
    "        print(\"Opening '{0}.csv'\".format(product))\n",
    "    main(product.strip(),n_steps_in,n_steps_out_list,data_folder)\n",
    "    print('Model trained on product \"%s\" in %i minutes and %i seconds' %(str(product),int((time.time()-start)/60),float((time.time()-start)%60)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ef3d00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "78e58e174bbf6f139d99cb175ce740dfc54e8c53f1ce58a8848962c143d22910"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
