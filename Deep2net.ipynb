{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b09e58e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda\\envs\\Deeplearning\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import math\n",
    "import pandas as pd\n",
    "import random\n",
    "import gzip\n",
    "import torch\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import warnings\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import make_scorer,balanced_accuracy_score,roc_curve,precision_recall_curve,mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from statsmodels.tsa.stattools import acf\n",
    "from numpy import hstack\n",
    "import time\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4d8c02",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ac88bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class datasets():\n",
    "    def __init__(self,data,n_steps_in):\n",
    "        self.data=data\n",
    "        self.n_steps_in=n_steps_in\n",
    "        self.n0 = np.where(data['PeriodsSepLastTwoNnZeroDemands']>0)[0][0]\n",
    "        self.n1 = self.n0+int((data.shape[0]-self.n0)*90/100)\n",
    "        self.size=int((self.n1)/5)\n",
    "        in_seq1=data['ZNZDemand'].values[self.n0:].reshape((-1, 1))\n",
    "        in_seq2=data['LastQty'].values[self.n0:].reshape((-1, 1))\n",
    "        in_seq3=data['WeekDay'].values[self.n0:].reshape((-1, 1))\n",
    "        in_seq4=data['Interval'].values[self.n0:].reshape((-1, 1))\n",
    "        in_seq5=data['PeriodsSepLastTwoNnZeroDemands'].values[self.n0:].reshape((-1, 1))\n",
    "        in_seq6=data['Month'].values[self.n0:].reshape((-1, 1))\n",
    "        in_seq7=data['Qty'].values[self.n0:].reshape((-1, 1))\n",
    "        self.dataset = hstack((in_seq1, in_seq2,in_seq3,in_seq4,in_seq5,in_seq6,in_seq7))\n",
    "        if np.count_nonzero(np.isnan(self.dataset))>0:\n",
    "            print('nan found')\n",
    "        self.scaler = MinMaxScaler(feature_range = (0, 1))\n",
    "        self.scaler.fit(data[['Qty']].values[self.n0:self.n1])\n",
    "        #random.shuffle(self.dataset)\n",
    "        \n",
    "        \n",
    "    def get_deep2net_datasets(self):\n",
    "        X = []\n",
    "        for i in range(len(self.dataset)):\n",
    "            # find the end of this pattern\n",
    "            end_ix = i + self.n_steps_in\n",
    "            out_end_ix = end_ix + 1\n",
    "            # check if we are beyond the dataset\n",
    "            if (out_end_ix-1) > len(self.dataset):\n",
    "                break\n",
    "            # gather input and output parts of the pattern\n",
    "            seq_x1, seq_y1 = list(self.dataset[i:end_ix,1:-1]), list(self.dataset[end_ix-1:out_end_ix-1, 0])\n",
    "            seq_x2, seq_y2 = [self.dataset[i+self.n_steps_in-1:i+self.n_steps_in,1],self.dataset[i+self.n_steps_in-1:i+self.n_steps_in,3]], list(self.dataset[i+self.n_steps_in-1:i+self.n_steps_in,6])\n",
    "            X.append([seq_x1, seq_y1, seq_x2, seq_y2])\n",
    "        train_data=X[:self.n1]\n",
    "        test_data=X[self.n1:]\n",
    "        calib_data = X[:4*self.size]\n",
    "        valid_data = X[4*self.size:self.n1]\n",
    "        #random.shuffle(calib_data)\n",
    "        #random.shuffle(valid_data)\n",
    "        #random.shuffle(train_data)\n",
    "        #random.shuffle(test_data)\n",
    "        return calib_data,valid_data,train_data,test_data\n",
    "    \n",
    "    def get_reg_datasets(self):\n",
    "        X = []\n",
    "        dataset1=self.dataset.copy()\n",
    "        dataset1[:,-1]=self.scaler.transform(np.array(dataset1[:,-1]).reshape(len(dataset1[:,-1]),1)).ravel()\n",
    "        for i in range(self.n_steps_in-1,len(dataset1)):\n",
    "            # gather input and output parts of the pattern\n",
    "            seq_x, seq_y = dataset1[i:i+1,1:-1], dataset1[i:i+1, -1]\n",
    "            X.append([seq_x,seq_y])\n",
    "        train_data=X[:self.n1]\n",
    "        test_data=X[self.n1:]\n",
    "        calib_data = X[:4*self.size]\n",
    "        valid_data = X[4*self.size:self.n1]\n",
    "        return calib_data,valid_data,train_data,test_data,self.scaler\n",
    "    \n",
    "    def get_ADI(self):\n",
    "        ADI = (self.data['ZNZDemand'].value_counts()[1]+self.data['ZNZDemand'].value_counts()[0])/self.data['ZNZDemand'].value_counts()[1]\n",
    "        return(ADI)\n",
    "    \n",
    "    def get_n0(self):\n",
    "        return self.n0+self.n_steps_in\n",
    "    \n",
    "    def get_n1(self):\n",
    "        return self.n1+self.n0+self.n_steps_in-1\n",
    "    \n",
    "    def get_CV2(self):\n",
    "        Test_df = (self.data[self.data['ZNZDemand']==1][['Qty','Interval']]).dropna(axis=0)\n",
    "        CV2 = (Test_df['Qty'].std()/Test_df['Qty'].mean())**2\n",
    "        return (CV2)\n",
    "    \n",
    "    def get_n_obs(self):\n",
    "        n_obs=self.data.shape[0]-self.n0\n",
    "        return (n_obs)\n",
    "    \n",
    "    def get_train_Y(self):\n",
    "        return (self.dataset[:self.n1, -1].tolist())\n",
    "    \n",
    "    def get_test_Y(self):\n",
    "        return (self.dataset[self.n1+self.n_steps_in-1:, -1].tolist())\n",
    "    \n",
    "    \n",
    "    def get_Nlags(self):\n",
    "        autocorr=acf(self.data['Qty'],False,100)\n",
    "        Nlags=np.where(autocorr==max(autocorr[(autocorr<1)]))[0][0]\n",
    "        return (Nlags)\n",
    "    \n",
    "    def get_w0(self):\n",
    "        w0 = 1\n",
    "        return w0\n",
    "    \n",
    "    def get_w1(self):\n",
    "        w1 = self.data['ZNZDemand'].value_counts()[0]/self.data['ZNZDemand'].value_counts()[1]/2\n",
    "        return (w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7f3f94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset_load(Dataset):\n",
    "    def __init__(self,xy=None):\n",
    "        self.x1_data=np.asarray([el[0] for el in xy],dtype=np.float32)\n",
    "        self.y1_data =np.asarray([el[1] for el in xy ],dtype=np.float32)\n",
    "        self.x2_data=np.asarray([el[2] for el in xy],dtype=np.float32).squeeze(axis=2)\n",
    "        self.y2_data =np.asarray([el[3] for el in xy ],dtype=np.float32)\n",
    "        self.x1_data = torch.from_numpy(self.x1_data)\n",
    "        self.y1_data = torch.from_numpy(self.y1_data)\n",
    "        self.x2_data = torch.from_numpy(self.x2_data)\n",
    "        self.y2_data = torch.from_numpy(self.y2_data)\n",
    "        self.len=len(self.x1_data)\n",
    "    def __getitem__(self, index):\n",
    "        return self.x1_data[index], self.y1_data[index], self.x2_data[index], self.y2_data[index]\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bca65212",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset_load1(Dataset):\n",
    "    def __init__(self,xy=None):\n",
    "        self.x=np.asarray([el[0] for el in xy],dtype=np.float32)\n",
    "        self.y =np.asarray([el[1] for el in xy ],dtype=np.float32)\n",
    "        self.x = torch.from_numpy(self.x)\n",
    "        self.y = torch.from_numpy(self.y)\n",
    "        self.len=len(self.x)\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "063adbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logsampler(a,b):\n",
    "    x=np.random.uniform(low=0,high=1)\n",
    "    y=10**((math.log10(b)-math.log10(a))*x + math.log10(a))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf912587",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MASE(X,Y,F):\n",
    "    N1=len(X)\n",
    "    N2=len(Y)\n",
    "    D1=np.sum(abs(np.array(Y)-np.array(F)))/N2\n",
    "    D2=np.sum(abs(np.array(X[1:])-np.array(X[:-1])))/N1\n",
    "    return (D1/D2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed31e102",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSSE(X,Y,F):\n",
    "    h=len(Y)\n",
    "    n=len(X)\n",
    "    D1=(1/h)*np.sum((np.array(Y)-np.array(F))**2)\n",
    "    D2=(1/(n-1))*np.sum((np.array(X[1:])-np.array(X[:-1]))**2)\n",
    "    return (np.sqrt(D1/D2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b19bcf",
   "metadata": {},
   "source": [
    "# Network 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3554d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Deepnet(nn.Module):\n",
    "    def __init__ (self,RNN,RNN_hidden_size,RNN_mean,RNN_sigma,layer_size,dropprob,n_features,Nout=1):\n",
    "        super(Deepnet,self).__init__()\n",
    "        self.RNN=RNN\n",
    "        self.RNN_hidden_size=RNN_hidden_size\n",
    "        self.RNN_sigma=RNN_sigma   \n",
    "        self.layer_size=layer_size\n",
    "        self.RNN_mean=RNN_mean   \n",
    "        self.input_channels=n_features\n",
    "        self.dropprob=dropprob\n",
    "        self.out=Nout\n",
    "        if self.RNN=='LSTM':\n",
    "            self.rnn = nn.LSTM(self.input_channels, RNN_hidden_size, num_layers=1, bidirectional=False).to(device)\n",
    "            self.FC_size= RNN_hidden_size\n",
    "        elif self.RNN=='BiLSTM':\n",
    "            self.rnn = nn.LSTM(self.input_channels, RNN_hidden_size, num_layers=1, bidirectional=True).to(device)\n",
    "            self.FC_size= 2*RNN_hidden_size\n",
    "        elif self.RNN=='GRU':\n",
    "            self.rnn = nn.GRU(self.input_channels, RNN_hidden_size, num_layers=1, bidirectional=False).to(device)\n",
    "            self.FC_size= RNN_hidden_size\n",
    "        elif self.RNN=='BiGRU':\n",
    "            self.rnn = nn.GRU(self.input_channels, RNN_hidden_size, num_layers=1, bidirectional=True).to(device)\n",
    "            self.FC_size= 2*RNN_hidden_size\n",
    "        for layer_p in self.rnn._all_weights:\n",
    "            for p in layer_p:\n",
    "                if 'weight' in p:\n",
    "                    torch.nn.init.normal_(self.rnn.__getattr__(p),mean=RNN_mean,std=RNN_sigma)  \n",
    "        self.dropout = torch.nn.Dropout(p=dropprob, inplace=False) #Dropout Layer (Dropout rate= p)\n",
    "        #weights between LSTM or GRU layers and fully connected layer\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.FC_size,self.layer_size),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=dropprob, inplace=False),\n",
    "            nn.Linear(self.layer_size, self.out),\n",
    "            nn.Sigmoid())\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x=x.permute(1,0,2)\n",
    "        output,_=self.rnn(x)\n",
    "        if self.RNN=='BiLSTM' or self.RNN=='BiGRU':\n",
    "            Normal_RNN=output[-1, :, :self.RNN_hidden_size]\n",
    "            Rev_RNN=output[0, :, self.RNN_hidden_size:]\n",
    "            x = torch.cat((Normal_RNN, Rev_RNN), 1)\n",
    "            x=self.dropout(x)\n",
    "        else:\n",
    "            x = output[-1, :, :]\n",
    "            x=self.dropout(x)\n",
    "        x = self.classifier(x)\n",
    "        return (x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f699e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Calibration(RNN,w1,w0,calib_loader,valid_loader,n_features,metric):\n",
    "    best_AUC = 0\n",
    "    if verbose:\n",
    "        print('Training on ',device)\n",
    "    max_learning_steps = 500\n",
    "    RNN_hidden_size_list = [20, 50, 80, 100]\n",
    "    dropoutList = [0, 0.15, 0.3, 0.45]\n",
    "    layer_size_list = [32, 64]\n",
    "    learning_rate_list = [10**-5,10**-4,10**-3,10**-2]\n",
    "    max_num_models = 40\n",
    "    for number in range(max_num_models):\n",
    "        if verbose:\n",
    "            print('model {0} out of {1}'.format(number+1,max_num_models))\n",
    "        # hyper-parameters\n",
    "        RNN_hidden_size = random.choice(RNN_hidden_size_list)\n",
    "        dropprob = random.choice(dropoutList)\n",
    "        layer_size = random.choice(layer_size_list)\n",
    "        learning_rate=random.choice(learning_rate_list)\n",
    "        RNN_mean = random.choice([-1,1])*logsampler(10 ** -4, 1)\n",
    "        RNN_sigma = logsampler(10 ** -4, 10 ** -2)\n",
    "        model = Deepnet(RNN,RNN_hidden_size,RNN_mean,RNN_sigma,layer_size,dropprob,n_features).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
    "        learning_steps = 0\n",
    "        while learning_steps < max_learning_steps:\n",
    "            loss_per_epoch = 0\n",
    "            model.train()\n",
    "            for batch_idx, (data1, target1, _, _) in enumerate(calib_loader):\n",
    "                data = data1.to(device)\n",
    "                target = target1.to(device)\n",
    "                # Forward pass\n",
    "                outputs = model(data)\n",
    "                criterion = nn.BCELoss(reduction='mean',weight=((torch.abs((target)) * w1) - (torch.subtract(target,1) * w0)))\n",
    "                loss = criterion(outputs, target)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                loss_per_epoch+=loss.item()\n",
    "            learning_steps+=1\n",
    "            if (learning_steps % 100 == 0):\n",
    "                if verbose:\n",
    "                    print(\"model trained for {0} epochs, loss per epoch is {1}.\".format(learning_steps,loss_per_epoch/(batch_idx+1)))\n",
    "                with torch.no_grad():\n",
    "                    model.eval()\n",
    "                    auc = []\n",
    "                    for batch_idx, (data1, target1, _, _) in enumerate(valid_loader):\n",
    "                        data = data1.to(device)\n",
    "                        target = target1.to(device)\n",
    "                        # Forward pass\n",
    "                        output = model(data)\n",
    "                        pred = output.cpu().detach().numpy().reshape(output.shape[0])\n",
    "                        labels = target.cpu().numpy().reshape(output.shape[0])\n",
    "                        if output.shape[0] > 60:\n",
    "                            if (metric=='ROC'):\n",
    "                                auc.append(metrics.roc_auc_score(labels, pred))\n",
    "                            elif (metric=='PRC'):\n",
    "                                precision, recall, _ = precision_recall_curve(labels, pred)\n",
    "                                auc.append(metrics.auc(recall, precision))\n",
    "                            else :\n",
    "                                print('Choose proper metric')\n",
    "                    AUC = np.mean(auc)\n",
    "                    # print(AUC)\n",
    "                    if AUC > best_AUC:\n",
    "                        best_AUC = AUC\n",
    "                        best_learning_steps = learning_steps\n",
    "                        best_LearningRate = learning_rate\n",
    "                        best_RNN_hidden_size=RNN_hidden_size\n",
    "                        best_dropprob = dropprob\n",
    "                        best_layer_size= layer_size\n",
    "                        best_RNN_sigma = RNN_sigma\n",
    "                        best_RNN_mean = RNN_mean\n",
    "    if verbose:\n",
    "        print('best_AUC=', best_AUC)\n",
    "        print('best_learning_steps=', best_learning_steps)\n",
    "        print('best_LearningRate=', best_LearningRate)\n",
    "        print('best_dropprob=', best_dropprob)\n",
    "        print('best_RNN_hidden_size=', best_RNN_hidden_size)\n",
    "        print('best_layer_size=', best_layer_size)\n",
    "        print('best_RNN_sigma=', best_RNN_sigma)\n",
    "        print('best_RNN_mean=', best_RNN_mean)\n",
    "    best_hyperparameters = {'best_learning_steps': best_learning_steps, \n",
    "                            'best_LearningRate': best_LearningRate,\n",
    "                            'best_dropprob': best_dropprob, \n",
    "                            'best_RNN_hidden_size': best_RNN_hidden_size,\n",
    "                            'best_layer_size': best_layer_size, \n",
    "                            'best_RNN_sigma': best_RNN_sigma,\n",
    "                            'best_RNN_mean':best_RNN_mean}\n",
    "    return best_hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b84c7539",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_model(RNN,w1,w0,best_hyperparameters,train_loader,n_features,metric):\n",
    "    best_learning_steps=8#best_hyperparameters['best_learning_steps']\n",
    "    best_LearningRate=best_hyperparameters['best_LearningRate']\n",
    "    best_RNN_hidden_size=best_hyperparameters['best_RNN_hidden_size']\n",
    "    best_dropprob=best_hyperparameters['best_dropprob']\n",
    "    best_RNN_mean=best_hyperparameters['best_RNN_mean']\n",
    "    best_RNN_sigma=best_hyperparameters['best_RNN_sigma']\n",
    "    best_layer_size=best_hyperparameters['best_layer_size']\n",
    "    best_AUC=0\n",
    "    best_threshold=0.5\n",
    "    for number_models in range(5):\n",
    "        model = Deepnet(RNN,best_RNN_hidden_size,best_RNN_mean,best_RNN_sigma,best_layer_size,best_dropprob,n_features).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(),lr=best_LearningRate)\n",
    "        learning_steps=0\n",
    "        model.train()\n",
    "        while learning_steps<best_learning_steps:\n",
    "            for batch_idx, (data1, target1, _, _) in enumerate(train_loader):\n",
    "                data = data1.to(device)\n",
    "                target = target1.to(device)\n",
    "                outputs = model(data)\n",
    "                criterion = nn.BCELoss(reduction='mean',weight=((torch.abs((target)) * w1) - (torch.subtract(target,1) * w0)))\n",
    "                loss = criterion(outputs, target)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            learning_steps+=1\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            auc=[]\n",
    "            threshold=[]\n",
    "            for batch_idx, (data1, target1, _, _) in enumerate(train_loader):\n",
    "                data = data1.to(device)\n",
    "                target = target1.to(device)\n",
    "                # Forward pass\n",
    "                output = model(data)\n",
    "                pred = output.cpu().detach().numpy().reshape(output.shape[0])\n",
    "                labels=target.cpu().numpy().reshape(output.shape[0])\n",
    "                if pred.shape[0]>60:\n",
    "                    if (metric=='ROC'):\n",
    "                        auc.append(metrics.roc_auc_score(labels, pred))\n",
    "                        fpr, tpr, thresholds = roc_curve(labels, pred)\n",
    "                        gmeans=np.sqrt(tpr*(1-fpr))\n",
    "                        threshold.append(thresholds[np.argmax(gmeans)])\n",
    "                    elif (metric=='PRC'):\n",
    "                        precision, recall, thresholds = precision_recall_curve(labels, pred)\n",
    "                        auc.append(metrics.auc(recall, precision))\n",
    "                        gmeans=(2*precision*recall)/(precision+recall)\n",
    "                        threshold.append(thresholds[np.argmax(gmeans)])\n",
    "                    else :\n",
    "                        print('Choose proper metric') \n",
    "            AUC_training=np.mean(auc)\n",
    "            if verbose:\n",
    "                print('AUC on training data for model ',number_models+1,' = ',AUC_training)\n",
    "            if AUC_training>best_AUC:\n",
    "                best_AUC=AUC_training\n",
    "                best_threshold=np.mean(np.array(threshold),axis=0)\n",
    "                best_model=model\n",
    "    return best_model,best_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d305e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_predict(best_model,test_loader):\n",
    "    with torch.no_grad():\n",
    "        best_model.eval()\n",
    "        auc = []\n",
    "        for batch_idx, (data1, target1, _, _) in enumerate(test_loader):\n",
    "            data = data1.to(device)\n",
    "            target = target1.to(device)\n",
    "            # Forward pass\n",
    "            output = best_model(data)\n",
    "            pred = output.cpu().detach().numpy().reshape(output.shape[0])\n",
    "            labels = target.cpu().numpy().reshape(output.shape[0])\n",
    "            if output.shape[0] > 50:\n",
    "                auc.append(metrics.roc_auc_score(labels, pred))\n",
    "        AUC_test = np.mean(auc)\n",
    "        if verbose:\n",
    "            print('AUC on test data is ',AUC_test)\n",
    "    return (AUC_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e75857",
   "metadata": {},
   "source": [
    "# Network 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eecef793",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Deepnet2(nn.Module):\n",
    "    def __init__ (self,layer_number,layer_size,dropprob,n_features):\n",
    "        super(Deepnet2,self).__init__()\n",
    "                \n",
    "        self.layer_size=layer_size\n",
    "        self.layer_number=layer_number\n",
    "        self.input_channels=n_features\n",
    "        self.dropprob=dropprob\n",
    "        \n",
    "\n",
    "        \n",
    "        self.FC=self.layer_size\n",
    "        self.fc1=nn.Linear(self.input_channels,self.layer_size)\n",
    "        if (self.layer_number>1):\n",
    "            self.fc2=nn.Linear(self.layer_size,self.layer_size)\n",
    "        if (self.layer_number>2):\n",
    "            self.fc3=nn.Linear(self.layer_size,self.layer_size)\n",
    "        self.fc_f=nn.Linear(self.layer_size,1)\n",
    "        \n",
    "        self.dropout = torch.nn.Dropout(p=dropprob, inplace=False) #Dropout Layer (Dropout rate= p)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x=F.relu(self.fc1(x))\n",
    "        x=self.dropout(x)\n",
    "        if (self.layer_number>1):\n",
    "            x=F.relu(self.fc2(x))\n",
    "            x=self.dropout(x)\n",
    "        \n",
    "        if (self.layer_number>2):\n",
    "            x=F.relu(self.fc3(x))\n",
    "            x=self.dropout(x)\n",
    "        \n",
    "        x=F.relu(self.fc_f(x))\n",
    "\n",
    "        return (x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "848ba664",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Calibration2(calib_loader,valid_loader,n_features,best_model1,best_threshold):\n",
    "    best_MSE = 100\n",
    "    if verbose:\n",
    "        print('Training on ',device)\n",
    "    max_learning_steps = 500\n",
    "    dropoutList = [0, 0.15, 0.3, 0.45]\n",
    "    layer_number_list=[1,2,3]\n",
    "    layer_size_list = [16, 32]\n",
    "    learning_rate_list = [10**-5,10**-4,10**-3,10**-2]\n",
    "    max_num_models = 40\n",
    "    for number in range(max_num_models):\n",
    "        if verbose:\n",
    "            print('model {0} out of {1}'.format(number+1,max_num_models))\n",
    "        # hyper-parameters\n",
    "        dropprob = random.choice(dropoutList)\n",
    "        layer_number=random.choice(layer_number_list)\n",
    "        layer_size = random.choice(layer_size_list)\n",
    "        learning_rate=random.choice(learning_rate_list)\n",
    "        model_MSE = []\n",
    "        model = Deepnet2(layer_number,layer_size,dropprob,n_features).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
    "\n",
    "        learning_steps = 0\n",
    "        while learning_steps <= max_learning_steps:\n",
    "            loss_per_epoch = 0\n",
    "            model.train()\n",
    "            for batch_idx, (data1, target1, data2, target2) in enumerate(calib_loader):\n",
    "                data = data1.to(device)\n",
    "                target = target1.to(device)\n",
    "                # Forward pass\n",
    "                ZNZ_output = best_model1(data).cpu().detach().numpy()\n",
    "                ZNZ_output=np.where(ZNZ_output<best_threshold,0,ZNZ_output)\n",
    "                ZNZ_output=np.where(ZNZ_output>best_threshold,1,ZNZ_output)\n",
    "                data2 = torch.hstack((data2,torch.tensor(ZNZ_output)))\n",
    "                data = data2.to(device)\n",
    "                target = target2.to(device)\n",
    "                # Forward pass\n",
    "                output = model(data)\n",
    "                criterion = nn.MSELoss()\n",
    "                loss = criterion(output,target)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                loss_per_epoch+=loss.item()\n",
    "            learning_steps += 1\n",
    "            if learning_steps % 100 == 0:\n",
    "                if verbose:\n",
    "                    print(\"model trained for {0} epochs, loss per epoch is {1}.\".format(learning_steps,loss_per_epoch/(batch_idx+1)))\n",
    "                with torch.no_grad():\n",
    "                    model.eval()\n",
    "                    mse = []\n",
    "                    for batch_idx, (data1, target1, data2, target2) in enumerate(valid_loader):\n",
    "                        data = data1.to(device)\n",
    "                        target = target1.to(device)\n",
    "                        # Forward pass\n",
    "                        ZNZ_output = best_model1(data).cpu().detach().numpy()\n",
    "                        ZNZ_output=np.where(ZNZ_output<best_threshold,0,ZNZ_output)\n",
    "                        ZNZ_output=np.where(ZNZ_output>best_threshold,1,ZNZ_output)\n",
    "                        data2 = torch.hstack((data2,torch.tensor(ZNZ_output)))\n",
    "                        data = data2.to(device)\n",
    "                        target = target2.to(device)\n",
    "                        # Forward pass\n",
    "                        output = model(data)\n",
    "                        pred = output.cpu().detach().numpy().reshape(output.shape[0])\n",
    "                        labels = target.cpu().numpy().reshape(output.shape[0])\n",
    "                        if output.shape[0] > 50:\n",
    "                            mse.append(metrics.mean_squared_error(labels, pred))  \n",
    "                    MSE=np.mean(mse)\n",
    "                    if MSE < best_MSE:\n",
    "                        best_MSE = MSE\n",
    "                        best_learning_steps = learning_steps\n",
    "                        best_LearningRate = learning_rate\n",
    "                        best_dropprob = dropprob\n",
    "                        best_layer_number= layer_number\n",
    "                        best_layer_size= layer_size\n",
    "    \n",
    "    if verbose:\n",
    "        print('best_MSE=', best_MSE)\n",
    "        print('best_learning_steps=', best_learning_steps)\n",
    "        print('best_LearningRate=', best_LearningRate)\n",
    "        print('best_dropprob=', best_dropprob)\n",
    "        print('best_layer_number=', best_layer_number)\n",
    "        print('best_layer_size=', best_layer_size)\n",
    "\n",
    "    best_hyperparameters = {'best_learning_steps': best_learning_steps, \n",
    "                            'best_LearningRate': best_LearningRate,\n",
    "                            'best_dropprob': best_dropprob, \n",
    "                            'best_layer_number': best_layer_number,\n",
    "                            'best_layer_size': best_layer_size}\n",
    "    return best_hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a2b7ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_model2(best_hyperparameters,train_loader,n_features,best_model1,best_threshold):\n",
    "    best_learning_steps=best_hyperparameters['best_learning_steps']\n",
    "    best_LearningRate=best_hyperparameters['best_LearningRate']\n",
    "    best_dropprob=best_hyperparameters['best_dropprob']\n",
    "    best_layer_number=best_hyperparameters['best_layer_number']\n",
    "    best_layer_size=best_hyperparameters['best_layer_size']\n",
    "    best_MSE=100\n",
    "\n",
    "    for number_models in range(5):\n",
    "        model = Deepnet2(best_layer_number,best_layer_size,best_dropprob,n_features).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(),lr=best_LearningRate)\n",
    "\n",
    "        learning_steps=0\n",
    "        model.train()\n",
    "        while learning_steps<=best_learning_steps:\n",
    "            for batch_idx, (data1, target1, data2, target2) in enumerate(train_loader):\n",
    "                data = data1.to(device)\n",
    "                target = target1.to(device)\n",
    "                # Forward pass\n",
    "                ZNZ_output = best_model1(data).cpu().detach().numpy()\n",
    "                ZNZ_output=np.where(ZNZ_output<best_threshold,0,ZNZ_output)\n",
    "                ZNZ_output=np.where(ZNZ_output>best_threshold,1,ZNZ_output)\n",
    "                data2 = torch.hstack((data2,torch.tensor(ZNZ_output)))\n",
    "                data = data2.to(device)\n",
    "                target = target2.to(device)\n",
    "                output = model(data)\n",
    "                loss = nn.MSELoss()\n",
    "                L=loss(output,target)\n",
    "                optimizer.zero_grad()\n",
    "                L.backward()\n",
    "                optimizer.step()\n",
    "            learning_steps+=1\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            mse=[]\n",
    "            for batch_idx, (data1, target1, data2, target2) in enumerate(train_loader):\n",
    "                data = data1.to(device)\n",
    "                target = target1.to(device)\n",
    "                # Forward pass\n",
    "                ZNZ_output = best_model1(data).cpu().detach().numpy()\n",
    "                ZNZ_output=np.where(ZNZ_output<best_threshold,0,ZNZ_output)\n",
    "                ZNZ_output=np.where(ZNZ_output>best_threshold,1,ZNZ_output)\n",
    "                data2 = torch.hstack((data2,torch.tensor(ZNZ_output)))\n",
    "                data = data2.to(device)\n",
    "                target = target2.to(device)\n",
    "                # Forward pass\n",
    "                output = model(data)\n",
    "                pred=output.cpu().detach().numpy().reshape(output.shape[0])\n",
    "                labels=target.cpu().numpy().reshape(output.shape[0])\n",
    "                if output.shape[0]>60:\n",
    "                    mse.append(metrics.mean_squared_error(labels, pred))\n",
    "            MSE_Training=np.mean(mse)     \n",
    "            if verbose:\n",
    "                print('MSE on training data for model ',number_models+1,' = ',MSE_Training)\n",
    "            if MSE_Training<best_MSE:\n",
    "                best_MSE=MSE_Training\n",
    "                best_model=model\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5967f274",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_generate_data(data1,pred1,data2,pred2,step_idx):\n",
    "    if pred1==0:\n",
    "        lstqt = data1[step_idx][-1][0]\n",
    "        weekday = data1[step_idx+1][-1][1]\n",
    "        interv = data1[step_idx+1][-1][2]\n",
    "        PSLTNZD = data1[step_idx+1][-1][3]\n",
    "        Month = data1[step_idx+1][-1][-1]\n",
    "    else:\n",
    "        lstqt = pred2\n",
    "        weekday = data1[step_idx+1][-1][1]\n",
    "        interv = 1\n",
    "        PSLTNZD = data1[step_idx][-1][2]\n",
    "        Month = data1[step_idx+1][-1][-1]\n",
    "    new_data1 = [lstqt,weekday,interv,PSLTNZD,Month]\n",
    "    new_data2 = [lstqt,interv]\n",
    "    data1[step_idx+1] = torch.vstack((data1[step_idx][1:],torch.tensor(new_data1).unsqueeze(dim=0)))\n",
    "    data2[step_idx+1] = torch.tensor(new_data2)\n",
    "    return (data1, data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d83b5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_predict2(best_model1,best_model2,test_loader,best_threshold,n_steps_out):\n",
    "    labels = []\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        best_model1.eval()\n",
    "        best_model2.eval()\n",
    "        for batch_idx, (data1, target1, data2, target2) in enumerate(test_loader):\n",
    "            n = data1.shape[0]\n",
    "            preds = []\n",
    "            for step_idx in range (n):\n",
    "                data = data1[step_idx].unsqueeze(dim=0).to(device)\n",
    "                # Forward pass\n",
    "                ZNZ_output = best_model1(data).cpu().detach().numpy()\n",
    "                ZNZ_output=np.where(ZNZ_output<best_threshold,0,ZNZ_output)\n",
    "                ZNZ_output=np.where(ZNZ_output>best_threshold,1,ZNZ_output)\n",
    "                data = torch.hstack((data2[step_idx].unsqueeze(dim=0),torch.tensor(ZNZ_output)))\n",
    "                data = data.to(device)\n",
    "                # Forward pass\n",
    "                output = best_model2(data)\n",
    "                pred=output.cpu().detach() * ZNZ_output\n",
    "                if step_idx<n-1:\n",
    "                    data1,data2 = test_generate_data(data1,ZNZ_output,data2,pred,step_idx)\n",
    "                preds.append(pred.ravel())\n",
    "            if (n==n_steps_out):\n",
    "                labels.extend(target2.ravel())\n",
    "                predictions.extend(preds)\n",
    "    return (labels,predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4409d3",
   "metadata": {},
   "source": [
    "# Regression Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "85f16749",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegNetwork(nn.Module):\n",
    "    def __init__(self,RNN , RNN_type , RNN_size , RNN_mean , RNN_sigma , NN_number , NN_size , NN_mean , NN_sigma ,\n",
    "                 dropprob , n_features):\n",
    "        super(RegNetwork,self).__init__()\n",
    "        self.RNN=RNN\n",
    "        self.RNN_type=RNN_type\n",
    "        self.RNN_size=RNN_size\n",
    "        self.RNN_mean=RNN_mean        \n",
    "        self.RNN_sigma=RNN_sigma\n",
    "        self.NN_number=NN_number\n",
    "        self.NN_mean=NN_mean\n",
    "        self.NN_sigma=NN_sigma\n",
    "        self.dropprob=dropprob\n",
    "        self.input_channels=n_features\n",
    "        self.FC_size=NN_size\n",
    "        if self.RNN:\n",
    "            if RNN_type=='LSTM':\n",
    "                self.rnn = nn.LSTM(self.input_channels, RNN_size, num_layers=1, bidirectional=False).to(device)\n",
    "                self.input_channels= RNN_size\n",
    "            elif RNN_type=='BiLSTM':\n",
    "                self.rnn = nn.LSTM(self.input_channels, RNN_size, num_layers=1, bidirectional=True).to(device)\n",
    "                self.input_channels= 2*RNN_size\n",
    "            elif RNN_type=='GRU':\n",
    "                self.rnn = nn.GRU(self.input_channels, RNN_size, num_layers=1, bidirectional=False).to(device)\n",
    "                self.input_channels= RNN_size\n",
    "            elif RNN_type=='BiGRU':\n",
    "                self.rnn = nn.GRU(self.input_channels, RNN_size, num_layers=1, bidirectional=True).to(device)\n",
    "                self.input_channels= 2*RNN_size\n",
    "\n",
    "            for layer_p in self.rnn._all_weights:\n",
    "                for p in layer_p:\n",
    "                    if 'weight' in p:\n",
    "                        torch.nn.init.normal_(self.rnn.__getattr__(p),mean=self.RNN_mean,std=self.RNN_sigma)\n",
    "        \n",
    "        self.fc1=nn.Linear(self.input_channels,self.FC_size)\n",
    "        torch.nn.init.normal_(self.fc1.weight,mean=self.NN_mean,std=self.NN_sigma)\n",
    "        torch.nn.init.normal_(self.fc1.bias,mean=self.NN_mean,std=self.NN_sigma)\n",
    "        self.classifier1 = nn.Sequential(\n",
    "            self.fc1,\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=dropprob, inplace=False))\n",
    "        if (self.NN_number>1):\n",
    "            self.fc2=nn.Linear(self.FC_size,self.FC_size)\n",
    "            torch.nn.init.normal_(self.fc2.weight,mean=self.NN_mean,std=self.NN_sigma)\n",
    "            torch.nn.init.normal_(self.fc2.bias,mean=self.NN_mean,std=self.NN_sigma)\n",
    "            self.classifier2 = nn.Sequential(\n",
    "                self.fc2,\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(p=dropprob, inplace=False))\n",
    "        if (self.NN_number>2):\n",
    "            self.fc3=nn.Linear(self.FC_size,self.FC_size)\n",
    "            torch.nn.init.normal_(self.fc3.weight,mean=self.NN_mean,std=self.NN_sigma)\n",
    "            torch.nn.init.normal_(self.fc3.bias,mean=self.NN_mean,std=self.NN_sigma)\n",
    "            self.classifier3 = nn.Sequential(\n",
    "                self.fc3,\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(p=dropprob, inplace=False))\n",
    "        self.fc_f=nn.Linear(self.FC_size,1)\n",
    "        torch.nn.init.normal_(self.fc_f.weight,mean=self.NN_mean,std=self.NN_sigma)\n",
    "        torch.nn.init.normal_(self.fc_f.bias,mean=self.NN_mean,std=self.NN_sigma)\n",
    "        self.dropout = torch.nn.Dropout(p=dropprob, inplace=False) #Dropout Layer (Dropout rate= p)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        if self.RNN:\n",
    "            x=x.permute(1,0,2)\n",
    "            output, _ = self.rnn(x)\n",
    "            if self.RNN_type=='BiLSTM' or self.RNN_type=='BiGRU':\n",
    "                Normal_RNN=output[-1, :, :self.RNN_size]\n",
    "                Rev_RNN=output[0, :, self.RNN_size:]\n",
    "                x = torch.cat((Normal_RNN, Rev_RNN), 1)\n",
    "                x = F.relu(x)\n",
    "                x = self.dropout(x)\n",
    "            else:\n",
    "                x = output[-1, :, :]\n",
    "                x = F.relu(x)\n",
    "                x = self.dropout(x)\n",
    "                \n",
    "        x=self.classifier1(x)\n",
    "        if (self.NN_number>1):\n",
    "            x=self.classifier2(x)\n",
    "        \n",
    "        if (self.NN_number>2):\n",
    "            x=self.classifier3(x)\n",
    "        \n",
    "        predict = torch.sigmoid(self.fc_f(x))\n",
    "        return (predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5e263212",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Calibration3(RNN,RNN_type,calib_loader,valid_loader,n_features):\n",
    "    best_MSE = 100\n",
    "    if verbose:\n",
    "        print('Training on ',device)\n",
    "    RNN_hidden_size_list = [20, 50, 80, 100]\n",
    "    dropoutList = [0, 0.15, 0.3, 0.45]\n",
    "    weight_decay_list=[10**-5,10**-4,10**-3,10**-2]\n",
    "    layer_number_list=[2,3]\n",
    "    layer_size_list = [32, 64]\n",
    "    learning_rate_list = [10**-5,10**-4,10**-3,10**-3]\n",
    "    max_learning_steps = 400\n",
    "    optimizers =['adam','sgd']\n",
    "    max_num_models = 40\n",
    "    for number in range(max_num_models):\n",
    "        if verbose:\n",
    "            print('model {0} out of {1}'.format(number+1,max_num_models))\n",
    "        # hyper-parameters\n",
    "        RNN_hidden_size = random.choice(RNN_hidden_size_list)\n",
    "        dropprob = random.choice(dropoutList)\n",
    "        RNN_mean=random.choice([-1,1])*logsampler(10 ** -4, 1)\n",
    "        RNN_sigma = logsampler(10 ** -4, 1)\n",
    "        Weight_decay=random.choice(weight_decay_list)\n",
    "        optimizer_ = random.choice(optimizers)\n",
    "        if RNN:\n",
    "            layer_number=1\n",
    "        else: \n",
    "            layer_number=random.choice(layer_number_list)\n",
    "        layer_size = random.choice(layer_size_list)\n",
    "        learning_rate=random.choice(learning_rate_list)\n",
    "        NN_mean=random.choice([-1,1])*logsampler(10 ** -4, 1)\n",
    "        N_sigma = logsampler(10 ** -4, 1)\n",
    "        model_MSE = []\n",
    "        \n",
    "        #init model\n",
    "        model = RegNetwork(RNN,RNN_type,RNN_hidden_size,RNN_mean,RNN_sigma,layer_number,layer_size,NN_mean,N_sigma,\n",
    "                           dropprob,n_features).to(device)\n",
    "        if optimizer_ == 'sgd':\n",
    "            optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate,weight_decay=Weight_decay)\n",
    "        else:\n",
    "            optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate,weight_decay=Weight_decay)\n",
    "        #training\n",
    "        learning_steps = 0\n",
    "        while learning_steps <= max_learning_steps:\n",
    "            loss_per_epoch = 0\n",
    "            model.train()\n",
    "            for batch_idx, (data, target) in enumerate(calib_loader):\n",
    "                data = data.to(device)\n",
    "                target = target.to(device)\n",
    "                # Forward pass\n",
    "                output = model(data)\n",
    "                criterion = nn.MSELoss()\n",
    "                loss = criterion(output,target)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                loss_per_epoch+=loss.item()\n",
    "            learning_steps += 1\n",
    "            if learning_steps % 100 == 0:\n",
    "                if verbose:\n",
    "                    print(\"model trained for {0} epochs, loss per epoch is {1}.\".format(learning_steps,loss_per_epoch/(batch_idx+1)))\n",
    "                with torch.no_grad():\n",
    "                    model.eval()\n",
    "                    mse = []\n",
    "                    for batch_idx, (data, target) in enumerate(valid_loader):\n",
    "                        data = data.to(device)\n",
    "                        target = target.to(device)\n",
    "                        # Forward pass\n",
    "                        output = model(data)\n",
    "                        pred = output.cpu().detach().numpy().reshape(output.shape[0])\n",
    "                        labels = target.cpu().numpy().reshape(output.shape[0])\n",
    "                        if output.shape[0] > 60:\n",
    "                            mse.append(metrics.mean_squared_error(labels, pred))\n",
    "                    MSE=np.mean(mse)\n",
    "                    if MSE < best_MSE:\n",
    "                        best_MSE = MSE\n",
    "                        best_learning_steps = learning_steps\n",
    "                        best_LearningRate = learning_rate\n",
    "                        best_dropprob = dropprob\n",
    "                        best_layer_number= layer_number\n",
    "                        best_layer_size= layer_size\n",
    "                        best_NN_mean=NN_mean\n",
    "                        best_N_sigma=N_sigma\n",
    "                        best_RNN_hidden_size= RNN_hidden_size\n",
    "                        best_RNN_mean=RNN_mean\n",
    "                        best_RNN_sigma=RNN_sigma\n",
    "                        best_weight_decay=Weight_decay\n",
    "                        best_optimizer=optimizer_\n",
    "    \n",
    "    if verbose:\n",
    "        print('best_MSE=', best_MSE)\n",
    "        print('best_learning_steps=', best_learning_steps)\n",
    "        print('best_LearningRate=', best_LearningRate)\n",
    "        print('best_dropprob=', best_dropprob)\n",
    "        print('best_weight_decay=',best_weight_decay)\n",
    "        print('best_layer_number=', best_layer_number)\n",
    "        print('best_layer_size=', best_layer_size)\n",
    "        print('best_NN_mean=', best_NN_mean)\n",
    "        print('best_N_sigma=', best_N_sigma)\n",
    "        print('best_RNN_hidden_size=', best_RNN_hidden_size)\n",
    "        print('best_RNN_mean=', best_RNN_mean)\n",
    "        print('best_RNN_sigma=', best_RNN_sigma)\n",
    "        print('best_optimizer=', best_optimizer)\n",
    "    best_hyperparameters = {'best_learning_steps': best_learning_steps, \n",
    "                            'best_LearningRate': best_LearningRate,\n",
    "                            'best_dropprob': best_dropprob, \n",
    "                            'best_weight_decay':best_weight_decay,\n",
    "                            'best_layer_number': best_layer_number,\n",
    "                            'best_layer_size': best_layer_size, \n",
    "                            'best_NN_mean':best_NN_mean,\n",
    "                            'best_N_sigma':best_N_sigma,\n",
    "                            'best_RNN_hidden_size' :best_RNN_hidden_size,\n",
    "                            'best_RNN_mean':best_RNN_mean,\n",
    "                            'best_RNN_sigma': best_RNN_sigma,\n",
    "                            'best_optimizer': best_optimizer}\n",
    "    return best_hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "06f41a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_model3(RNN,RNN_type,best_hyperparameters,train_loader,n_features):\n",
    "    best_learning_steps=best_hyperparameters['best_learning_steps']\n",
    "    best_LearningRate=best_hyperparameters['best_LearningRate']\n",
    "    best_dropprob=best_hyperparameters['best_dropprob']\n",
    "    best_weight_decay=best_hyperparameters['best_weight_decay']\n",
    "    best_layer_number=best_hyperparameters['best_layer_number']\n",
    "    best_N_sigma=best_hyperparameters['best_N_sigma']\n",
    "    best_layer_size=best_hyperparameters['best_layer_size']\n",
    "    best_RNN_hidden_size=best_hyperparameters['best_RNN_hidden_size']\n",
    "    best_RNN_sigma=best_hyperparameters['best_RNN_sigma']\n",
    "    best_RNN_mean=best_hyperparameters['best_RNN_mean']\n",
    "    best_NN_mean=best_hyperparameters['best_NN_mean']\n",
    "    best_optimizer=best_hyperparameters['best_optimizer']\n",
    "    best_MSE=100\n",
    "\n",
    "    for number_models in range(5):\n",
    "        model = RegNetwork(RNN,RNN_type,best_RNN_hidden_size,best_RNN_mean,best_RNN_sigma,best_layer_number,\n",
    "                           best_layer_size,best_NN_mean,best_N_sigma,best_dropprob,n_features).to(device)\n",
    "        if best_optimizer == 'sgd':\n",
    "            optimizer = torch.optim.SGD(model.parameters(),lr=best_LearningRate,weight_decay=best_weight_decay)\n",
    "        else:\n",
    "            optimizer = torch.optim.Adam(model.parameters(),lr=best_LearningRate,weight_decay=best_weight_decay)\n",
    "        learning_steps=0\n",
    "        model.train()\n",
    "        while (learning_steps<=best_learning_steps):    \n",
    "            for batch_idx, (data, target) in enumerate (train_loader):\n",
    "                data = data.to(device)\n",
    "                target = target.to(device)\n",
    "                # Forward pass\n",
    "                output = model(data)\n",
    "                L=F.mse_loss(target,output)\n",
    "                optimizer.zero_grad()\n",
    "                L.backward()\n",
    "                optimizer.step()\n",
    "            learning_steps+=1\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            mse=[]\n",
    "            for batch_idx, (data, target) in enumerate(train_loader):\n",
    "                data = data.to(device)\n",
    "                target = target.to(device)\n",
    "                # Forward pass\n",
    "                output = model(data)\n",
    "                pred=output.cpu().detach().numpy().reshape(output.shape[0])\n",
    "                labels=target.cpu().numpy().reshape(output.shape[0])\n",
    "                if (output.shape[0]>30):\n",
    "                    mse.append(metrics.mean_squared_error(labels, pred))\n",
    "            MSE_Training=np.mean(mse)     \n",
    "            if verbose:\n",
    "                print('MSE on training data for model ',number_models+1,' = ',MSE_Training)\n",
    "            if (MSE_Training<best_MSE):\n",
    "                best_MSE=MSE_Training\n",
    "                best_model=model\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3d997389",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_predict3(best_model,test_loader):\n",
    "    with torch.no_grad():\n",
    "        best_model.eval()\n",
    "        for batch_idx, (data, target) in enumerate(test_loader):\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            # Forward pass\n",
    "            output = best_model(data)\n",
    "            pred = output.cpu().detach().numpy().reshape(output.shape[0])\n",
    "            labels = target.cpu().numpy().reshape(output.shape[0])\n",
    "    return (pred,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cda9d3",
   "metadata": {},
   "source": [
    "# SES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "faf38a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SES(ts,alpha=0.1):\n",
    "    d = np.array(ts) # Transform the input into a numpy array\n",
    "    cols = len(d) # Historical period length\n",
    "    \n",
    "    #level (a), periodicity(p) and forecast (f)\n",
    "    f = np.full((cols),np.nan)\n",
    "    \n",
    "    # Initialization\n",
    "    f[0] = d[0]\n",
    "# Create all the t+1 forecasts\n",
    "    for t in range(0,cols-1):        \n",
    "        f[t+1] = alpha*d[t] + (1-alpha)*f[t]   \n",
    "                      \n",
    "    df = pd.DataFrame.from_dict({\"Demand\":d,\"Forecast\":f,\"Error\":d-f})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2fe25812",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_SES(DATA_Micro,n1,n2):\n",
    "    Opt_SES=[0,10000]\n",
    "    for i in np.arange (0,1,0.001):\n",
    "        SES_train=SES(DATA_Micro.Qty[n1:n2],alpha=i)\n",
    "        if (np.sqrt(mean_squared_error(SES_train['Forecast'].values,DATA_Micro['Qty'][n1:n2].values))<Opt_SES[1]):\n",
    "            Opt_SES[1]=np.sqrt(mean_squared_error(SES_train['Forecast'].values,DATA_Micro['Qty'][n1:n2].values))\n",
    "            Opt_SES[0]=i\n",
    "    return (Opt_SES[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612d52db",
   "metadata": {},
   "source": [
    "# Croston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "686cadc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Croston(ts,extra_periods=1,alpha=0.4):\n",
    "    d = np.array(ts) # Transform the input into a numpy array\n",
    "    cols = len(d) # Historical period length\n",
    "    d = np.append(d,[np.nan]*extra_periods) # Append np.nan into the demand array to cover future periods\n",
    "    \n",
    "    #level (a), periodicity(p) and forecast (f)\n",
    "    a,p,f = np.full((3,cols+extra_periods),np.nan)\n",
    "    q = 1 #periods since last demand observation\n",
    "    \n",
    "    # Initialization\n",
    "    first_occurence = np.argmax(d[:cols]>0)\n",
    "    a[0] = d[first_occurence]\n",
    "    p[0] = 1 + first_occurence\n",
    "    f[0] = a[0]/p[0]\n",
    "# Create all the t+1 forecasts\n",
    "    for t in range(0,cols):        \n",
    "        if d[t] > 0:\n",
    "            a[t+1] = alpha*d[t] + (1-alpha)*a[t] \n",
    "            p[t+1] = alpha*q + (1-alpha)*p[t]\n",
    "            f[t+1] = a[t+1]/p[t+1]\n",
    "            q = 1           \n",
    "        else:\n",
    "            a[t+1] = a[t]\n",
    "            p[t+1] = p[t]\n",
    "            f[t+1] = f[t]\n",
    "            q += 1\n",
    "       \n",
    "    # Future Forecast \n",
    "    a[cols+1:cols+extra_periods] = a[cols]\n",
    "    p[cols+1:cols+extra_periods] = p[cols]\n",
    "    f[cols+1:cols+extra_periods] = f[cols]\n",
    "                      \n",
    "    df = pd.DataFrame.from_dict({\"Demand\":d,\"Forecast\":f,\"Period\":p,\"Level\":a,\"Error\":d-f})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "86b4b7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_CR(DATA_Micro,n1,n2):\n",
    "    Opt_CR=[0,100]\n",
    "    for i in np.arange (0,1,0.001):\n",
    "        Cr_train=Croston(DATA_Micro.Qty[n1:n2],alpha=i)\n",
    "        if (np.sqrt(mean_squared_error(Cr_train['Forecast'].values[:-1],DATA_Micro['Qty'][n1:n2].values))<Opt_CR[1]):\n",
    "            Opt_CR[1]=np.sqrt(mean_squared_error(Cr_train['Forecast'].values[:-1],DATA_Micro['Qty'][n1:n2].values))\n",
    "            Opt_CR[0]=i\n",
    "    return(Opt_CR[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13821cb",
   "metadata": {},
   "source": [
    "# SBA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c1749ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SBA(ts,extra_periods=1,alpha=0.1,beta=0.1):\n",
    "    d = np.array(ts) # Transform the input into a numpy array\n",
    "    cols = len(d) # Historical period length\n",
    "    d = np.append(d,[np.nan]*extra_periods) # Append np.nan into the demand array to cover future periods\n",
    "    \n",
    "    #level (a), periodicity(p) and forecast (f)\n",
    "    a,p,f = np.full((3,cols+extra_periods),np.nan)\n",
    "    q = 1 #periods since last demand observation\n",
    "    \n",
    "    # Initialization\n",
    "    first_occurence = np.argmax(d[:cols]>0)\n",
    "    a[0] = d[first_occurence]\n",
    "    p[0] = 1 + first_occurence\n",
    "    f[0] = a[0]/p[0]\n",
    "# Create all the t+1 forecasts\n",
    "    for t in range(0,cols):        \n",
    "        if d[t] > 0:\n",
    "            a[t+1] = alpha*d[t] + (1-alpha)*a[t] \n",
    "            p[t+1] = beta*q + (1-beta)*p[t]\n",
    "            f[t+1] = (1-alpha/2)*a[t+1]/p[t+1]\n",
    "            q = 1           \n",
    "        else:\n",
    "            a[t+1] = a[t]\n",
    "            p[t+1] = p[t]\n",
    "            f[t+1] = f[t]\n",
    "            q += 1\n",
    "       \n",
    "    # Future Forecast \n",
    "    a[cols+1:cols+extra_periods] = a[cols]\n",
    "    p[cols+1:cols+extra_periods] = p[cols]\n",
    "    f[cols+1:cols+extra_periods] = f[cols]\n",
    "                      \n",
    "    df = pd.DataFrame.from_dict({\"Demand\":d,\"Forecast\":f,\"Period\":p,\"Level\":a,\"Error\":d-f})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d4379bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_SBA(DATA_Micro,n1,n2):\n",
    "    Opt_SBA=[0,0,100]\n",
    "    for i in np.arange (0,1,0.01):\n",
    "        for j in np.arange (0,1,0.01):\n",
    "            SBA_train=SBA(DATA_Micro.Qty[n1:n2],alpha=i,beta=j)\n",
    "            if (np.sqrt(mean_squared_error(SBA_train['Forecast'].values[:-1],DATA_Micro['Qty'][n1:n2].values))<Opt_SBA[2]):\n",
    "                Opt_SBA[2]=np.sqrt(mean_squared_error(SBA_train['Forecast'].values[:-1],DATA_Micro['Qty'][n1:n2].values))\n",
    "                Opt_SBA[1]=j\n",
    "                Opt_SBA[0]=i\n",
    "    return(Opt_SBA[0],Opt_SBA[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "465d3ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segmentate_results(predictions,n_steps_out,change=True):\n",
    "    n = len(predictions)\n",
    "    for i in range (0,n+1,n_steps_out):\n",
    "        if (change) and (i<n):\n",
    "            predictions[i:i+n_steps_out]=[predictions[i]]*n_steps_out\n",
    "    return (predictions[0:i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac5991b",
   "metadata": {},
   "source": [
    "# Generating results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dea40bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_csv(product,n_obs,ADI,CV2,n_steps_in,n_steps_out,AUROC_HM,MASE_HM,MASE_NN,MASE_SES,MASE_Cr,MASE_SBA,MASE_RNN,\n",
    "                MASE_Naive,MASE_ZF,RMSSE_HM,RMSSE_NN,RMSSE_SES,RMSSE_Cr,RMSSE_SBA,RMSSE_RNN,RMSSE_Naive,RMSSE_ZF,file='Forecast_results.csv'):\n",
    "    header=['product','Samples','ADI','CV2','n_steps_in','n_steps_out','AUROC_HM','MASE_HM','MASE_NN','MASE_SES','MASE_Cr','MASE_SBA','MASE_RNN','MASE_Naive','MASE_ZF',\n",
    "    'RMSSE_HM','RMSSE_NN','RMSSE_SES','RMSSE_Cr','RMSSE_SBA','RMSSE_RNN','RMSSE_Naive','RMSSE_ZF']\n",
    "    dicti={\n",
    "        'product':product,\n",
    "        'Samples':n_obs,\n",
    "        'ADI':ADI,\n",
    "        'CV2':CV2,\n",
    "        'n_steps_in':n_steps_in,\n",
    "        'n_steps_out':n_steps_out,\n",
    "        'AUROC_HM':AUROC_HM,\n",
    "        'MASE_HM':MASE_HM,\n",
    "        'MASE_NN':MASE_NN,\n",
    "        'MASE_SES':MASE_SES,\n",
    "        'MASE_Cr':MASE_Cr,\n",
    "        'MASE_SBA':MASE_SBA,\n",
    "        'MASE_RNN':MASE_RNN,\n",
    "        'MASE_Naive':MASE_Naive,\n",
    "        'MASE_ZF': MASE_ZF,\n",
    "        'RMSSE_HM':RMSSE_HM,\n",
    "        'RMSSE_NN':RMSSE_NN,\n",
    "        'RMSSE_SES':RMSSE_SES,\n",
    "        'RMSSE_Cr':RMSSE_Cr,\n",
    "        'RMSSE_SBA':RMSSE_SBA,\n",
    "        'RMSSE_RNN':RMSSE_RNN,\n",
    "        'RMSSE_Naive':RMSSE_Naive,\n",
    "        'RMSSE_ZF':RMSSE_ZF\n",
    "        }\n",
    "    # saving results\n",
    "    try:\n",
    "        df=pd.read_csv('Forecast_results.csv')\n",
    "        new=False\n",
    "    except:\n",
    "        new=True\n",
    "    if new:\n",
    "        with open('Forecast_results.csv','w') as fd:\n",
    "            writer = csv.writer(fd)\n",
    "            writer.writerow(header)\n",
    "            writer = csv.DictWriter(fd, fieldnames=header)\n",
    "            writer.writerow(dicti)\n",
    "    else:\n",
    "        with open('Forecast_results.csv','a',newline='') as fd:\n",
    "            writer = csv.DictWriter(fd, fieldnames=header)\n",
    "            writer.writerow(dicti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "07efe112",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(product,n_steps_in=1,n_steps_out_list=[1],path=\"./gen data/\"):  \n",
    "    DATA=pd.read_csv(path+product+\".csv\", sep=';',encoding = \"ISO-8859-1\")\n",
    "\n",
    "    #preprocess\n",
    "    data = datasets(DATA,n_steps_in)\n",
    "    \n",
    "    #number of input lags, train and test samples\n",
    "    Nlags = data.get_Nlags()\n",
    "    n0 = data.get_n0()\n",
    "    n1 = data.get_n1()\n",
    "    \n",
    "    #loss function coef\n",
    "    w0 = data.get_w0()\n",
    "    w1 = data.get_w1()\n",
    "    \n",
    "    #metric\n",
    "    metric='PRC'\n",
    "    \n",
    "    #number of features\n",
    "    n_features=5\n",
    "    n_features2=3\n",
    "    n_features3=5\n",
    "    \n",
    "    #model type and param\n",
    "    RNN='LSTM'\n",
    "    batch_size=64\n",
    "    evaluate_performance=True\n",
    "    \n",
    "    #Deep2net\n",
    "    calib_data,valid_data,train_data,test_data=data.get_deep2net_datasets()\n",
    "    calib_dataset=dataset_load(calib_data)\n",
    "    valid_dataset=dataset_load(valid_data)\n",
    "    train_dataset=dataset_load(train_data)\n",
    "    test_dataset=dataset_load(test_data)\n",
    "    calib_loader = DataLoader(dataset=calib_dataset,batch_size=batch_size,shuffle=False)\n",
    "    valid_loader = DataLoader(dataset=valid_dataset,batch_size=batch_size,shuffle=False)\n",
    "    train_loader = DataLoader(dataset=train_dataset,batch_size=batch_size,shuffle=False)\n",
    "    best_hyperparameters=Calibration(RNN,w1,w0,calib_loader,valid_loader,n_features,metric)\n",
    "    best_model,best_threshold=Train_model(RNN,w1,w0,best_hyperparameters,train_loader,n_features,metric)\n",
    "    best_hyperparameters2=Calibration2(calib_loader,valid_loader,n_features2,best_model,best_threshold)\n",
    "    best_model2=Train_model2(best_hyperparameters2,train_loader,n_features2,best_model,best_threshold)\n",
    "    \n",
    "    #RegNetwork-RNN\n",
    "    calib_data1,valid_data1,train_data1,test_data1,scaler=data.get_reg_datasets()\n",
    "    calib_dataset1=dataset_load1(calib_data1)\n",
    "    valid_dataset1=dataset_load1(valid_data1)\n",
    "    train_dataset1=dataset_load1(train_data1)\n",
    "    test_dataset1=dataset_load1(test_data1)\n",
    "    calib_loader1 = DataLoader(dataset=calib_dataset1,batch_size=batch_size,shuffle=False)\n",
    "    valid_loader1 = DataLoader(dataset=valid_dataset1,batch_size=batch_size,shuffle=False)\n",
    "    train_loader1 = DataLoader(dataset=train_dataset1,batch_size=batch_size,shuffle=False)\n",
    "    test_loader1 = DataLoader(dataset=test_dataset1,batch_size=10000,shuffle=False)\n",
    "    best_hyperparameters_RNN=Calibration3(True,RNN,calib_loader1,valid_loader1,n_features3)\n",
    "    best_model_RNN=Train_model3(True,RNN,best_hyperparameters_RNN,train_loader1,n_features3)\n",
    "    pred_RNN, labels_RNN = test_predict3(best_model_RNN,test_loader1)\n",
    "    pred_RNN=scaler.inverse_transform([pred_RNN]).ravel()\n",
    "    labels_RNN=scaler.inverse_transform([labels_RNN]).ravel()\n",
    "\n",
    "    #RegNetwork-MLP\n",
    "    best_hyperparameters_MLP=Calibration3(False,RNN,calib_loader1,valid_loader1,n_features3)\n",
    "    best_model_MLP=Train_model3(False,RNN,best_hyperparameters_MLP,train_loader1,n_features3)\n",
    "    pred_MLP, labels_MLP = test_predict3(best_model_MLP,test_loader1)\n",
    "    pred_MLP=scaler.inverse_transform([pred_MLP]).ravel()\n",
    "    \n",
    "    #SES\n",
    "    SES_test=SES(DATA.Qty[:],evaluate_SES(DATA,n0,n1))\n",
    "    SES_pred_=SES_test['Forecast'][n1:]\n",
    "    \n",
    "    #Croston\n",
    "    Cr_test=Croston(DATA.Qty[0:],alpha=evaluate_CR(DATA,n0,n1))\n",
    "    Cr_pred_=Cr_test['Forecast'][n1:-1]\n",
    "    \n",
    "    #SBA\n",
    "    Opt_SBA=evaluate_SBA(DATA,n0,n1)\n",
    "    SBA_test=SBA(DATA.Qty[0:],alpha=Opt_SBA[0],beta=Opt_SBA[1])\n",
    "    SBA_pred_=SBA_test['Forecast'][n1:-1]\n",
    "    \n",
    "    #Naive\n",
    "    naiv_pred_=DATA['Qty'][n1-Nlags:-Nlags].values\n",
    "    \n",
    "    #Saving to CSV\n",
    "    n_obs = data.get_n_obs()\n",
    "    ADI = data.get_ADI()\n",
    "    CV2 = data.get_CV2()\n",
    "    Train_X = data.get_train_Y()\n",
    "    Test_Y = data.get_test_Y()\n",
    "    \n",
    "    test_loader = DataLoader(dataset=test_dataset,batch_size=batch_size,shuffle=False)\n",
    "    AUROC_HM = test_predict(best_model,test_loader)\n",
    "    \n",
    "    for n_steps_out in n_steps_out_list:\n",
    "        test_loader = DataLoader(dataset=test_dataset,batch_size=n_steps_out,shuffle=False)\n",
    "        labels_HM,predictions_HM = test_predict2(best_model,best_model2,test_loader,best_threshold,n_steps_out)\n",
    "        seg_Test_Y = segmentate_results(Test_Y,n_steps_out,change=False)\n",
    "        MASE_HM = MASE(Train_X,seg_Test_Y,predictions_HM).item()\n",
    "        RMSSE_HM = RMSSE(Train_X,seg_Test_Y,predictions_HM).item()\n",
    "    \n",
    "        if n_steps_out==1:\n",
    "            pred_MLP = segmentate_results(pred_MLP,n_steps_out,change=False)\n",
    "            MASE_NN = MASE(Train_X,seg_Test_Y,pred_MLP)\n",
    "            RMSSE_NN = RMSSE(Train_X,seg_Test_Y,pred_MLP)\n",
    "            pred_RNN = segmentate_results(pred_RNN,n_steps_out,change=False)\n",
    "            MASE_RNN = MASE(Train_X,seg_Test_Y,pred_RNN)\n",
    "            RMSSE_RNN = RMSSE(Train_X,seg_Test_Y,pred_RNN)\n",
    "        else:\n",
    "            MASE_NN,RMSSE_NN,MASE_RNN,RMSSE_RNN=0,0,0,0\n",
    "    \n",
    "    \n",
    "        SES_pred = segmentate_results(list(SES_pred_),n_steps_out)\n",
    "        MASE_SES = MASE(Train_X,seg_Test_Y,SES_pred)\n",
    "        RMSSE_SES = RMSSE(Train_X,seg_Test_Y,SES_pred)\n",
    "\n",
    "        Cr_pred = segmentate_results(list(Cr_pred_),n_steps_out)\n",
    "        MASE_Cr = MASE(Train_X,seg_Test_Y,Cr_pred)\n",
    "        RMSSE_Cr = RMSSE(Train_X,seg_Test_Y,Cr_pred)\n",
    "\n",
    "        SBA_pred = segmentate_results(list(SBA_pred_),n_steps_out)\n",
    "        MASE_SBA = MASE(Train_X,seg_Test_Y,SBA_pred)\n",
    "        RMSSE_SBA = RMSSE(Train_X,seg_Test_Y,SBA_pred)\n",
    "\n",
    "        if (Nlags<n_steps_out):\n",
    "            naiv_pred = segmentate_results(naiv_pred_,n_steps_out)\n",
    "            MASE_Naive = MASE(Train_X,seg_Test_Y,naiv_pred)\n",
    "            RMSSE_Naive = RMSSE(Train_X,seg_Test_Y,naiv_pred)\n",
    "        else:\n",
    "            naiv_pred = segmentate_results(naiv_pred_,n_steps_out,change=False)\n",
    "            MASE_Naive = MASE(Train_X,seg_Test_Y,naiv_pred)\n",
    "            RMSSE_Naive = RMSSE(Train_X,seg_Test_Y,naiv_pred)\n",
    "    \n",
    "        ZF_pred = [0]*len(seg_Test_Y)\n",
    "        MASE_ZF = MASE(Train_X,seg_Test_Y,ZF_pred)\n",
    "        RMSSE_ZF = RMSSE(Train_X,seg_Test_Y,ZF_pred)\n",
    "    \n",
    "        save_to_csv(product,n_obs,ADI,CV2,n_steps_in,n_steps_out,AUROC_HM,MASE_HM,MASE_NN,MASE_SES,MASE_Cr,MASE_SBA,MASE_RNN,\n",
    "                MASE_Naive,MASE_ZF,RMSSE_HM,RMSSE_NN,RMSSE_SES,RMSSE_Cr,RMSSE_SBA,RMSSE_RNN,RMSSE_Naive,RMSSE_ZF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a5616715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening 'CETREMIDE GELOSE.csv'\n",
      "Training on  cuda\n",
      "model 1 out of 1\n",
      "model trained for 100 epochs, loss per epoch is 0.8470074424037227.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14544/2469624416.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Opening '{0}.csv'\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproduct\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproduct\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_steps_in\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_steps_out\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Model trained on product \"%s\" in %i minutes and %i seconds'\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproduct\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14544/1028932878.py\u001b[0m in \u001b[0;36mmain\u001b[1;34m(product, n_steps_in, n_steps_out, path)\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[0mvalid_loader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalid_dataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[0mtrain_loader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m     \u001b[0mbest_hyperparameters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mCalibration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcalib_loader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalid_loader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_features\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmetric\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m     \u001b[0mbest_model\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbest_threshold\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbest_hyperparameters\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_features\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmetric\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[0mbest_hyperparameters2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mCalibration2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcalib_loader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalid_loader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_features2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbest_model\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbest_threshold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14544/1243008362.py\u001b[0m in \u001b[0;36mCalibration\u001b[1;34m(RNN, w1, w0, calib_loader, valid_loader, n_features, metric)\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdata1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcalib_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m                 \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtarget1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m                 \u001b[1;31m# Forward pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "global device\n",
    "global verbose \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "verbose = True\n",
    "\n",
    "#n_forecast_ahead\n",
    "n_steps_in = 30\n",
    "n_steps_out_list = [1,7]\n",
    "products_list = open('product_List.txt','r').read().strip().split('\\n')\n",
    "for product in products_list:\n",
    "    start=time.time()\n",
    "    if verbose:\n",
    "        print(\"Opening '{0}.csv'\".format(product))\n",
    "    main(product,n_steps_in,n_steps_out)\n",
    "    print('Model trained on product \"%s\" in %i minutes and %i seconds' %(str(product),int((time.time()-start)/60),float((time.time()-start)%60)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7d3260",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "78e58e174bbf6f139d99cb175ce740dfc54e8c53f1ce58a8848962c143d22910"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
