{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b09e58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import math\n",
    "import pandas as pd\n",
    "import random\n",
    "import gzip\n",
    "import torch\n",
    "from sklearn import metrics\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import warnings\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import make_scorer,balanced_accuracy_score,roc_curve,precision_recall_curve,mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from statsmodels.tsa.stattools import acf\n",
    "from numpy import array\n",
    "from numpy import hstack\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0448b015",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4d8c02",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e768f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split a multivariate sequence into samples\n",
    "def Convert_format1(data, n_steps_in, n_steps_out):\n",
    "    n0=np.where(data['PeriodsSepLastTwoNnZeroDemands']>0)[0][1]\n",
    "    n1=n0+int((data.shape[0]-n0)*90/100)\n",
    "    in_seq1=data['ZNZDemand'].values[n0:]\n",
    "    in_seq2=data['LastQtte'].values[n0:]\n",
    "    in_seq3=data['WeekDay'].values[n0:]\n",
    "    in_seq4=data['Interval'].values[n0:]\n",
    "    in_seq5=data['PeriodsSepLastTwoNnZeroDemands'].values[n0:]\n",
    "    in_seq6=data['Month'].values[n0:]\n",
    "    in_seq1 = in_seq1.reshape((len(in_seq1), 1))\n",
    "    in_seq2 = in_seq2.reshape((len(in_seq2), 1))\n",
    "    in_seq3 = in_seq3.reshape((len(in_seq3), 1))\n",
    "    in_seq4 = in_seq4.reshape((len(in_seq4), 1))\n",
    "    in_seq5 = in_seq5.reshape((len(in_seq5), 1))\n",
    "    in_seq6 = in_seq6.reshape((len(in_seq6), 1))\n",
    "    # horizontally stack columns\n",
    "    dataset = hstack((in_seq1, in_seq2,in_seq3,in_seq4,in_seq5,in_seq6))\n",
    "    X = []\n",
    "    for i in range(len(dataset)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps_in\n",
    "        out_end_ix = end_ix + n_steps_out\n",
    "        # check if we are beyond the dataset\n",
    "        if (out_end_ix-1) > len(dataset):\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = list(dataset[i:end_ix,1:]), list(dataset[end_ix-1:out_end_ix-1, 0])\n",
    "        X.append([seq_x,seq_y])\n",
    "    size=int((n1) / 5)\n",
    "    train_data=X[:n1]\n",
    "    test_data=X[n1:]\n",
    "    firsttrain = X[:4*size]\n",
    "    firstvalid = X[4*size:n1]\n",
    "    alldata=train_data+test_data\n",
    "    return firsttrain,firstvalid,train_data,test_data,alldata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1d68dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset_load(Dataset):\n",
    "    def __init__(self,xy=None):\n",
    "        self.x_data=np.asarray([el[0] for el in xy],dtype=np.float32)\n",
    "        self.y_data =np.asarray([el[1] for el in xy ],dtype=np.float32)\n",
    "        self.x_data = torch.from_numpy(self.x_data)\n",
    "        self.y_data = torch.from_numpy(self.y_data)\n",
    "        self.len=len(self.x_data)\n",
    "      \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec0de1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Data to the defined input in the 2nd phase\n",
    "def Convert_format2(data,RNN_out, n_steps_in, n_steps_out):\n",
    "    n0=np.where(data['PeriodsSepLastTwoNnZeroDemands']>0)[0][1]\n",
    "    n1=n0+int((data.shape[0]-n0)*90/100)\n",
    "    #scaler = MinMaxScaler(feature_range = (0, 1))\n",
    "    #scaler.fit(data[['Quantite']].values[n0:n1] )\n",
    "    #in_seq1=scaler.transform(data[['Quantite']].values[n0:])\n",
    "    in_seq1=data['Quantite'].values[n0:]\n",
    "    in_seq2=data['LastQtte'].values[n0:]\n",
    "    in_seq3=data['Interval'].values[n0:]\n",
    "    in_seq4=RNN_out\n",
    "    in_seq1 = in_seq1.reshape((len(in_seq1), 1))\n",
    "    in_seq2 = in_seq2.reshape((len(in_seq2), 1))\n",
    "    in_seq3 = in_seq3.reshape((len(in_seq3), 1))\n",
    "    in_seq4 = in_seq4.reshape((len(in_seq4), 1))\n",
    "    # horizontally stack columns\n",
    "    dataset = hstack((in_seq1, in_seq2,in_seq3,in_seq4))\n",
    "    X = list()\n",
    "    for i in range(len(dataset)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps_in\n",
    "        out_end_ix = end_ix + n_steps_out\n",
    "        # check if we are beyond the dataset\n",
    "        if (out_end_ix-1) > len(dataset):\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = dataset[i:end_ix,1:].ravel(), dataset[end_ix-1:out_end_ix-1, 0].ravel()\n",
    "        X.append([seq_x,seq_y])\n",
    "    size=int(n1 / 5)\n",
    "    train_data=X[:n1]\n",
    "    test_data=X[n1:]\n",
    "    firsttrain = X[:4*size]\n",
    "    firstvalid = X[4*size:n1]\n",
    "    return firsttrain,firstvalid,train_data,test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ded7771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Data to the defined input in the RegNetwork\n",
    "def Convert_format3(data, n_steps_in, n_steps_out):\n",
    "    n0=np.where(data['PeriodsSepLastTwoNnZeroDemands']>0)[0][1]\n",
    "    n1=n0+int((data.shape[0]-n0)*90/100)\n",
    "    scaler = MinMaxScaler(feature_range = (0, 1))\n",
    "    scaler.fit(data[['Quantite']].values[n0:n1] )\n",
    "    in_seq1=scaler.transform(data[['Quantite']].values[n0:])\n",
    "    #in_seq1=data['Quantite'].values[n0:]\n",
    "    in_seq2=data['LastQtte'].values[n0:]\n",
    "    in_seq3=data['WeekDay'].values[n0:]\n",
    "    in_seq4=data['Interval'].values[n0:]\n",
    "    in_seq5=data['PeriodsSepLastTwoNnZeroDemands'].values[n0:]\n",
    "    in_seq6=data['Month'].values[n0:]\n",
    "    in_seq1 = in_seq1.reshape((len(in_seq1), 1))\n",
    "    in_seq2 = in_seq2.reshape((len(in_seq2), 1))\n",
    "    in_seq3 = in_seq3.reshape((len(in_seq3), 1))\n",
    "    in_seq4 = in_seq4.reshape((len(in_seq4), 1))\n",
    "    in_seq5 = in_seq5.reshape((len(in_seq5), 1))\n",
    "    in_seq6 = in_seq6.reshape((len(in_seq6), 1))\n",
    "    # horizontally stack columns\n",
    "    dataset = hstack((in_seq1, in_seq2,in_seq3,in_seq4,in_seq5,in_seq6))\n",
    "    X = []\n",
    "    for i in range(len(dataset)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps_in\n",
    "        out_end_ix = end_ix + n_steps_out\n",
    "        # check if we are beyond the dataset\n",
    "        if (out_end_ix-1) > len(dataset):\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = dataset[i:end_ix,1:], dataset[end_ix-1:out_end_ix-1, 0]\n",
    "        X.append([seq_x,seq_y])\n",
    "    size=int(n1 / 5)\n",
    "    train_data=X[:n1]\n",
    "    test_data=X[n1:]\n",
    "    firsttrain = X[:4*size]\n",
    "    firstvalid = X[4*size:n1]\n",
    "    return firsttrain,firstvalid,train_data,test_data,scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063adbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logsampler(a,b):\n",
    "        x=np.random.uniform(low=0,high=1)\n",
    "        y=10**((math.log10(b)-math.log10(a))*x + math.log10(a))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d1dee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pb_func(row):\n",
    "    best=row.idxmin(axis=0, skipna=True)\n",
    "    return (best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf912587",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MASE(X,Y,F):\n",
    "    N1=len(X)\n",
    "    N2=len(Y)\n",
    "    D1=np.sum(abs(Y-F))/N2\n",
    "    D2=np.sum(abs(X[1:]-X[:-1]))/N1\n",
    "    return (D1/D2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed31e102",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSSE(X,Y,F):\n",
    "    h=len(Y)\n",
    "    n=len(X)\n",
    "    result=np.sqrt(((1/h)*np.sum((Y-F)**2))/(1/(n-1)*np.sum((X[1:]-X[:-1])**2)))\n",
    "    return (result)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b19bcf",
   "metadata": {},
   "source": [
    "# Network 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3554d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Deepnet(nn.Module):\n",
    "    def __init__ (self,RNN,RNN_hidden_size,RNN_sigma,layer_size,N_sigma,dropprob,n_features):\n",
    "        super(Deepnet,self).__init__()\n",
    "        self.RNN=RNN\n",
    "        self.RNN_hidden_size=RNN_hidden_size\n",
    "        self.RNN_sigma=RNN_sigma\n",
    "        \n",
    "        self.layer_size=layer_size\n",
    "        self.N_sigma=N_sigma\n",
    "        \n",
    "        self.input_channels=n_features\n",
    "        self.dropprob=dropprob\n",
    "        \n",
    "        if self.RNN=='LSTM':\n",
    "            self.rnn = nn.LSTM(self.input_channels, RNN_hidden_size, num_layers=1, bidirectional=False).to(device)\n",
    "            self.FC_size= RNN_hidden_size\n",
    "        elif self.RNN=='BiLSTM':\n",
    "            self.rnn = nn.LSTM(self.input_channels, RNN_hidden_size, num_layers=1, bidirectional=True).to(device)\n",
    "            self.FC_size= 2*RNN_hidden_size\n",
    "        elif self.RNN=='GRU':\n",
    "            self.rnn = nn.GRU(self.input_channels, RNN_hidden_size, num_layers=1, bidirectional=False).to(device)\n",
    "            self.FC_size= RNN_hidden_size\n",
    "        elif self.RNN=='BiGRU':\n",
    "            self.rnn = nn.GRU(self.input_channels, RNN_hidden_size, num_layers=1, bidirectional=True).to(device)\n",
    "            self.FC_size= 2*RNN_hidden_size\n",
    "        for layer_p in self.rnn._all_weights:\n",
    "            for p in layer_p:\n",
    "                if 'weight' in p:\n",
    "                    torch.nn.init.normal_(self.rnn.__getattr__(p),mean=0,std=RNN_sigma)\n",
    "        \n",
    "        #weights between LSTM or CNN or GRU layers and fully connected layer\n",
    "        self.wHidden = torch.randn(self.FC_size, self.layer_size).to(device)\n",
    "        self.wHiddenBias = torch.randn(self.layer_size).to(device)\n",
    "        self.wHidden.requires_grad = True\n",
    "        self.wHiddenBias.requires_grad = True\n",
    "        \n",
    "        #weights between the fully connected layer and the output node\n",
    "        self.wNeu=torch.randn(self.layer_size,1).to(device)\n",
    "        self.wNeuBias=torch.randn(1).to(device) \n",
    "        torch.nn.init.xavier_uniform(self.wNeu)\n",
    "        torch.nn.init.xavier_uniform(self.wHidden)\n",
    "        self.wNeu.requires_grad = True\n",
    "        self.wNeuBias.requires_grad = True\n",
    "        \n",
    "        torch.nn.init.normal_(self.wNeu,mean=0,std=self.N_sigma)\n",
    "        torch.nn.init.normal_(self.wNeuBias,mean=0,std=self.N_sigma)\n",
    "        torch.nn.init.normal_(self.wHidden,mean=0,std=self.N_sigma)\n",
    "        torch.nn.init.normal_(self.wHiddenBias,mean=0,std=self.N_sigma)\n",
    "        \n",
    "        self.dropout = torch.nn.Dropout(p=dropprob, inplace=False) #Dropout Layer (Dropout rate= p)\n",
    "        self.max = torch.nn.MaxPool1d(3, stride=1) #Pooling layer (pooling size =3)\n",
    "    def get_weights(self):\n",
    "        ll = []\n",
    "        for layer_p in self.rnn._all_weights:\n",
    "            for p in layer_p:\n",
    "                if 'weight' in p:\n",
    "                    ll.append(self.rnn.__getattr__(p))\n",
    "        return ll\n",
    "    def forward(self,x):\n",
    "        x=x.permute(1,0,2)\n",
    "        output,_=self.rnn(x)\n",
    "        if self.RNN=='BiLSTM' or self.RNN=='BiGRU':\n",
    "            Normal_RNN=output[-1, :, :self.RNN_hidden_size]\n",
    "            Rev_RNN=output[0, :, self.RNN_hidden_size:]\n",
    "            x = torch.cat((Normal_RNN, Rev_RNN), 1)\n",
    "            x=self.dropout(x)\n",
    "        else:\n",
    "            x = output[-1, :, :]\n",
    "            x=self.dropout(x)\n",
    "        x=x @ self.wHidden + self.wHiddenBias\n",
    "        x=x.clamp(min=0)\n",
    "        x=self.dropout(x)\n",
    "        x=x @ self.wNeu + self.wNeuBias\n",
    "        return (torch.sigmoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f699e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Calibration(RNN,w1,w0,firsttrain_loader,firstvalid_loader,n_features,metric):\n",
    "    print('start')\n",
    "    best_AUC = 0\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    print(device)\n",
    "    learning_steps_list = [50, 100, 150, 200, 250, 300, 350, 400]\n",
    "    for number in range(40):\n",
    "        # hyper-parameters\n",
    "        RNN_hidden_size_list = [20, 50, 80, 100]\n",
    "        RNN_hidden_size = random.choice(RNN_hidden_size_list)\n",
    "        dropoutList = [0, 0.15, 0.3, 0.45]\n",
    "        dropprob = random.choice(dropoutList)\n",
    "        layer_size_list = [32, 64]\n",
    "        layer_size = random.choice(layer_size_list)\n",
    "        learning_rate_list = [10**-5,10**-4,10**-3,10**-2]\n",
    "        learning_rate=random.choice(learning_rate_list)\n",
    "        RNN_sigma = logsampler(10 ** -4, 10 ** -2)\n",
    "        N_sigma = logsampler(10 ** -4, 10 ** -2)\n",
    "        model_auc = []\n",
    "        model = Deepnet(RNN,RNN_hidden_size,RNN_sigma,layer_size,N_sigma,dropprob,n_features).to(device)\n",
    "        optimizer = torch.optim.Adam(\n",
    "                model.get_weights() + [model.wNeu, model.wNeuBias, model.wHidden, model.wHiddenBias],\n",
    "                lr=learning_rate)\n",
    "\n",
    "        learning_steps = 0\n",
    "        while learning_steps <= 400:\n",
    "            auc = []\n",
    "            model.train()\n",
    "            for i, (data, target) in enumerate(firsttrain_loader):\n",
    "\n",
    "                data = data.to(device)\n",
    "                target = target.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                output = model(data)\n",
    "                loss = F.binary_cross_entropy(output, target,weight=((torch.abs((target)) * w1) - (torch.subtract(target,1) * w0)))\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                if (learning_steps % 50 == 0):\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        model.eval()\n",
    "                        auc = []\n",
    "                        for j, (data1, target1) in enumerate(firstvalid_loader):\n",
    "                            data1 = data1.to(device)\n",
    "                            target1 = target1.to(device)\n",
    "\n",
    "                            # Forward pass\n",
    "                            output = model(data1)\n",
    "\n",
    "                            pred = output.cpu().detach().numpy().reshape(output.shape[0])\n",
    "                            labels = target1.cpu().numpy().reshape(output.shape[0])\n",
    "                            if output.shape[0] > 60:\n",
    "                                if (metric=='ROC'):\n",
    "                                    auc.append(metrics.roc_auc_score(labels, pred))\n",
    "                                elif (metric=='PRC'):\n",
    "                                    precision, recall, _ = precision_recall_curve(labels, pred)\n",
    "                                    auc.append(metrics.auc(recall, precision))\n",
    "                                else :\n",
    "                                    print('Choose proper metric')\n",
    "                                    break;\n",
    "                        model_auc.append(np.mean(auc))\n",
    "\n",
    "                        model.train()\n",
    "            learning_steps += 1\n",
    "\n",
    "        for n in range(8):\n",
    "            AUC = model_auc[n]\n",
    "            # print(AUC)\n",
    "            if AUC > best_AUC:\n",
    "                best_AUC = AUC\n",
    "                best_learning_steps = learning_steps_list[n]\n",
    "                best_LearningRate = learning_rate\n",
    "                best_RNN_hidden_size=RNN_hidden_size\n",
    "                best_dropprob = dropprob\n",
    "                best_layer_size= layer_size\n",
    "                best_RNN_sigma = RNN_sigma\n",
    "                best_N_sigma=N_sigma\n",
    "\n",
    "    print('best_AUC=', best_AUC)\n",
    "    print('best_learning_steps=', best_learning_steps)\n",
    "    print('best_LearningRate=', best_LearningRate)\n",
    "    print('best_dropprob=', best_dropprob)\n",
    "    print('best_RNN_hidden_size=', best_RNN_hidden_size)\n",
    "    print('best_layer_size=', best_layer_size)\n",
    "    print('best_RNN_sigma=', best_RNN_sigma)\n",
    "    print('best_N_sigma=', best_N_sigma)\n",
    "\n",
    "    best_hyperparameters = {'best_learning_steps': best_learning_steps, \n",
    "                            'best_LearningRate': best_LearningRate,\n",
    "                            'best_dropprob': best_dropprob, \n",
    "                            'best_RNN_hidden_size': best_RNN_hidden_size,\n",
    "                            'best_layer_size': best_layer_size, \n",
    "                            'best_RNN_sigma': best_RNN_sigma,\n",
    "                            'best_N_sigma':best_N_sigma}\n",
    "    return best_hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabdf3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_model(RNN,w1,w0,best_hyperparameters,train_loader,n_features,metric):\n",
    "    best_learning_steps=best_hyperparameters['best_learning_steps']\n",
    "    best_LearningRate=best_hyperparameters['best_LearningRate']\n",
    "    best_RNN_hidden_size=best_hyperparameters['best_RNN_hidden_size']\n",
    "    best_dropprob=best_hyperparameters['best_dropprob']\n",
    "    best_N_sigma=best_hyperparameters['best_N_sigma']\n",
    "    best_RNN_sigma=best_hyperparameters['best_RNN_sigma']\n",
    "    best_layer_size=best_hyperparameters['best_layer_size']\n",
    "    best_AUC=0\n",
    "    best_threshold=0.5\n",
    "    for number_models in range(5):\n",
    "        model = Deepnet(RNN,best_RNN_hidden_size,best_RNN_sigma,best_layer_size,best_N_sigma,best_dropprob,n_features).to(device)\n",
    "        optimizer = torch.optim.Adam(\n",
    "                model.get_weights() + [model.wNeu, model.wNeuBias, model.wHidden, model.wHiddenBias],\n",
    "                lr=best_LearningRate)\n",
    "\n",
    "        learning_steps=0\n",
    "        model.train()\n",
    "        while learning_steps<=best_learning_steps:\n",
    "            \n",
    "            for i, (data, target) in enumerate(train_loader):\n",
    "                data = data.to(device)\n",
    "                target = target.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                output = model(data)\n",
    "                loss = F.binary_cross_entropy(output, target,weight=((torch.abs((target)) * w1) - (torch.subtract(target,1) * w0)))\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            learning_steps+=1\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            auc=[]\n",
    "            threshold=[]\n",
    "            for i, (data, target) in enumerate(train_loader):\n",
    "                data = data.to(device)\n",
    "                target = target.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                output = model(data)\n",
    "\n",
    "                pred=output.cpu().detach().numpy().reshape(output.shape[0])\n",
    "                labels=target.cpu().numpy().reshape(output.shape[0])\n",
    "                if output.shape[0]>60:\n",
    "                    if (metric=='ROC'):\n",
    "                        auc.append(metrics.roc_auc_score(labels, pred))\n",
    "                        fpr, tpr, thresholds = roc_curve(labels, pred)\n",
    "                        gmeans=np.sqrt(tpr*(1-fpr))\n",
    "                    elif (metric=='PRC'):\n",
    "                        precision, recall, thresholds = precision_recall_curve(labels, pred)\n",
    "                        auc.append(metrics.auc(recall, precision))\n",
    "                        gmeans = (2*precision*recall)/(precision+recall)\n",
    "                    else :\n",
    "                        print('Choose proper metric')\n",
    "                        break;\n",
    "                    ix = np.argmax(gmeans)\n",
    "                    threshold.append(thresholds[ix])\n",
    "            #             \n",
    "            AUC_training=np.mean(auc)\n",
    "            print('AUC on training data for model ',number_models+1,' = ',AUC_training)\n",
    "            if AUC_training>best_AUC:\n",
    "                best_AUC=AUC_training\n",
    "                best_threshold=np.mean(threshold)\n",
    "                best_model=model\n",
    "    return best_model,best_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb237be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_predict(best_model,test_loader):\n",
    "\n",
    "    with torch.no_grad():\n",
    "        best_model.eval()\n",
    "        auc = []\n",
    "\n",
    "        for i, (data, target) in enumerate(test_loader):\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            output = best_model(data)\n",
    "            pred = output.cpu().detach().numpy().reshape(output.shape[0])\n",
    "            myprob = \"\\n\".join(map(str, pred[:]))\n",
    "            labels = target.cpu().numpy().reshape(output.shape[0])\n",
    "            if output.shape[0] > 50:\n",
    "                auc.append(metrics.roc_auc_score(labels, pred))\n",
    "\n",
    "        AUC_test = np.mean(auc)\n",
    "    return (labels,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4e43c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_predict(best_model,alldata_loader):\n",
    "\n",
    "    with torch.no_grad():\n",
    "        best_model.eval()\n",
    "        auc = []\n",
    "\n",
    "        for i, (data, target) in enumerate(alldata_loader):\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            output = best_model(data)\n",
    "            pred = output.cpu().detach().numpy().reshape(output.shape[0])\n",
    "            myprob = \"\\n\".join(map(str, pred[:]))\n",
    "            labels = target.cpu().numpy().reshape(output.shape[0])\n",
    "    return (labels,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8963d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_confusion_matrix(cf,labels,pred,group_names=None,categories='auto',count=True,percent=True,cbar=True,xyticks=True,xyplotlabels=True,sum_stats=True,figsize=None,cmap='Blues',title=None):\n",
    "    blanks = ['' for i in range(cf.size)]\n",
    "\n",
    "    if group_names and len(group_names)==cf.size:\n",
    "        group_labels = [\"{}\\n\".format(value) for value in group_names]\n",
    "    else:\n",
    "        group_labels = blanks\n",
    "\n",
    "    if count:\n",
    "        group_counts = [\"{0:0.0f}\\n\".format(value) for value in cf.flatten()]\n",
    "    else:\n",
    "        group_counts = blanks\n",
    "\n",
    "    if percent:\n",
    "        group_percentages = [\"{0:.2%}\".format(value) for value in cf.flatten()/np.sum(cf)]\n",
    "    else:\n",
    "        group_percentages = blanks\n",
    "\n",
    "    box_labels = [f\"{v1}{v2}{v3}\".strip() for v1, v2, v3 in zip(group_labels,group_counts,group_percentages)]\n",
    "    box_labels = np.asarray(box_labels).reshape(cf.shape[0],cf.shape[1])\n",
    "\n",
    "\n",
    "    # CODE TO GENERATE SUMMARY STATISTICS & TEXT FOR SUMMARY STATS\n",
    "    if sum_stats:\n",
    "        #if it is a binary confusion matrix, show some more stats\n",
    "        if len(cf)==2:\n",
    "            #Metrics for Binary Confusion Matrices\n",
    "            precision, recall, thresholds = precision_recall_curve(labels, pred)\n",
    "            Auc_PRC=metrics.auc(recall, precision)\n",
    "            Auc_ROC=metrics.roc_auc_score(labels, pred)\n",
    "            stats_text = \"\\n\\nAU_PRC={:0.3f}\\nAU_ROC={:0.3f}\".format(Auc_PRC,Auc_ROC)\n",
    "        else:\n",
    "            stats_text = \"\\n\\nAccuracy={:0.3f}\".format(accuracy)\n",
    "    else:\n",
    "        stats_text = \"\"\n",
    "\n",
    "\n",
    "    # SET FIGURE PARAMETERS ACCORDING TO OTHER ARGUMENTS\n",
    "    if figsize==None:\n",
    "        #Get default figure size if not set\n",
    "        figsize = plt.rcParams.get('figure.figsize')\n",
    "\n",
    "    if xyticks==False:\n",
    "        #Do not show categories if xyticks is False\n",
    "        categories=False\n",
    "\n",
    "\n",
    "    # MAKE THE HEATMAP VISUALIZATION\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.heatmap(cf,annot=box_labels,fmt=\"\",cmap=cmap,cbar=cbar,xticklabels=categories,yticklabels=categories)\n",
    "\n",
    "    if xyplotlabels:\n",
    "        plt.ylabel('True label')\n",
    "        plt.xlabel('Predicted label' + stats_text)\n",
    "    else:\n",
    "        plt.xlabel(stats_text)\n",
    "    \n",
    "    if title:\n",
    "        plt.title(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e75857",
   "metadata": {},
   "source": [
    "# Network 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecef793",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Deepnet2(nn.Module):\n",
    "    def __init__ (self,layer_number,layer_size,dropprob,n_features):\n",
    "        super(Deepnet2,self).__init__()\n",
    "                \n",
    "        self.layer_size=layer_size\n",
    "        self.layer_number=layer_number\n",
    "        self.input_channels=n_features\n",
    "        self.dropprob=dropprob\n",
    "        \n",
    "\n",
    "        \n",
    "        self.FC=self.layer_size\n",
    "        self.fc1=nn.Linear(self.input_channels,self.layer_size)\n",
    "        if (self.layer_number>1):\n",
    "            self.fc2=nn.Linear(self.layer_size,self.layer_size)\n",
    "        if (self.layer_number>2):\n",
    "            self.fc3=nn.Linear(self.layer_size,self.layer_size)\n",
    "        self.fc_f=nn.Linear(self.layer_size,1)\n",
    "        \n",
    "        self.dropout = torch.nn.Dropout(p=dropprob, inplace=False) #Dropout Layer (Dropout rate= p)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x=F.relu(self.fc1(x))\n",
    "        x=self.dropout(x)\n",
    "        if (self.layer_number>1):\n",
    "            x=F.relu(self.fc2(x))\n",
    "            x=self.dropout(x)\n",
    "        \n",
    "        if (self.layer_number>2):\n",
    "            x=F.relu(self.fc3(x))\n",
    "            x=self.dropout(x)\n",
    "        \n",
    "        x=F.relu(self.fc_f(x))\n",
    "\n",
    "        return (x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d8aa3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Calibration2(firsttrain_loader2,firstvalid_loader2,n_features2):\n",
    "    print('start')\n",
    "    best_MSE = 100\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    print(device)\n",
    "    learning_steps_list = [50, 100, 150, 200, 250, 300, 350, 400]\n",
    "    for number in range(40):\n",
    "        # hyper-parameters\n",
    "        dropoutList = [0, 0.15, 0.3, 0.45]\n",
    "        dropprob = random.choice(dropoutList)\n",
    "        layer_number_list=[1,2,3]\n",
    "        layer_number=random.choice(layer_number_list)\n",
    "        layer_size_list = [16, 32]\n",
    "        layer_size = random.choice(layer_size_list)\n",
    "        learning_rate_list = [10**-5,10**-4,10**-3,10**-2]\n",
    "        learning_rate=random.choice(learning_rate_list)\n",
    "        model_MSE = []\n",
    "        model = Deepnet2(layer_number,layer_size,dropprob,n_features2).to(device)\n",
    "        optimizer = torch.optim.Adam(\n",
    "                model.parameters(),\n",
    "                lr=learning_rate)\n",
    "\n",
    "        learning_steps = 0\n",
    "        while learning_steps <= 400:\n",
    "            model.train()\n",
    "            for i, (data, target) in enumerate(firsttrain_loader2):\n",
    "\n",
    "                data = data.to(device)\n",
    "                target = target.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                output = model(data)\n",
    "                loss = nn.MSELoss()\n",
    "                L=loss(output,target)\n",
    "                optimizer.zero_grad()\n",
    "                L.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                if learning_steps % 50 == 0:\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        model.eval()\n",
    "                        mse = []\n",
    "                        for j, (data1, target1) in enumerate(firstvalid_loader2):\n",
    "                            data1 = data1.to(device)\n",
    "                            target1 = target1.to(device)\n",
    "\n",
    "                            # Forward pass\n",
    "                            output = model(data1)\n",
    "\n",
    "                            pred = output.cpu().detach().numpy().reshape(output.shape[0])\n",
    "                            labels = target1.cpu().numpy().reshape(output.shape[0])\n",
    "                            if output.shape[0] > 60:\n",
    "                                mse.append(metrics.mean_squared_error(labels, pred))\n",
    "                        # print(np.mean(auc))\n",
    "                        model_MSE.append(np.mean(mse))\n",
    "\n",
    "                        model.train()\n",
    "            learning_steps += 1\n",
    "\n",
    "        for n in range(8):\n",
    "            MSE = model_MSE[n]\n",
    "            # print(AUC)\n",
    "            if MSE < best_MSE:\n",
    "                best_MSE = MSE\n",
    "                best_learning_steps = learning_steps_list[n]\n",
    "                best_LearningRate = learning_rate\n",
    "                best_dropprob = dropprob\n",
    "                best_layer_number= layer_number\n",
    "                best_layer_size= layer_size\n",
    "\n",
    "    print('best_MSE=', best_MSE)\n",
    "    print('best_learning_steps=', best_learning_steps)\n",
    "    print('best_LearningRate=', best_LearningRate)\n",
    "    print('best_dropprob=', best_dropprob)\n",
    "    print('best_layer_number=', best_layer_number)\n",
    "    print('best_layer_size=', best_layer_size)\n",
    "\n",
    "    best_hyperparameters = {'best_learning_steps': best_learning_steps, \n",
    "                            'best_LearningRate': best_LearningRate,\n",
    "                            'best_dropprob': best_dropprob, \n",
    "                            'best_layer_number': best_layer_number,\n",
    "                            'best_layer_size': best_layer_size}\n",
    "    return best_hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2b7ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_model2(best_hyperparameters,train_loader2,n_features2):\n",
    "    best_learning_steps=best_hyperparameters['best_learning_steps']\n",
    "    best_LearningRate=best_hyperparameters['best_LearningRate']\n",
    "    best_dropprob=best_hyperparameters['best_dropprob']\n",
    "    best_layer_number=best_hyperparameters['best_layer_number']\n",
    "    best_layer_size=best_hyperparameters['best_layer_size']\n",
    "    best_MSE=100\n",
    "\n",
    "    for number_models in range(5):\n",
    "        model = Deepnet2(best_layer_number,best_layer_size,best_dropprob,n_features2).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(),lr=best_LearningRate)\n",
    "\n",
    "        learning_steps=0\n",
    "        model.train()\n",
    "        while learning_steps<=best_learning_steps:\n",
    "            \n",
    "            for i, (data, target) in enumerate(train_loader2):\n",
    "                data = data.to(device)\n",
    "                target = target.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                output = model(data)\n",
    "                loss = nn.MSELoss()\n",
    "                L=loss(output,target)\n",
    "                optimizer.zero_grad()\n",
    "                L.backward()\n",
    "                optimizer.step()\n",
    "            learning_steps+=1\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            mse=[]\n",
    "            for i, (data, target) in enumerate(train_loader2):\n",
    "                data = data.to(device)\n",
    "                target = target.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                output = model(data)\n",
    "\n",
    "                pred=output.cpu().detach().numpy().reshape(output.shape[0])\n",
    "                labels=target.cpu().numpy().reshape(output.shape[0])\n",
    "                if output.shape[0]>60:\n",
    "                    mse.append(metrics.mean_squared_error(labels, pred))\n",
    "                    \n",
    "            MSE_Training=np.mean(mse)     \n",
    "\n",
    "            print('MSE on training data for model ',number_models+1,' = ',MSE_Training)\n",
    "            if MSE_Training<best_MSE:\n",
    "                best_MSE=MSE_Training\n",
    "                best_model=model\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d83b5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_predict2(best_model,test_loader2):\n",
    "\n",
    "    with torch.no_grad():\n",
    "        best_model.eval()\n",
    "\n",
    "        for i, (data, target) in enumerate(test_loader2):\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            output = best_model(data)\n",
    "            pred = output.cpu().detach().numpy().reshape(output.shape[0])\n",
    "            myprob = \"\\n\".join(map(str, pred[:]))\n",
    "            labels = target.cpu().numpy().reshape(output.shape[0])\n",
    "    return (pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4409d3",
   "metadata": {},
   "source": [
    "# Regression Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f16749",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegNetwork(nn.Module):\n",
    "    def __init__(self,RNN , RNN_type , RNN_size , RNN_mean , RNN_sigma , NN_number , NN_size , NN_mean , NN_sigma ,\n",
    "                 dropprob , n_features):\n",
    "        super(RegNetwork,self).__init__()\n",
    "        self.RNN=RNN\n",
    "        self.RNN_type=RNN_type\n",
    "        self.RNN_size=RNN_size\n",
    "        self.RNN_mean=RNN_mean        \n",
    "        self.RNN_sigma=RNN_sigma\n",
    "        self.NN_number=NN_number\n",
    "        self.NN_mean=NN_mean\n",
    "        self.NN_sigma=NN_sigma\n",
    "        self.dropprob=dropprob\n",
    "        self.input_channels=n_features\n",
    "        self.FC_size=NN_size\n",
    "        if self.RNN:\n",
    "            if RNN_type=='LSTM':\n",
    "                self.rnn = nn.LSTM(self.input_channels, RNN_size, num_layers=1, bidirectional=False).to(device)\n",
    "                self.input_channels= RNN_size\n",
    "            elif RNN_type=='BiLSTM':\n",
    "                self.rnn = nn.LSTM(self.input_channels, RNN_size, num_layers=1, bidirectional=True).to(device)\n",
    "                self.input_channels= 2*RNN_size\n",
    "            elif RNN_type=='GRU':\n",
    "                self.rnn = nn.GRU(self.input_channels, RNN_size, num_layers=1, bidirectional=False).to(device)\n",
    "                self.input_channels= RNN_size\n",
    "            elif RNN_type=='BiGRU':\n",
    "                self.rnn = nn.GRU(self.input_channels, RNN_size, num_layers=1, bidirectional=True).to(device)\n",
    "                self.input_channels= 2*RNN_size\n",
    "\n",
    "            for layer_p in self.rnn._all_weights:\n",
    "                for p in layer_p:\n",
    "                    if 'weight' in p:\n",
    "                        torch.nn.init.normal_(self.rnn.__getattr__(p),mean=self.RNN_mean,std=self.RNN_sigma)\n",
    "        \n",
    "        self.fc1=nn.Linear(self.input_channels,self.FC_size)\n",
    "        torch.nn.init.normal_(self.fc1.weight,mean=self.NN_mean,std=self.NN_sigma)\n",
    "        torch.nn.init.normal_(self.fc1.bias,mean=self.NN_mean,std=self.NN_sigma)\n",
    "        if (self.NN_number>1):\n",
    "            self.fc2=nn.Linear(self.FC_size,self.FC_size)\n",
    "            torch.nn.init.normal_(self.fc2.weight,mean=self.NN_mean,std=self.NN_sigma)\n",
    "            torch.nn.init.normal_(self.fc2.bias,mean=self.NN_mean,std=self.NN_sigma)\n",
    "        if (self.NN_number>2):\n",
    "            self.fc3=nn.Linear(self.FC_size,self.FC_size)\n",
    "            torch.nn.init.normal_(self.fc3.weight,mean=self.NN_mean,std=self.NN_sigma)\n",
    "            torch.nn.init.normal_(self.fc3.bias,mean=self.NN_mean,std=self.NN_sigma)\n",
    "        self.fc_f=nn.Linear(self.FC_size,1)\n",
    "        torch.nn.init.normal_(self.fc_f.weight,mean=self.NN_mean,std=self.NN_sigma)\n",
    "        torch.nn.init.normal_(self.fc_f.bias,mean=self.NN_mean,std=self.NN_sigma)\n",
    "        self.dropout = torch.nn.Dropout(p=dropprob, inplace=False) #Dropout Layer (Dropout rate= p)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        if self.RNN:\n",
    "            x=x.permute(1,0,2)\n",
    "            output, _ = self.rnn(x)\n",
    "            if self.RNN_type=='BiLSTM' or self.RNN_type=='BiGRU':\n",
    "                Normal_RNN=output[-1, :, :self.RNN_size]\n",
    "                Rev_RNN=output[0, :, self.RNN_size:]\n",
    "                x = torch.cat((Normal_RNN, Rev_RNN), 1)\n",
    "                x=x.clamp(min=0)\n",
    "                x=self.dropout(x)\n",
    "            else:\n",
    "                x = output[-1, :, :]\n",
    "                x=self.dropout(x)\n",
    "                \n",
    "        \n",
    "        x=F.relu(self.fc1(x))\n",
    "        x=self.dropout(x)\n",
    "        if (self.NN_number>1):\n",
    "            x=F.relu(self.fc2(x))\n",
    "            x=self.dropout(x)\n",
    "        \n",
    "        if (self.NN_number>2):\n",
    "            x=F.relu(self.fc3(x))\n",
    "            x=self.dropout(x)\n",
    "        \n",
    "        x=F.sigmoid(self.fc_f(x))\n",
    "        #x=F.relu(self.fc_f(x))\n",
    "        return (x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e263212",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Calibration3(RNN,RNN_type,firsttrain_loader3,firstvalid_loader3,n_features3):\n",
    "    print('start')\n",
    "    best_MSE = 100\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    print(device)\n",
    "    learning_steps_list = [50, 100, 150, 200, 250, 300, 350, 400]\n",
    "    for number in range(40):\n",
    "        # hyper-parameters\n",
    "        RNN_hidden_size_list = [20, 50, 80, 100]\n",
    "        RNN_hidden_size = random.choice(RNN_hidden_size_list)\n",
    "        dropoutList = [0, 0.15, 0.3, 0.45]\n",
    "        dropprob = random.choice(dropoutList)\n",
    "        RNN_mean_v = logsampler(10 ** -1, 2)\n",
    "        RNN_mean=random.choice([-RNN_mean_v,RNN_mean_v])\n",
    "        RNN_sigma = logsampler(10 ** -4, 1)\n",
    "        weight_decay_list=[10**-5,10**-4,10**-3,10**-2]\n",
    "        Weight_decay=random.choice(weight_decay_list)\n",
    "        if RNN:\n",
    "            layer_number=1\n",
    "        else: \n",
    "            layer_number_list=[2,3]\n",
    "            layer_number=random.choice(layer_number_list)\n",
    "        layer_size_list = [32, 64]\n",
    "        layer_size = random.choice(layer_size_list)\n",
    "        learning_rate_list = [10**-6,10**-5,10**-4,10**-3]\n",
    "        learning_rate=random.choice(learning_rate_list)\n",
    "        NN_mean_v = logsampler(10**-1, 2)\n",
    "        NN_mean=random.choice([-NN_mean_v,NN_mean_v])\n",
    "        N_sigma = logsampler(10 ** -4, 1)\n",
    "        model_MSE = []\n",
    "        model = RegNetwork(RNN,RNN_type,RNN_hidden_size,RNN_mean,RNN_sigma,layer_number,layer_size,NN_mean,N_sigma,\n",
    "                           dropprob,n_features3).to(device)\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate,weight_decay=Weight_decay)\n",
    "\n",
    "        learning_steps = 0\n",
    "        while learning_steps <= 400:\n",
    "            mse = []\n",
    "            model.train()\n",
    "            for i, (data, target) in enumerate(firsttrain_loader3):\n",
    "\n",
    "                data = data.to(device)\n",
    "                target = target.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                output = model(data)\n",
    "                L=F.mse_loss(target,output)\n",
    "                optimizer.zero_grad()\n",
    "                L.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                if learning_steps % 50 == 0:\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        model.eval()\n",
    "                        mse = []\n",
    "                        for j, (data1, target1) in enumerate(firstvalid_loader3):\n",
    "                            data1 = data1.to(device)\n",
    "                            target1 = target1.to(device)\n",
    "                            # Forward pass\n",
    "                            output = model(data1)\n",
    "                            \n",
    "                            pred = output.cpu().detach().numpy().reshape(output.shape[0])\n",
    "                            labels = target1.cpu().numpy().reshape(output.shape[0])\n",
    "\n",
    "                            if output.shape[0] > 60:\n",
    "                                mse.append(metrics.mean_squared_error(labels, pred))\n",
    "                        # print(np.mean(auc))\n",
    "                        model_MSE.append(np.mean(mse))\n",
    "\n",
    "                        model.train()\n",
    "            learning_steps += 1\n",
    "\n",
    "        for n in range(8):\n",
    "            MSE = model_MSE[n]\n",
    "            # print(AUC)\n",
    "            if MSE < best_MSE:\n",
    "                best_MSE = MSE\n",
    "                best_learning_steps = learning_steps_list[n]\n",
    "                best_LearningRate = learning_rate\n",
    "                best_dropprob = dropprob\n",
    "                best_layer_number= layer_number\n",
    "                best_layer_size= layer_size\n",
    "                best_NN_mean=NN_mean\n",
    "                best_N_sigma=N_sigma\n",
    "                best_RNN_hidden_size= RNN_hidden_size\n",
    "                best_RNN_mean=RNN_mean\n",
    "                best_RNN_sigma=RNN_sigma\n",
    "                best_weight_decay=Weight_decay\n",
    "\n",
    "    print('best_MSE=', best_MSE)\n",
    "    print('best_learning_steps=', best_learning_steps)\n",
    "    print('best_LearningRate=', best_LearningRate)\n",
    "    print('best_dropprob=', best_dropprob)\n",
    "    print('best_weight_decay=',best_weight_decay)\n",
    "    print('best_layer_number=', best_layer_number)\n",
    "    print('best_layer_size=', best_layer_size)\n",
    "    print('best_NN_mean=', best_NN_mean)\n",
    "    print('best_N_sigma=', best_N_sigma)\n",
    "    print('best_RNN_hidden_size=', best_RNN_hidden_size)\n",
    "    print('best_RNN_mean=', best_RNN_mean)\n",
    "    print('best_RNN_sigma=', best_RNN_sigma)\n",
    "    best_hyperparameters = {'best_learning_steps': best_learning_steps, \n",
    "                            'best_LearningRate': best_LearningRate,\n",
    "                            'best_dropprob': best_dropprob, \n",
    "                            'best_weight_decay':best_weight_decay,\n",
    "                            'best_layer_number': best_layer_number,\n",
    "                            'best_layer_size': best_layer_size, \n",
    "                            'best_NN_mean':best_NN_mean,\n",
    "                            'best_N_sigma':best_N_sigma,\n",
    "                            'best_RNN_hidden_size' :best_RNN_hidden_size,\n",
    "                            'best_RNN_mean':best_RNN_mean,\n",
    "                            'best_RNN_sigma': best_RNN_sigma}\n",
    "    return best_hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f41a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_model3(RNN,RNN_type,best_hyperparameters,train_loader3,n_features3):\n",
    "    best_learning_steps=best_hyperparameters['best_learning_steps']\n",
    "    best_LearningRate=best_hyperparameters['best_LearningRate']\n",
    "    best_dropprob=best_hyperparameters['best_dropprob']\n",
    "    best_weight_decay=0#best_hyperparameters['best_weight_decay']\n",
    "    best_layer_number=best_hyperparameters['best_layer_number']\n",
    "    best_N_sigma=best_hyperparameters['best_N_sigma']\n",
    "    best_layer_size=best_hyperparameters['best_layer_size']\n",
    "    best_RNN_hidden_size=best_hyperparameters['best_RNN_hidden_size']\n",
    "    best_RNN_sigma=best_hyperparameters['best_RNN_sigma']\n",
    "    best_RNN_mean=best_hyperparameters['best_RNN_mean']\n",
    "    best_NN_mean=best_hyperparameters['best_NN_mean']\n",
    "    best_MSE=100\n",
    "\n",
    "    for number_models in range(5):\n",
    "        model = RegNetwork(RNN,RNN_type,best_RNN_hidden_size,best_RNN_mean,best_RNN_sigma,best_layer_number,\n",
    "                           best_layer_size,best_NN_mean,best_N_sigma,best_dropprob,n_features3).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(),lr=best_learning_steps,weight_decay=best_weight_decay)\n",
    "        learning_steps=0\n",
    "        model.train()\n",
    "        while (learning_steps<=best_learning_steps):    \n",
    "            for i, (data, target) in enumerate (train_loader3):\n",
    "                data = data.to(device)\n",
    "                target = target.to(device)\n",
    "                # Forward pass\n",
    "                output = model(data)\n",
    "                L=F.mse_loss(target,output)\n",
    "                optimizer.zero_grad()\n",
    "                L.backward()\n",
    "                optimizer.step()\n",
    "            learning_steps+=1\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            mse=[]\n",
    "            for i, (data, target) in enumerate(train_loader3):\n",
    "\n",
    "                data = data.to(device)\n",
    "                target = target.to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                output = model(data)\n",
    "\n",
    "                pred=output.cpu().detach().numpy().reshape(output.shape[0])\n",
    "\n",
    "                labels=target.cpu().numpy().reshape(output.shape[0])\n",
    "                if (output.shape[0]>30):\n",
    "                    mse.append(metrics.mean_squared_error(labels, pred))\n",
    "\n",
    "            MSE_Training=np.mean(mse)     \n",
    "\n",
    "            print('MSE on training data for model ',number_models+1,' = ',MSE_Training)\n",
    "            if (MSE_Training<best_MSE):\n",
    "                best_MSE=MSE_Training\n",
    "                best_model=model\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d997389",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_predict3(best_model,test_loader3):\n",
    "\n",
    "    with torch.no_grad():\n",
    "        best_model.eval()\n",
    "\n",
    "        for i, (data, target) in enumerate(test_loader3):\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            output = best_model(data)\n",
    "            pred = output.cpu().detach().numpy().reshape(output.shape[0])\n",
    "            myprob = \"\\n\".join(map(str, pred[:]))\n",
    "            labels = target.cpu().numpy().reshape(output.shape[0])\n",
    "    return (pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cda9d3",
   "metadata": {},
   "source": [
    "# SES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf38a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SES(ts,alpha=0.1):\n",
    "    d = np.array(ts) # Transform the input into a numpy array\n",
    "    cols = len(d) # Historical period length\n",
    "    \n",
    "    #level (a), periodicity(p) and forecast (f)\n",
    "    f = np.full((cols),np.nan)\n",
    "    \n",
    "    # Initialization\n",
    "    f[0] = d[0]\n",
    "# Create all the t+1 forecasts\n",
    "    for t in range(0,cols-1):        \n",
    "        f[t+1] = alpha*d[t] + (1-alpha)*f[t]   \n",
    "                      \n",
    "    df = pd.DataFrame.from_dict({\"Demand\":d,\"Forecast\":f,\"Error\":d-f})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe25812",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_SES(DATA_Micro,n1,n2):\n",
    "    Opt_SES=[0,10000]\n",
    "    for i in np.arange (0,1,0.001):\n",
    "        SES_train=SES(DATA_Micro.Quantite[n1:n2],alpha=i)\n",
    "        if (np.sqrt(mean_squared_error(SES_train['Forecast'].values[:],DATA_Micro['Quantite'][n1:n2].values))<Opt_SES[1]):\n",
    "            Opt_SES[1]=np.sqrt(mean_squared_error(SES_train['Forecast'].values[:],DATA_Micro['Quantite'][n1:n2].values))\n",
    "            Opt_SES[0]=i\n",
    "    return (Opt_SES[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612d52db",
   "metadata": {},
   "source": [
    "# Croston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686cadc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Croston(ts,extra_periods=1,alpha=0.4):\n",
    "    d = np.array(ts) # Transform the input into a numpy array\n",
    "    cols = len(d) # Historical period length\n",
    "    d = np.append(d,[np.nan]*extra_periods) # Append np.nan into the demand array to cover future periods\n",
    "    \n",
    "    #level (a), periodicity(p) and forecast (f)\n",
    "    a,p,f = np.full((3,cols+extra_periods),np.nan)\n",
    "    q = 1 #periods since last demand observation\n",
    "    \n",
    "    # Initialization\n",
    "    first_occurence = np.argmax(d[:cols]>0)\n",
    "    a[0] = d[first_occurence]\n",
    "    p[0] = 1 + first_occurence\n",
    "    f[0] = a[0]/p[0]\n",
    "# Create all the t+1 forecasts\n",
    "    for t in range(0,cols):        \n",
    "        if d[t] > 0:\n",
    "            a[t+1] = alpha*d[t] + (1-alpha)*a[t] \n",
    "            p[t+1] = alpha*q + (1-alpha)*p[t]\n",
    "            f[t+1] = a[t+1]/p[t+1]\n",
    "            q = 1           \n",
    "        else:\n",
    "            a[t+1] = a[t]\n",
    "            p[t+1] = p[t]\n",
    "            f[t+1] = f[t]\n",
    "            q += 1\n",
    "       \n",
    "    # Future Forecast \n",
    "    a[cols+1:cols+extra_periods] = a[cols]\n",
    "    p[cols+1:cols+extra_periods] = p[cols]\n",
    "    f[cols+1:cols+extra_periods] = f[cols]\n",
    "                      \n",
    "    df = pd.DataFrame.from_dict({\"Demand\":d,\"Forecast\":f,\"Period\":p,\"Level\":a,\"Error\":d-f})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b4b7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_CR(DATA_Micro,n1,n2):\n",
    "    Opt_CR=[0,100]\n",
    "    for i in np.arange (0,1,0.001):\n",
    "        Cr_train=Croston(DATA_Micro.Quantite[n1:n2],alpha=i)\n",
    "        if (np.sqrt(mean_squared_error(Cr_train['Forecast'].values[:-1],DATA_Micro['Quantite'][n1:n2].values))<Opt_CR[1]):\n",
    "            Opt_CR[1]=np.sqrt(mean_squared_error(Cr_train['Forecast'].values[:-1],DATA_Micro['Quantite'][n1:n2].values))\n",
    "            Opt_CR[0]=i\n",
    "    return(Opt_CR[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13821cb",
   "metadata": {},
   "source": [
    "# SBA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1749ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SBA(ts,extra_periods=1,alpha=0.1,beta=0.1):\n",
    "    d = np.array(ts) # Transform the input into a numpy array\n",
    "    cols = len(d) # Historical period length\n",
    "    d = np.append(d,[np.nan]*extra_periods) # Append np.nan into the demand array to cover future periods\n",
    "    \n",
    "    #level (a), periodicity(p) and forecast (f)\n",
    "    a,p,f = np.full((3,cols+extra_periods),np.nan)\n",
    "    q = 1 #periods since last demand observation\n",
    "    \n",
    "    # Initialization\n",
    "    first_occurence = np.argmax(d[:cols]>0)\n",
    "    a[0] = d[first_occurence]\n",
    "    p[0] = 1 + first_occurence\n",
    "    f[0] = a[0]/p[0]\n",
    "# Create all the t+1 forecasts\n",
    "    for t in range(0,cols):        \n",
    "        if d[t] > 0:\n",
    "            a[t+1] = alpha*d[t] + (1-alpha)*a[t] \n",
    "            p[t+1] = beta*q + (1-beta)*p[t]\n",
    "            f[t+1] = (1-alpha/2)*a[t+1]/p[t+1]\n",
    "            q = 1           \n",
    "        else:\n",
    "            a[t+1] = a[t]\n",
    "            p[t+1] = p[t]\n",
    "            f[t+1] = f[t]\n",
    "            q += 1\n",
    "       \n",
    "    # Future Forecast \n",
    "    a[cols+1:cols+extra_periods] = a[cols]\n",
    "    p[cols+1:cols+extra_periods] = p[cols]\n",
    "    f[cols+1:cols+extra_periods] = f[cols]\n",
    "                      \n",
    "    df = pd.DataFrame.from_dict({\"Demand\":d,\"Forecast\":f,\"Period\":p,\"Level\":a,\"Error\":d-f})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4379bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_SBA(DATA_Micro,n1,n2):\n",
    "    Opt_SBA=[0,0,100]\n",
    "    for i in np.arange (0,1,0.01):\n",
    "        for j in np.arange (0,1,0.01):\n",
    "            SBA_train=SBA(DATA_Micro.Quantite[n1:n2],alpha=i,beta=j)\n",
    "            if (np.sqrt(mean_squared_error(SBA_train['Forecast'].values[:-1],DATA_Micro['Quantite'][n1:n2].values))<Opt_SBA[2]):\n",
    "                Opt_SBA[2]=np.sqrt(mean_squared_error(SBA_train['Forecast'].values[:-1],DATA_Micro['Quantite'][n1:n2].values))\n",
    "                Opt_SBA[1]=j\n",
    "                Opt_SBA[0]=i\n",
    "    return(Opt_SBA[0],Opt_SBA[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac5991b",
   "metadata": {},
   "source": [
    "# Generating results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07efe112",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(path,product):\n",
    "    DATA_Micro=pd.read_csv(path+product+\".csv\", sep=';',encoding = \"ISO-8859-1\")\n",
    "    \n",
    "    #preprocess\n",
    "    #number of input lags\n",
    "    autocorr=acf(DATA_Micro['Quantite'],False,100)\n",
    "    max(autocorr[(autocorr<1)])\n",
    "    Nlags=np.where(autocorr==max(autocorr[(autocorr<1)]))[0][0]\n",
    "    \n",
    "    #number of output values\n",
    "    metric='PRC'\n",
    "    Nout=1\n",
    "    Nin=1\n",
    "    n_features=5\n",
    "    n_features2=3\n",
    "    n_features3=5\n",
    "    w1=DATA_Micro['ZNZDemand'].value_counts()[0]/DATA_Micro['ZNZDemand'].value_counts()[1]/2\n",
    "    w0=1\n",
    "    n1=np.where(DATA_Micro['PeriodsSepLastTwoNnZeroDemands']>0)[0][1]\n",
    "    n2=2*n1+int((DATA_Micro.shape[0]-n1)*90/100)\n",
    "    n_steps_in, n_steps_out = Nin, Nout\n",
    "    \n",
    "    RNN='BiLSTM'\n",
    "    batch_size=64\n",
    "    evaluate_performance=True\n",
    "    #Network1\n",
    "    firsttrain,firstvalid,train_data,test_data,alldata=Convert_format1(DATA_Micro,n_steps_in,n_steps_out)\n",
    "    firsttrain_dataset=dataset_load(firsttrain)\n",
    "    firstvalid_dataset=dataset_load(firstvalid)\n",
    "    train_dataset=dataset_load(train_data)\n",
    "    test_dataset=dataset_load(test_data)\n",
    "    all_dataset=dataset_load(alldata)\n",
    "    firsttrain_loader = DataLoader(dataset=firsttrain_dataset,batch_size=batch_size,shuffle=False)\n",
    "    firstvalid_loader = DataLoader(dataset=firstvalid_dataset,batch_size=batch_size,shuffle=False)\n",
    "    train_loader = DataLoader(dataset=train_dataset,batch_size=batch_size,shuffle=False)\n",
    "    test_loader = DataLoader(dataset=test_dataset,batch_size=10000,shuffle=False)\n",
    "    alldata_loader = DataLoader(dataset=all_dataset,batch_size=10000,shuffle=False)\n",
    "    best_hyperparameters=Calibration(RNN,w1,w0,firsttrain_loader,firstvalid_loader,n_features,metric)\n",
    "    best_model,best_threshold=Train_model(RNN,w1,w0,best_hyperparameters,train_loader,n_features,metric)\n",
    "    labels,pred=test_predict(best_model,test_loader)\n",
    "    prediction_test=np.where(pred<best_threshold,0,pred)\n",
    "    prediction_test=np.where(prediction_test>best_threshold,1,prediction_test)\n",
    "    labels_train,pred_train=train_predict(best_model,alldata_loader)\n",
    "    prediction_train=np.where(pred_train<best_threshold,0,pred_train)\n",
    "    prediction_train=np.where(prediction_train>best_threshold,1,prediction_train)\n",
    "    \n",
    "    #Network2\n",
    "    firsttrain2,firstvalid2,train_data2,test_data2=Convert_format2(DATA_Micro,prediction_train,n_steps_in,n_steps_out)\n",
    "    firsttrain_dataset2=dataset_load(firsttrain2)\n",
    "    firstvalid_dataset2=dataset_load(firstvalid2)\n",
    "    train_dataset2=dataset_load(train_data2)\n",
    "    test_dataset2=dataset_load(test_data2)\n",
    "    firsttrain_loader2 = DataLoader(dataset=firsttrain_dataset2,batch_size=batch_size,shuffle=False)\n",
    "    firstvalid_loader2 = DataLoader(dataset=firstvalid_dataset2,batch_size=batch_size,shuffle=False)\n",
    "    train_loader2 = DataLoader(dataset=train_dataset2,batch_size=batch_size,shuffle=False)\n",
    "    test_loader2 = DataLoader(dataset=test_dataset2,batch_size=10000,shuffle=False)\n",
    "    best_hyperparameters2=Calibration2(firsttrain_loader2,firstvalid_loader2,n_features2)\n",
    "    best_model2=Train_model2(best_hyperparameters2,train_loader2,n_features2)\n",
    "    predsHM_scaled=test_predict2(best_model2,test_loader2)\n",
    "    prediction_HM=np.multiply(prediction_test,predsHM_scaled)\n",
    "    \n",
    "    #RegNetwork-RNN\n",
    "    firsttrain3,firstvalid3,train_data3,test_data3,scaler=Convert_format3(DATA_Micro,n_steps_in,n_steps_out)\n",
    "    firsttrain_dataset3=dataset_load(firsttrain3)\n",
    "    firstvalid_dataset3=dataset_load(firstvalid3)\n",
    "    train_dataset3=dataset_load(train_data3)\n",
    "    test_dataset3=dataset_load(test_data3)\n",
    "    firsttrain_loader3 = DataLoader(dataset=firsttrain_dataset3,batch_size=batch_size,shuffle=False)\n",
    "    firstvalid_loader3 = DataLoader(dataset=firstvalid_dataset3,batch_size=batch_size,shuffle=False)\n",
    "    train_loader3 = DataLoader(dataset=train_dataset3,batch_size=batch_size,shuffle=False)\n",
    "    test_loader3 = DataLoader(dataset=test_dataset3,batch_size=10000,shuffle=False)\n",
    "    best_hyperparameters3=Calibration3(True,'LSTM',firsttrain_loader3,firstvalid_loader3,n_features3)\n",
    "    best_model3=Train_model3(True,'LSTM',best_hyperparameters3,train_loader3,n_features3)\n",
    "    pred_BGru=scaler.inverse_transform([test_predict3(best_model3,test_loader3)]).ravel()\n",
    "    \n",
    "    #RegNetwork-MLP\n",
    "    best_hyperparameters4=Calibration3(False,'LSTM',firsttrain_loader3,firstvalid_loader3,n_features3)\n",
    "    best_model4=Train_model3(False,'LSTM',best_hyperparameters4,train_loader3,n_features3)\n",
    "    pred_MLP=scaler.inverse_transform([test_predict3(best_model4,test_loader3)]).ravel()\n",
    "    \n",
    "    #SES\n",
    "    SES_test=SES(DATA_Micro.Quantite[:],evaluate_SES(DATA_Micro,n1,n2))\n",
    "    SES_pred=SES_test['Forecast'][n2:]\n",
    "    \n",
    "    #Croston\n",
    "    Cr_test=Croston(DATA_Micro.Quantite[0:],alpha=evaluate_CR(DATA_Micro,n1,n2))\n",
    "    Cr_pred=Cr_test['Forecast'][n2:-1]\n",
    "    \n",
    "    #SBA\n",
    "    Opt_SBA=evaluate_SBA(DATA_Micro,n1,n2)\n",
    "    SBA_test=SBA(DATA_Micro.Quantite[0:],alpha=Opt_SBA[0],beta=Opt_SBA[1])\n",
    "    SBA_pred=SBA_test['Forecast'][n2:-1]\n",
    "    \n",
    "    #Naive\n",
    "    naiv_pred=DATA_Micro['Quantite'][n2-Nlags:-Nlags].values\n",
    "    \n",
    "    return(prediction_HM,pred_MLP,pred_BGru,SES_pred,Cr_pred,SBA_pred,naiv_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5616715",
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"./gen data/\"\n",
    "product=\"Prd1\"\n",
    "prediction_HM,pred_MLP,pred_BGru,SES_pred,Cr_pred,SBA_pred,naiv_pred=main(path,product)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252eab9b",
   "metadata": {},
   "source": [
    "# Saving results in csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3708c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PB_all(DATA_Micro,n2):\n",
    "    L_PB=[]\n",
    "    PB=pd.DataFrame({'pt':DATA_Micro['Quantite'][n2:],'phm':prediction_HM.ravel(),'pNN':pred_MLP.ravel(),'pRNN':pred_BGru.ravel(),'pses':SES_pred,'pcroston':Cr_pred,'pSBA':SBA_pred,'pNaive':naiv_pred})\n",
    "    PB[\"error\"]=PB.apply(lambda row: pb_func(abs(row[1:]-row[0])),axis=1)\n",
    "    PB['error'].value_counts()/(DATA_Micro.shape[0]-n2-1)\n",
    "    try:\n",
    "        Pb_HM=(PB['error'].value_counts()/(DATA_Micro.shape[0]-n2))['phm']\n",
    "    except:\n",
    "        Pb_HM=0\n",
    "    L_PB.append(Pb_HM)\n",
    "    PB=pd.DataFrame({'pt':DATA_Micro['Quantite'][n2:-1],'pNN':pred_MLP.ravel(),'phm':prediction_HM.ravel(),'pRNN':pred_BGru.ravel(),'pses':SES_pred,'pcroston':Cr_pred,'pSBA':SBA_pred,'pNaive':naiv_pred})\n",
    "    PB[\"error\"]=PB.apply(lambda row: pb_func(abs(row[1:]-row[0])),axis=1)\n",
    "    try:\n",
    "        Pb_NN=(PB['error'].value_counts()/(DATA_Micro.shape[0]-n2-1))['pNN']\n",
    "    except:\n",
    "        Pb_NN=0\n",
    "    L_PB.append(Pb_NN)\n",
    "    PB=pd.DataFrame({'pt':DATA_Micro['Quantite'][n2:-1],'pses':SES_pred,'pNN':pred_MLP.ravel(),'phm':prediction_HM.ravel(),'pRNN':pred_BGru.ravel(),'pcroston':Cr_pred,'pSBA':SBA_pred,'pNaive':naiv_pred})\n",
    "    PB[\"error\"]=PB.apply(lambda row: pb_func(abs(row[1:]-row[0])),axis=1)\n",
    "    PB['error'].value_counts()/(DATA_Micro.shape[0]-n2-1)\n",
    "    try:\n",
    "        Pb_SES=(PB['error'].value_counts()/(DATA_Micro.shape[0]-n2-1))['pses']\n",
    "    except:\n",
    "        Pb_SES=0\n",
    "    L_PB.append(Pb_SES)\n",
    "    PB=pd.DataFrame({'pt':DATA_Micro['Quantite'][n2:-1],'pcroston':Cr_pred,'pNN':pred_MLP.ravel(),'phm':prediction_HM.ravel(),'pses':SES_pred,'pSBA':SBA_pred,'pRNN':pred_BGru.ravel(),'pNaive':naiv_pred})\n",
    "    PB[\"error\"]=PB.apply(lambda row: pb_func(abs(row[1:]-row[0])),axis=1)\n",
    "    PB['error'].value_counts()/(DATA_Micro.shape[0]-n2-1)\n",
    "    try:\n",
    "        Pb_CR=(PB['error'].value_counts()/(DATA_Micro.shape[0]-n2-1))['pcroston']\n",
    "    except:\n",
    "        Pb_CR=0\n",
    "    L_PB.append(Pb_CR)\n",
    "    PB=pd.DataFrame({'pt':DATA_Micro['Quantite'][n2:-1],'pSBA':SBA_pred,'pcroston':Cr_pred,'pNN':pred_MLP.ravel(),'phm':prediction_HM.ravel(),'pses':SES_pred,'pRNN':pred_BGru.ravel(),'pNaive':naiv_pred})\n",
    "    PB[\"error\"]=PB.apply(lambda row: pb_func(abs(row[1:]-row[0])),axis=1)\n",
    "    PB['error'].value_counts()/(DATA_Micro.shape[0]-n2-1)\n",
    "    try:\n",
    "        Pb_SBA=(PB['error'].value_counts()/(DATA_Micro.shape[0]-n2-1))['pSBA']\n",
    "    except:\n",
    "        Pb_SBA=0\n",
    "    L_PB.append(Pb_SBA)\n",
    "    PB=pd.DataFrame({'pt':DATA_Micro['Quantite'][n2:-1],'pRNN':pred_BGru.ravel(),'pNN':pred_MLP.ravel(),'phm':prediction_HM.ravel(),'pses':SES_pred,'pcroston':Cr_pred,'pSBA':SBA_pred,'pNaive':naiv_pred})\n",
    "    PB[\"error\"]=PB.apply(lambda row: pb_func(abs(row[1:]-row[0])),axis=1)\n",
    "    PB['error'].value_counts()/(DATA_Micro.shape[0]-n2-1)\n",
    "    try:\n",
    "        Pb_RNN=(PB['error'].value_counts()/(DATA_Micro.shape[0]-n2-1))['pRNN']\n",
    "    except:\n",
    "        Pb_RNN=0\n",
    "    L_PB.append(Pb_RNN)\n",
    "    PB=pd.DataFrame({'pt':DATA_Micro['Quantite'][n2:-1],'pNaive':naiv_pred,'pRNN':pred_BGru.ravel(),'pNN':pred_MLP.ravel(),'phm':prediction_HM.ravel(),'pses':SES_pred,'pcroston':Cr_pred,'pSBA':SBA_pred})\n",
    "    PB[\"error\"]=PB.apply(lambda row: pb_func(abs(row[1:]-row[0])),axis=1)\n",
    "    PB['error'].value_counts()/(DATA_Micro.shape[0]-n2-1)\n",
    "    try:\n",
    "        Pb_Naive=(PB['error'].value_counts()/(DATA_Micro.shape[0]-n2-1))['pNaive']\n",
    "    except:\n",
    "        Pb_Naive=0\n",
    "    L_PB.append(Pb_Naive)\n",
    "    return L_PB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b5ceda",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_Micro=pd.read_csv(path+product+\".csv\", sep=';',encoding = \"ISO-8859-1\")\n",
    "n1=np.where(DATA_Micro['PeriodsSepLastTwoNnZeroDemands']>0)[0][1]\n",
    "n2=2*n1+int((DATA_Micro.shape[0]-n1)*90/100)\n",
    "Train_X=DATA_Micro['Quantite'][n1:n2].values\n",
    "Test_Y=DATA_Micro['Quantite'][n2:].values\n",
    "ADI=(DATA_Micro['ZNZDemand'].value_counts()[1]+DATA_Micro['ZNZDemand'].value_counts()[0])/DATA_Micro['ZNZDemand'].value_counts()[1]\n",
    "Test_df=(DATA_Micro[DATA_Micro['ZNZDemand']==1][['Quantite','Interval']]).dropna(axis=0)\n",
    "CV2=(Test_df['Quantite'].std()/Test_df['Quantite'].mean())**2\n",
    "n_obs=DATA_Micro.shape[0]-n1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11de309",
   "metadata": {},
   "outputs": [],
   "source": [
    "L_PB=PB_all(DATA_Micro,n2)\n",
    "PB=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e37470",
   "metadata": {},
   "outputs": [],
   "source": [
    "PB=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f93c20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if PB:\n",
    "    header=['product','Samples','ADI','CV2','MASE_HM','MASE_NN','MASE_SES','MASE_Cr','MASE_SBA','MASE_RNN','MASE_Naive',\n",
    "        'RMSSE_HM','RMSSE_NN','RMSSE_SES','RMSSE_Cr','RMSSE_SBA','RMSSE_RNN','RMSSE_Naive',\n",
    "        'PB_HM','PB_NN','PB_SES','PB_Cr','PB_SBA','PB_RNN','PB_Naive']\n",
    "    dicti={\n",
    "        'product':product,\n",
    "        'Samples':n_obs,\n",
    "        'ADI':ADI,\n",
    "        'CV2':CV2,\n",
    "        'MASE_HM':MASE(Train_X,Test_Y,prediction_HM),\n",
    "        'MASE_NN':MASE(Train_X,Test_Y,pred_MLP),\n",
    "        'MASE_SES':MASE(Train_X,Test_Y,SES_pred.values),\n",
    "        'MASE_Cr':MASE(Train_X,Test_Y,Cr_pred),\n",
    "        'MASE_SBA':MASE(Train_X,Test_Y,SBA_pred),\n",
    "        'MASE_RNN':MASE(Train_X,Test_Y,pred_BGru),\n",
    "        'MASE_Naive':MASE(Train_X,Test_Y,naiv_pred),\n",
    "        'RMSSE_HM':RMSSE(Train_X,Test_Y,prediction_HM),\n",
    "        'RMSSE_NN':RMSSE(Train_X,Test_Y,pred_MLP),\n",
    "        'RMSSE_SES':RMSSE(Train_X,Test_Y,SES_pred),\n",
    "        'RMSSE_Cr':RMSSE(Train_X,Test_Y,Cr_pred),\n",
    "        'RMSSE_SBA':RMSSE(Train_X,Test_Y,SBA_pred),\n",
    "        'RMSSE_RNN':RMSSE(Train_X,Test_Y,pred_BGru),\n",
    "        'RMSSE_Naive':RMSSE(Train_X,Test_Y,naiv_pred),\n",
    "        'PB_HM':L_PB[0],\n",
    "        'PB_NN':L_PB[1],\n",
    "        'PB_SES':L_PB[2],\n",
    "        'PB_Cr':L_PB[3],\n",
    "        'PB_SBA':L_PB[4],\n",
    "        'PB_RNN':L_PB[5],\n",
    "        'PB_Naive':L_PB[6]\n",
    "        }\n",
    "else:\n",
    "    header=['product','Samples','ADI','CV2','MASE_HM','MASE_NN','MASE_SES','MASE_Cr','MASE_SBA','MASE_RNN','MASE_Naive',\n",
    "        'RMSSE_HM','RMSSE_NN','RMSSE_SES','RMSSE_Cr','RMSSE_SBA','RMSSE_RNN','RMSSE_Naive']\n",
    "    dicti={\n",
    "        'product':product,\n",
    "        'Samples':n_obs,\n",
    "        'ADI':ADI,\n",
    "        'CV2':CV2,\n",
    "        'MASE_HM':MASE(Train_X,Test_Y,prediction_HM),\n",
    "        'MASE_NN':MASE(Train_X,Test_Y,pred_MLP),\n",
    "        'MASE_SES':MASE(Train_X,Test_Y,SES_pred.values),\n",
    "        'MASE_Cr':MASE(Train_X,Test_Y,Cr_pred),\n",
    "        'MASE_SBA':MASE(Train_X,Test_Y,SBA_pred),\n",
    "        'MASE_RNN':MASE(Train_X,Test_Y,pred_BGru),\n",
    "        'MASE_Naive':MASE(Train_X,Test_Y,naiv_pred),\n",
    "        'RMSSE_HM':RMSSE(Train_X,Test_Y,prediction_HM),\n",
    "        'RMSSE_NN':RMSSE(Train_X,Test_Y,pred_MLP),\n",
    "        'RMSSE_SES':RMSSE(Train_X,Test_Y,SES_pred),\n",
    "        'RMSSE_Cr':RMSSE(Train_X,Test_Y,Cr_pred),\n",
    "        'RMSSE_SBA':RMSSE(Train_X,Test_Y,SBA_pred),\n",
    "        'RMSSE_RNN':RMSSE(Train_X,Test_Y,pred_BGru),\n",
    "        'RMSSE_Naive':RMSSE(Train_X,Test_Y,naiv_pred)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244a151d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving results\n",
    "try:\n",
    "    df=pd.read_csv('Reg_res.csv')\n",
    "    new=False\n",
    "except:\n",
    "    new=True\n",
    "if new:\n",
    "    with open('Reg_res.csv','w') as fd:\n",
    "        writer = csv.writer(fd)\n",
    "        writer.writerow(header)\n",
    "        writer = csv.DictWriter(fd, fieldnames=header)\n",
    "        writer.writerow(dicti)\n",
    "else:\n",
    "    with open('Reg_res.csv','a',newline='') as fd:\n",
    "        writer = csv.DictWriter(fd, fieldnames=header)\n",
    "        writer.writerow(dicti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b07763f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
